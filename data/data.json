[
    {
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence - Wikipedia",
        "content": "Artificial intelligence(AI) is the capability ofcomputational systemsto perform tasks typically associated withhuman intelligence, such aslearning,reasoning,problem-solving,perception, anddecision-making. It is afield of researchincomputer sciencethat develops and studies methods andsoftwarethat enable machines toperceive their environmentand uselearningandintelligenceto take actions that maximize their chances of achieving defined goals.[1]\nHigh-profileapplications of AIinclude advancedweb search engines(e.g.,Google Search);recommendation systems(used byYouTube,Amazon, andNetflix);virtual assistants(e.g.,Google Assistant,Siri, andAlexa);autonomous vehicles(e.g.,Waymo);generativeandcreativetools (e.g.,language modelsandAI art); andsuperhumanplay and analysis instrategy games(e.g.,chessandGo). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it'snot labeled AI anymore.\"[2][3]\nVarious subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning,reasoning,knowledge representation,planning,natural language processing,perception, and support forrobotics.[a]To reach these goals, AI researchers have adapted and integrated a wide range of techniques, includingsearchandmathematical optimization,formal logic,artificial neural networks, and methods based onstatistics,operations research, andeconomics.[b]AI also draws uponpsychology,linguistics,philosophy,neuroscience, and other fields.[4]Some companies, such asOpenAI,Google DeepMindandMeta,[5]aim to createartificial general intelligence(AGI)—AI that can complete virtually any cognitive task at least as well as a human.\nArtificial intelligence was founded as an academic discipline in 1956,[6]and the field went through multiple cycles of optimism throughoutits history,[7][8]followed by periods of disappointment and loss of funding, known asAI winters.[9][10]Funding and interest vastly increased after 2012 whengraphics processing unitsstarted being used to accelerate neural networks anddeep learningoutperformed previous AI techniques.[11]This growth accelerated further after 2017 with thetransformer architecture.[12]In the 2020s, an ongoing period of rapidprogressin advanced generative AI became known as theAI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms, which has raisedethical concernsaboutAI's long-term effectsandpotential existential risks, prompting discussions aboutregulatory policiesto ensurethe safetyand benefits of the technology.\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.[a]\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logicaldeductions.[13]By the late 1980s and 1990s, methods were developed for dealing withuncertainor incomplete information, employing concepts fromprobabilityandeconomics.[14]\nMany of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow.[15]Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[16]Accurate and efficient reasoning is an unsolved problem.\nKnowledge representationandknowledge engineering[17]allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval,[18]scene interpretation,[19]clinical decision support,[20]knowledge discovery (mining \"interesting\" and actionable inferences from largedatabases),[21]and other areas.[22]\nAknowledge baseis a body of knowledge represented in a form that can be used by a program. Anontologyis the set of objects, relations, concepts, and properties used by a particular domain of knowledge.[23]Knowledge bases need to represent things such as objects, properties, categories, and relations between objects;[24]situations, events, states, and time;[25]causes and effects;[26]knowledge about knowledge (what we know about what other people know);[27]default reasoning(things that humans assume are true until they are told differently and will remain true even when other facts are changing);[28]and many other aspects and domains of knowledge.\nAmong the most difficult problems in knowledge representation are the breadth ofcommonsense knowledge(the set of atomic facts that the average person knows is enormous);[29]and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally).[16]There is also the difficulty ofknowledge acquisition, the problem of obtaining knowledge for AI applications.[c]\nAn \"agent\" is anything that perceives and takes actions in the world. Arational agenthas goals or preferences and takes actions to make them happen.[d][32]Inautomated planning, the agent has a specific goal.[33]Inautomated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": theutilityof all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.[34]\nInclassical planning, the agent knows exactly what the effect of any action will be.[35]In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.[36]\nIn some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., withinverse reinforcement learning), or the agent can seek information to improve its preferences.[37]Information value theorycan be used to weigh the value of exploratory or experimental actions.[38]The space of possible future actions and situations is typicallyintractablylarge, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.\nAMarkov decision processhas atransition modelthat describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. Apolicyassociates a decision with each possible state. The policy could be calculated (e.g., byiteration), beheuristic, or it can be learned.[39]\nGame theorydescribes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.[40]\nMachine learningis the study of programs that can improve their performance on a given task automatically.[41]It has been a part of AI from the beginning.[e]\nThere are several kinds of machine learning.Unsupervised learninganalyzes a stream of data and finds patterns and makes predictions without any other guidance.[44]Supervised learningrequires labeling the training data with the expected answers, and comes in two main varieties:classification(where the program must learn to predict what category the input belongs in) andregression(where the program must deduce a numeric function based on numeric input).[45]\nInreinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\".[46]Transfer learningis when the knowledge gained from one problem is applied to a new problem.[47]Deep learningis a type of machine learning that runs inputs through biologically inspiredartificial neural networksfor all of these types of learning.[48]\nComputational learning theorycan assess learners bycomputational complexity, bysample complexity(how much data is required), or by other notions ofoptimization.[49]\nNatural language processing(NLP) allows programs to read, write and communicate in human languages.[50]Specific problems includespeech recognition,speech synthesis,machine translation,information extraction,information retrievalandquestion answering.[51]\nEarly work, based onNoam Chomsky'sgenerative grammarandsemantic networks, had difficulty withword-sense disambiguation[f]unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem[29]).Margaret Mastermanbelieved that it was meaning and not grammar that was the key to understanding languages, and thatthesauriand not dictionaries should be the basis of computational language structure.\nModern deep learning techniques for NLP includeword embedding(representing words, typically asvectorsencoding their meaning),[52]transformers(a deep learning architecture using anattentionmechanism),[53]and others.[54]In 2019,generative pre-trained transformer(or \"GPT\") language models began to generate coherent text,[55][56]and by 2023, these models were able to get human-level scores on thebar exam,SATtest,GREtest, and many other real-world applications.[57]\nMachine perceptionis the ability to use input from sensors (such as cameras, microphones, wireless signals, activelidar, sonar, radar, andtactile sensors) to deduce aspects of the world.Computer visionis the ability to analyze visual input.[58]\nThe field includesspeech recognition,[59]image classification,[60]facial recognition,object recognition,[61]object tracking,[62]androbotic perception.[63]\nAffective computingis a field that comprises systems that recognize, interpret, process, or simulate humanfeeling, emotion, and mood.[65]For example, somevirtual assistantsare programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitatehuman–computer interaction.\nHowever, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents.[66]Moderate successes related to affective computing include textualsentiment analysisand, more recently,multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject.[67]\nA machine withartificial general intelligencewould be able to solve a wide variety of problems with breadth and versatility similar tohuman intelligence.[68]\nAI research uses a wide variety of techniques to accomplish the goals above.[b]\nAI can solve many problems by intelligently searching through many possible solutions.[69]There are two very different kinds of search used in AI:state space searchandlocal search.\nState space searchsearches through a tree of possible states to try to find a goal state.[70]For example,planningalgorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process calledmeans-ends analysis.[71]\nSimple exhaustive searches[72]are rarely sufficient for most real-world problems: thesearch space(the number of places to search) quickly grows toastronomical numbers. The result is a search that istoo slowor never completes.[15]\"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal.[73]\nAdversarial searchis used forgame-playingprograms, such as chess or Go. It searches through atreeof possible moves and countermoves, looking for a winning position.[74]\nLocal searchusesmathematical optimizationto find a solution to a problem. It begins with some form of guess and refines it incrementally.[75]\nGradient descentis a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize aloss function. Variants of gradient descent are commonly used to trainneural networks,[76]through thebackpropagationalgorithm.\nAnother type of local search isevolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them,selectingonly the fittest to survive each generation.[77]\nDistributed search processes can coordinate viaswarm intelligencealgorithms. Two popular swarm algorithms used in search areparticle swarm optimization(inspired by birdflocking) andant colony optimization(inspired byant trails).[78]\nFormallogicis used forreasoningandknowledge representation.[79]Formal logic comes in two main forms:propositional logic(which operates on statements that are true or false and useslogical connectivessuch as \"and\", \"or\", \"not\" and \"implies\")[80]andpredicate logic(which also operates on objects, predicates and relations and usesquantifierssuch as \"EveryXis aY\" and \"There aresomeXs that areYs\").[81]\nDeductive reasoningin logic is the process ofprovinga new statement (conclusion) from other statements that are given and assumed to be true (thepremises).[82]Proofs can be structured as prooftrees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes byinference rules.\nGiven a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whoseleaf nodesare labelled by premises oraxioms. In the case ofHorn clauses, problem-solving search can be performed by reasoningforwardsfrom the premises orbackwardsfrom the problem.[83]In the more general case of the clausal form offirst-order logic,resolutionis a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.[84]\nInference in both Horn clause logic and first-order logic isundecidable, and thereforeintractable. However, backward reasoning with Horn clauses, which underpins computation in thelogic programminglanguageProlog, isTuring complete. Moreover, its efficiency is competitive with computation in othersymbolic programminglanguages.[85]\nFuzzy logicassigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true.[86]\nNon-monotonic logics, including logic programming withnegation as failure, are designed to handledefault reasoning.[28]Other specialized versions of logic have been developed to describe many complex domains.\nMany problems in AI (including reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods fromprobabilitytheory and economics.[87]Precise mathematical tools have been developed that analyze how an agent can make choices and plan, usingdecision theory,decision analysis,[88]andinformation value theory.[89]These tools include models such asMarkov decision processes,[90]dynamicdecision networks,[91]game theoryandmechanism design.[92]\nBayesian networks[93]are a tool that can be used forreasoning(using theBayesian inferencealgorithm),[g][95]learning(using theexpectation–maximization algorithm),[h][97]planning(usingdecision networks)[98]andperception(usingdynamic Bayesian networks).[91]\nProbabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g.,hidden Markov modelsorKalman filters).[91]\nThe simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand.Classifiers[99]are functions that usepattern matchingto determine the closest match. They can be fine-tuned based on chosen examples usingsupervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as adata set. When a new observation is received, that observation is classified based on previous experience.[45]\nThere are many kinds of classifiers in use.[100]Thedecision treeis the simplest and most widely used symbolic machine learning algorithm.[101]K-nearest neighboralgorithm was the most widely used analogical AI until the mid-1990s, andKernel methodssuch as thesupport vector machine(SVM) displaced k-nearest neighbor in the 1990s.[102]Thenaive Bayes classifieris reportedly the \"most widely used learner\"[103]at Google, due in part to its scalability.[104]Neural networksare also used as classifiers.[105]\nAn artificial neural network is based on a collection of nodes also known asartificial neurons, which loosely model theneuronsin a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once theweightcrosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.[105]\nLearning algorithms for neural networks uselocal searchto choose the weights that will get the right output for each input during training. The most common training technique is thebackpropagationalgorithm.[106]Neural networks learn to model complex relationships between inputs and outputs andfind patternsin data. In theory, a neural network can learn any function.[107]\nInfeedforward neural networksthe signal passes in only one direction.[108]The termperceptrontypically refers to a single-layer neural network.[109]In contrast, deep learning uses many layers.[110]Recurrent neural networks(RNNs) feed the output signal back into the input, which allows short-term memories of previous input events.Long short-term memorynetworks (LSTMs) are recurrent neural networks that better preserve longterm dependencies and are less sensitive to thevanishing gradient problem.[111]Convolutional neural networks(CNNs) use layers ofkernelsto more efficiently process local patterns. This local processing is especially important inimage processing, where the early CNN layers typically identify simple local patterns such as edges and curves, with subsequent layers detecting more complex patterns like textures, and eventually whole objects.[112]\nDeep learninguses several layers of neurons between the network's inputs and outputs.[110]The multiple layers can progressively extract higher-level features from the raw input. For example, inimage processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.[114]\nDeep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, includingcomputer vision,speech recognition,natural language processing,image classification,[115]and others. The reason that deep learning performs so well in so many applications is not known as of 2021.[116]The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s)[i]but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching toGPUs) and the availability of vast amounts of training data, especially the giantcurated datasetsused for benchmark testing, such asImageNet.[j]\nGenerative pre-trained transformers(GPT) arelarge language models(LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a largecorpus of textthat can be from the Internet. The pretraining consists of predicting the nexttoken(a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique calledreinforcement learning from human feedback(RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems.[124]Such systems are used inchatbots, which allow people to ask a question or request a task in simple text.[125][126]\nCurrent models and services includeChatGPT,Claude,Gemini,Copilot, andMeta AI.[127]MultimodalGPT models can process different types of data (modalities) such as images, videos, sound, and text.[128]\nIn the late 2010s,graphics processing units(GPUs) that were increasingly designed with AI-specific enhancements and used with specializedTensorFlowsoftware had replaced previously usedcentral processing unit(CPUs) as the dominant means for large-scale (commercial and academic)machine learningmodels' training.[129]Specializedprogramming languagessuch asPrologwere used in early AI research,[130]butgeneral-purpose programming languageslikePythonhave become predominant.[131]\nThe transistor density inintegrated circuitshas been observed to roughly double every 18 months—a trend known asMoore's law, named after theIntelco-founderGordon Moore, who first identified it. Improvements inGPUshave been even faster,[132]a trend sometimes calledHuang's law,[133]named afterNvidiaco-founder and CEOJensen Huang.\nAI and machine learning technology is used in most of the essential applications of the 2020s, including:search engines(such asGoogle Search),targeting online advertisements,recommendation systems(offered byNetflix,YouTubeorAmazon), drivinginternet traffic,targeted advertising(AdSense,Facebook),virtual assistants(such asSiriorAlexa),autonomous vehicles(includingdrones,ADASandself-driving cars),automatic language translation(Microsoft Translator,Google Translate),facial recognition(Apple'sFaceIDorMicrosoft'sDeepFaceandGoogle'sFaceNet) andimage labeling(used byFacebook, Apple'sPhotosandTikTok). The deployment of AI may be overseen by achief automation officer(CAO).\nThe application of AI inmedicineandmedical researchhas the potential to increase patient care and quality of life.[134]Through the lens of theHippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.[135][136]\nFor medical research, AI is an important tool for processing and integratingbig data. This is particularly important fororganoidandtissue engineeringdevelopment which usemicroscopyimaging as a key technique in fabrication.[137]It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research.[137][138]New AI tools can deepen the understanding of biomedically relevant pathways. For example,AlphaFold 2(2021) demonstrated the ability to approximate, in hours rather than months, the 3Dstructure of a protein.[139]In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria.[140]In 2024, researchers used machine learning to accelerate the search forParkinson's diseasedrug treatments. Their aim was to identify compounds that block the clumping, or aggregation, ofalpha-synuclein(the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.[141][142]\nGame playingprograms have been used since the 1950s to demonstrate and test AI's most advanced techniques.[143]Deep Bluebecame the first computer chess-playing system to beat a reigning world chess champion,Garry Kasparov, on 11 May 1997.[144]In 2011, in aJeopardy!quiz showexhibition match,IBM'squestion answering system,Watson, defeated the two greatestJeopardy!champions,Brad RutterandKen Jennings, by a significant margin.[145]In March 2016,AlphaGowon 4 out of 5 games ofGoin a match with Go championLee Sedol, becoming the firstcomputer Go-playing system to beat a professional Go player withouthandicaps. Then, in 2017, itdefeated Ke Jie, who was the best Go player in the world.[146]Other programs handleimperfect-informationgames, such as thepoker-playing programPluribus.[147]DeepMinddeveloped increasingly generalisticreinforcement learningmodels, such as withMuZero, which could be trained to play chess, Go, orAtarigames.[148]In 2019, DeepMind's AlphaStar achieved grandmaster level inStarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map.[149]In 2021, an AI agent competed in a PlayStationGran Turismocompetition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning.[150]In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseenopen-worldvideo games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.[151]\nLarge language models, such asGPT-4,Gemini,Claude,LlamaorMistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form ofhallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such assupervisedfine-tuning[152]or trainedclassifierswith human-annotated data to improve answers for new problems and learn from corrections.[153]A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data.[154]One technique to improve their performance involves training the models to produce correctreasoningsteps, rather than just the correct result.[155]TheAlibaba Groupdeveloped a version of itsQwenmodels calledQwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems.[156]In January 2025, Microsoft proposed the techniquerStar-Maththat leveragesMonte Carlo tree searchand step-by-step reasoning, enabling a relatively small language model likeQwen-7Bto solve 53% of theAIME2024 and 90% of the MATH benchmark problems.[157]\nAlternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such asAlphaTensor,AlphaGeometry,AlphaProofandAlphaEvolve[158]all fromGoogle DeepMind,[159]LlemmafromEleutherAI[160]orJulius.[161]\nWhen natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such asLeanto define mathematical tasks. The experimental modelGemini Deep Thinkaccepts natural language prompts directly and achieved gold medal results in theInternational Math Olympiadof 2025.[162]\nSome models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.[163]\nTopological deep learningintegrates varioustopologicalapproaches.\nFinance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years.[164]\nAccording to Nicolas Firzli, director of theWorld Pensions & Investments Forum, it may be too early to see the emergence of highly innovative AI-informed financial products and services. He argues that \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\"[165]\nVarious countries are deploying AI military applications.[166]The main applications enhancecommand and control, communications, sensors, integration and interoperability.[167]Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous andautonomous vehicles.[166]AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions,target acquisition, coordination and deconfliction of distributedJoint Firesbetween networked combat vehicles, both human-operated andautonomous.[167]\nAI has been used in military operations in Iraq, Syria, Israel and Ukraine.[166][168][169][170]\nGenerative artificial intelligence(Generative AI, GenAI,[171]or GAI) is a subfield of artificial intelligence that usesgenerative modelsto producetext,images,videos,audio,software codeor other forms of data.[172][173][174]These modelslearnthe underlying patterns and structures of theirtraining dataand use them to produce new data[175][176]based on the input, which often comes in the form of natural languageprompts.[177][178]\nGenerative AI tools have become more common since theAI boomin the 2020s. This boom was made possible by improvements intransformer-baseddeepneural networks, particularlylarge language models(LLMs). Major tools includechatbotssuch asChatGPT,Copilot,Gemini,Claude,Grok, andDeepSeek;text-to-imagemodels such asStable Diffusion,Midjourney, andDALL-E; andtext-to-videomodels such asVeoandSora.[179][180][181][182][183]Technology companies developing generative AI includeOpenAI,xAI,Anthropic,Meta AI,Microsoft,Google,DeepSeek, andBaidu.[177][184][185]\nAI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, includingvirtual assistants,chatbots,autonomous vehicles,game-playing systems, andindustrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.[196][197][198]\nMicrosoft introducedCopilot Searchin February 2023 under the nameBing Chat, as a built-in feature for Microsoft Edge and Bing mobile app. Copilot Search provides AI-generated summaries[199]and step-by-step reasoning based of information from web publishers, ranked in Bing Search.[200]For safety, Copilot uses AI-based classifiers and filters to reduce potentially harmful content.[201]\nGoogle officially pushed its AI Search at its Google I/O event on May 20, 2025.[202]It keeps people looking at Google instead of clicking on a search result.AI Overviewsuses Gemini 2.5 to provide contextual answers to user queries based on web content.[203]\nApplications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer predictions,[204]AI-integrated sex toys (e.g.,teledildonics),[205]AI-generated sexual education content,[206]and AI agents that simulate sexual and romantic partners (e.g.,Replika).[207]AI is also used for the production of non-consensualdeepfake pornography, raising significant ethical and legal concerns.[208]\nAI technologies have also been used to attempt to identifyonline gender-based violenceand onlinesexual groomingof minors.[209][210]\nThere are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes.[211]A few examples areenergy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions,foreign policy, or supply chain management.\nAI applications for evacuation anddisastermanagement are growing. AI has been used to investigate patterns in large-scale and small-scale evacuations using historical data from GPS, videos or social media. Furthermore, AI can provide real-time information on the evacuation conditions.[212][213][214]\nIn agriculture, AI has helped farmers to increase yield and identify areas that need irrigation, fertilization, pesticide treatments. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conductpredictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.\nArtificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.\nDuring the2024 Indian elections, US$50 million was spent on authorized AI-generated content, notably by creatingdeepfakesof allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages.[215]\nAI has potential benefits and potential risks.[218]AI may be able to advance science and find solutions for serious problems:Demis HassabisofDeepMindhopes to \"solve intelligence, and then use that to solve everything else\".[219]However, as the use of AI has become widespread, several unintended consequences and risks have been identified.[220][221]In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.[222]\nMachine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns aboutprivacy,surveillanceandcopyright.\nAI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency.\nSensitive user data collected may include online activity records, geolocation data, video, or audio.[223]For example, in order to buildspeech recognitionalgorithms,Amazonhas recorded millions of private conversations and allowedtemporary workersto listen to and transcribe some of them.[224]Opinions about this widespread surveillance range from those who see it as anecessary evilto those for whom it is clearlyunethicaland a violation of theright to privacy.[225]\nAI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such asdata aggregation,de-identificationanddifferential privacy.[226]Since 2016, some privacy experts, such asCynthia Dwork, have begun to view privacy in terms offairness.Brian Christianwrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\"[227]\nGenerative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\".[228][229]Website owners who do not wish to have their content scraped can indicate it in a \"robots.txt\" file.[230]In 2023, leading authors (includingJohn GrishamandJonathan Franzen) sued AI companies for using their work to train generative AI.[231][232]Another discussed approach is to envision a separatesui generissystem of protection for creations generated by AI to ensure fair attribution and compensation for human authors.[233]\nThe commercial AI scene is dominated byBig Techcompanies such asAlphabet Inc.,Amazon,Apple Inc.,Meta Platforms, andMicrosoft.[234][235][236]Some of these players already own the vast majority of existingcloud infrastructureandcomputingpower fromdata centers, allowing them to entrench further in the marketplace.[237][238]\nIn January 2024, theInternational Energy Agency(IEA) releasedElectricity 2024, Analysis and Forecast to 2026, forecasting electric power use.[239]This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.[240]\nProdigious power consumption by AI is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. The tech firms argue that – in the long view – AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.[241]\nA 2024Goldman SachsResearch Paper,AI Data Centers and the Coming US Power Demand Surge, found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means.[242]Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.[243]\nIn 2024, theWall Street Journalreported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for US$650 million.[244]NvidiaCEOJensen Huangsaid nuclear power is a good option for the data centers.[245]\nIn September 2024,Microsoftannounced an agreement withConstellation Energyto re-open theThree Mile Islandnuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the USNuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power – enough for 800,000 homes – of energy will be produced. The cost for re-opening and upgrading is estimated at US$1.6 billion and is dependent on tax breaks for nuclear power contained in the 2022 USInflation Reduction Act.[246]The US government and the state of Michigan are investing almost US$2 billion to reopen thePalisades Nuclearreactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO ofExelonwho was responsible for Exelon's spinoff of Constellation.[247]\nAfter the last approval in September 2023,Taiwansuspended the approval of data centers north ofTaoyuanwith a capacity of more than 5 MW in 2024, due to power supply shortages.[248]Taiwan aims tophase out nuclear powerby 2025.[248]On the other hand,Singaporeimposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban.[248]\nAlthough most nuclear plants in Japan have been shut down after the 2011Fukushima nuclear accident, according to an October 2024Bloombergarticle in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near a nuclear power plant for a new data center for generative AI.[249]Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI.[249]\nOn 1 November 2024, theFederal Energy Regulatory Commission(FERC) rejected an application submitted byTalen Energyfor approval to supply some electricity from the nuclear power stationSusquehannato Amazon's data center.[250]According to the Commission ChairmanWillie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors.[250]\nIn 2025, a report prepared by the International Energy Agency estimated thegreenhouse gas emissionsfrom the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300–500 million tonnes depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, butrebound effects(for example if people switch from public transport to autonomous cars) can reduce it.[251]\nYouTube,Facebookand others userecommender systemsto guide users to more content. These AI programs were given the goal ofmaximizinguser engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choosemisinformation,conspiracy theories, and extremepartisancontent, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people intofilter bubbleswhere they received multiple versions of the same misinformation.[252]This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government.[253]The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took some steps to mitigate the problem.[254]\nIn the early 2020s,generative AIbegan to create images, audio, and texts that are virtually indistinguishable from real photographs, recordings, or human writing,[255]while realistic AI-generated videos became feasible in the mid-2020s.[256][257][258]It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda;[259]one such potential malicious use is deepfakes forcomputational propaganda.[260]AI pioneerGeoffrey Hintonexpressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks.[261]\nAI researchers atMicrosoft,OpenAI, universities and other organisations have suggested using \"personhood credentials\" as a way to overcome online deception enabled by AI models.[262]\nMachine learning applications will bebiased[k]if they learn from biased data.[264]The developers may not be aware that the bias exists.[265]Bias can be introduced by the waytraining datais selected and by the way a model is deployed.[266][264]If a biased algorithm is used to make decisions that can seriouslyharmpeople (as it can inmedicine,finance,recruitment,housingorpolicing) then the algorithm may causediscrimination.[267]The field offairnessstudies how to prevent harms from algorithmic biases.\nOn June 28, 2015,Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people,[268]a problem called \"sample size disparity\".[269]Google \"fixed\" this problem by preventing the system from labellinganythingas a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.[270]\nCOMPASis a commercial program widely used byU.S. courtsto assess the likelihood of adefendantbecoming arecidivist. In 2016,Julia AngwinatProPublicadiscovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend.[271]In 2017, several researchers[l]showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.[273]\nA program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\".[274]Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\"[275]\nCriticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions asrecommendations, some of these \"recommendations\" will likely be racist.[276]Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will bebetterthan the past. It is descriptive rather than prescriptive.[m]\nBias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.[269]\nThere are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category isdistributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negativestereotypesor render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict withanti-discrimination laws.[263]\nAt its 2022Conference on Fairness, Accountability, and Transparency(ACM FAccT 2022), theAssociation for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.[dubious–discuss][278]\nMany AI systems are so complex that their designers cannot explain how they reach their decisions.[279]Particularly withdeep neural networks, in which there are many non-linearrelationships between inputs and outputs. But some popular explainability techniques exist.[280]\nIt is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with aruleras \"cancerous\", because pictures of malignancies typically include a ruler to show the scale.[281]Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.[282]\nPeople who have been harmed by an algorithm's decision have a right to an explanation.[283]Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union'sGeneral Data Protection Regulationin 2016 included an explicit statement that this right exists.[n]Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.[284]\nDARPAestablished theXAI(\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems.[285]\nSeveral approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output.[286]LIME can locally approximate a model's outputs with a simpler, interpretable model.[287]Multitask learningprovides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned.[288]Deconvolution,DeepDreamand othergenerativemethods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning.[289]Forgenerative pre-trained transformers,Anthropicdeveloped a technique based ondictionary learningthat associates patterns of neuron activations with human-understandable concepts.[290]\nArtificial intelligence provides a number of tools that are useful tobad actors, such asauthoritarian governments,terrorists,criminalsorrogue states.\nA lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision.[o]Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentiallyweapons of mass destruction.[292]Even when used in conventional warfare, they currently cannot reliably choose targets and could potentiallykill an innocent person.[292]In 2014, 30 nations (including China) supported a ban on autonomous weapons under theUnited Nations'Convention on Certain Conventional Weapons, however theUnited Statesand others disagreed.[293]By 2015, over fifty countries were reported to be researching battlefield robots.[294]\nAI tools make it easier forauthoritarian governmentsto efficiently control their citizens in several ways.Faceandvoice recognitionallow widespreadsurveillance.Machine learning, operating this data, canclassifypotential enemies of the state and prevent them from hiding.Recommendation systemscan precisely targetpropagandaandmisinformationfor maximum effect.Deepfakesandgenerative AIaid in producing misinformation. Advanced AI can make authoritariancentralized decision-makingmore competitive than liberal and decentralized systems such asmarkets. It lowers the cost and difficulty ofdigital warfareandadvanced spyware.[295]All these technologies have been available since 2020 or earlier—AIfacial recognition systemsare already being used formass surveillancein China.[296][297]\nThere are many other ways in which AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.[298]\nEconomists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.[299]\nIn the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI.[300]A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-termunemployment, but they generally agree that it could be a net benefit ifproductivitygains areredistributed.[301]Risk estimates vary; for example, in the 2010s, Michael Osborne andCarl Benedikt Freyestimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\".[p][303]The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies.[299]In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.[304][305]\nUnlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence;The Economiststated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\".[306]Jobs at extreme risk range fromparalegalsto fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.[307]\nFrom the early days of the development of artificial intelligence, there have been arguments, for example, those put forward byJoseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.[308]\nIt has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicistStephen Hawkingstated, \"spell the end of the human race\".[309]This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character.[q]These sci-fi scenarios are misleading in several ways.\nFirst, AI does not require human-likesentienceto be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. PhilosopherNick Bostromargued that if one givesalmost anygoal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of apaperclip maximizer).[311]Stuart Russellgives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\"[312]In order to be safe for humanity, asuperintelligencewould have to be genuinelyalignedwith humanity's morality and values so that it is \"fundamentally on our side\".[313]\nSecond,Yuval Noah Harariargues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things likeideologies,law,government,moneyand theeconomyare built onlanguage; they exist because there are stories that billions of people believe. The current prevalence ofmisinformationsuggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.[314]\nThe opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI.[315]Personalities such asStephen Hawking,Bill Gates, andElon Musk,[316]as well as AI pioneers such asYoshua Bengio,Stuart Russell,Demis Hassabis, andSam Altman, have expressed concerns about existential risk from AI.\nIn May 2023,Geoffrey Hintonannounced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\".[317]He notably mentioned risks of anAI takeover,[318]and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.[319]\nIn 2023, many leading AI experts endorsedthe joint statementthat \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".[320]\nSome other researchers were more optimistic. AI pioneerJürgen Schmidhuberdid not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\"[321]While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\"[322][323]Andrew Ngalso argued that \"it's a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests.\"[324]Yann LeCun\"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\"[325]In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine.[326]However, after 2016, the study of current and future risks and possible solutions became a serious area of research.[327]\nFriendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans.Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.[328]\nMachines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.[329]The field of machine ethics is also called computational morality,[329]and was founded at anAAAIsymposium in 2005.[330]\nOther approaches includeWendell Wallach's \"artificial moral agents\"[331]andStuart J. Russell'sthree principlesfor developing provably beneficial machines.[332]\nActive organizations in the AI open-source community includeHugging Face,[333]Google,[334]EleutherAIandMeta.[335]Various AI models, such asLlama 2,MistralorStable Diffusion, have been made open-weight,[336][337]meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freelyfine-tuned, which allows companies to specialize them with their own data and for their own use-case.[338]Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitatebioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.[339]\nArtificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by theAlan Turing Instituteand based on the SUM values, outlines four main ethical dimensions, defined as follows:[340][341]\nOther developments in ethical frameworks include those decided upon during theAsilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others;[342]however, these principles are not without criticism, especially regarding the people chosen to contribute to these frameworks.[343]\nPromotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.[344]\nTheUK AI Safety Institutereleased in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under an MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.[345]\nThe regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms.[346]The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally.[347]According to AI Index atStanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.[348][349]Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.[350]Most EU member states had released national AI strategies, as hadCanada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.[350]TheGlobal Partnership on Artificial Intelligencewas launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.[350]Henry Kissinger,Eric Schmidt, andDaniel Huttenlocherpublished a joint statement in November 2021 calling for a government commission to regulate AI.[351]In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.[352]In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, government officials and academics.[353]On 1 August 2024, the EUArtificial Intelligence Actentered into force, establishing the first comprehensive EU-wide AI regulation.[354]In 2024, theCouncil of Europecreated the first international legally binding treaty on AI, called the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.[355]\nIn a 2022Ipsossurvey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\".[348]A 2023Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity.[356]In a 2023Fox Newspoll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".[357][358]\nIn November 2023, the first globalAI Safety Summitwas held inBletchley Parkin the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks.[359]28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence.[360][361]In May 2024 at theAI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.[362][363]\nThe study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly toAlan Turing'stheory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning.[365][366]This, along with concurrent discoveries incybernetics,information theoryandneurobiology, led researchers to consider the possibility of building an \"electronic brain\".[r]They developed several areas of research that would become part of AI,[368]such asMcCullochandPittsdesign for \"artificial neurons\" in 1943,[117]and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced theTuring testand showed that \"machine intelligence\" was plausible.[369][366]\nThe field of AI research was founded ata workshopatDartmouth Collegein 1956.[s][6]The attendees became the leaders of AI research in the 1960s.[t]They and their students produced programs that the press described as \"astonishing\":[u]computers were learningcheckersstrategies, solving word problems in algebra, provinglogical theoremsand speaking English.[v][7]Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.[366]\nResearchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine withgeneral intelligenceand considered this the goal of their field.[373]In 1965Herbert Simonpredicted, \"machines will be capable, within twenty years, of doing any work a man can do\".[374]In 1967Marvin Minskyagreed, writing that \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\".[375]They had, however, underestimated the difficulty of the problem.[w]In 1974, both the U.S. and British governments cut off exploratory research in response to thecriticismofSir James Lighthill[377]and ongoing pressure from the U.S. Congress tofund more productive projects.[378]MinskyandPapert's bookPerceptronswas understood as proving thatartificial neural networkswould never be useful for solving real-world tasks, thus discrediting the approach altogether.[379]The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.[9]\nIn the early 1980s, AI research was revived by the commercial success ofexpert systems,[380]a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan'sfifth generation computerproject inspired the U.S. and British governments to restore funding foracademic research.[8]However, beginning with the collapse of theLisp Machinemarket in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.[10]\nUp to this point, most of AI's funding had gone to projects that used high-levelsymbolsto representmental objectslike plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especiallyperception,robotics,learningandpattern recognition,[381]and began to look into \"sub-symbolic\" approaches.[382]Rodney Brooksrejected \"representation\" in general and focussed directly on engineering machines that move and survive.[x]Judea Pearl,Lotfi Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic.[87][387]But the most important development was the revival of \"connectionism\", including neural network research, byGeoffrey Hintonand others.[388]In 1990,Yann LeCunsuccessfully showed thatconvolutional neural networkscan recognize handwritten digits, the first of many successful applications of neural networks.[389]\nAI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such asstatistics,economicsandmathematics).[390]By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as theAI effect).[391]However, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield ofartificial general intelligence(or \"AGI\"), which had several well-funded institutions by the 2010s.[68]\nDeep learningbegan to dominate industry benchmarks in 2012 and was adopted throughout the field.[11]For many specific tasks, other methods were abandoned.[y]Deep learning's success was based on both hardware improvements (faster computers,[393]graphics processing units,cloud computing[394]) and access tolarge amounts of data[395](including curated datasets,[394]such asImageNet). Deep learning's success led to an enormous increase in interest and funding in AI.[z]The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019.[350]\nIn 2016, issues offairnessand the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. Thealignment problembecame a serious field of academic study.[327]\nIn the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015,AlphaGo, developed byDeepMind, beat the world championGo player. The program taught only the game's rules and developed a strategy by itself.GPT-3is alarge language modelthat was released in 2020 byOpenAIand is capable of generating high-quality human-like text.[396]ChatGPT, launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months.[397]It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness.[398]These programs, and others, inspired an aggressiveAI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\".[399]About 800,000 \"AI\"-related U.S. job openings existed in 2022.[400]According to PitchBook research, 22% of newly fundedstartupsin 2024 claimed to be AI companies.[401]\nPhilosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines.[402]Another major focus has been whether machines can be conscious, and the associated ethical implications.[403]Many other topics in philosophy are relevant to AI, such asepistemologyandfree will.[404]Rapid advancements have intensified public discussions on the philosophy andethics of AI.[403]\nAlan Turingwrote in 1950 \"I propose to consider the question 'can machines think'?\"[405]He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\".[405]He devised theTuring test, which measures the ability of a machine to simulate human conversation.[369]Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes thatwe can not determine these things about other peoplebut \"it is usual to have a polite convention that everyone thinks.\"[406]\nRussellandNorvigagree with Turing that intelligence must be defined in terms of external behavior, not internal structure.[1]However, they are critical that the test requires the machine to imitate humans. \"Aeronautical engineeringtexts\", they wrote, \"do not define the goal of their field as making 'machines that fly so exactly likepigeonsthat they can fool other pigeons.'\"[408]AI founderJohn McCarthyagreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".[409]\nMcCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\".[410]Another AI founder,Marvin Minsky, similarly describes it as \"the ability to solve hard problems\".[411]The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals.[1]These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine—and no other philosophical discussion is required, or may not even be possible.\nAnother definition has been adopted by Google,[412]a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\nAs a result of the many circulating definitions scholars have started to critically analyze and order the AI discourse itself[413]including discussing the many AI narratives and myths to be found within societal, political and academic discourses.[414]Similarly, in practice, some authors have suggested that the term 'AI' is often used too broadly and vaguely. This raises the question of where the line should be drawn between AI and classical algorithms,[415]with many companies during the early 2020s AI boom using the term as a marketingbuzzword, often even if they did \"not actually use AI in a material way\".[416]\nThere has been debate over whetherlarge language modelsexhibit genuine intelligence or merely simulate it byimitating human text.[417]\nNo established unifying theory orparadigmhas guided AI research for most of its history.[aa]The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostlysub-symbolic,softandnarrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.\nSymbolic AI(or \"GOFAI\")[419]simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed thephysical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"[420]\nHowever, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object orcommonsense reasoning.Moravec's paradoxis the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult.[421]PhilosopherHubert Dreyfushadarguedsince the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge.[422]Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.[ab][16]\nThe issue is not resolved:sub-symbolicreasoning can make many of the same inscrutable mistakes that human intuition does, such asalgorithmic bias. Critics such asNoam Chomskyargue continuing research into symbolic AI will still be necessary to attain general intelligence,[424][425]in part because sub-symbolic AI is a move away fromexplainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field ofneuro-symbolic artificial intelligenceattempts to bridge the two approaches.\n\"Neats\" hope that intelligent behavior is described using simple, elegant principles (such aslogic,optimization, orneural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s,[426]but eventually was seen as irrelevant. Modern AI has elements of both.\nFinding a provably correct or optimal solution isintractablefor many important problems.[15]Soft computing is a set of techniques, includinggenetic algorithms,fuzzy logicand neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\nAI researchers are divided as to whether to pursue the goals of artificial general intelligence andsuperintelligencedirectly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.[427][428]General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively.\nThere is no settled consensus inphilosophy of mindon whether a machine can have amind,consciousnessandmental statesin the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence.RussellandNorvigadd that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\"[429]However, the question has become central to the philosophy of mind. It is also typically the central question at issue inartificial intelligence in fiction.\nDavid Chalmersidentified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness.[430]The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how thisfeelsor why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While humaninformation processingis easy to explain, humansubjective experienceis difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person toknow what red looks like.[431]\nComputationalism is the position in thephilosophy of mindthat the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to themind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophersJerry FodorandHilary Putnam.[432]\nPhilosopherJohn Searlecharacterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"[ac]Searle challenges this claim with hisChinese roomargument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind.[436]\nIt is difficult or impossible to reliably evaluate whether an advancedAI is sentient(has the ability to feel), and if so, to what degree.[437]But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals.[438][439]Sapience(a set of capacities related to high intelligence, such as discernment orself-awareness) may provide another moral basis for AI rights.[438]Robot rightsare also sometimes proposed as a practical way to integrate autonomous agents into society.[440]\nIn 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities.[441]Critics argued in 2018 that granting rights to AI systems would downplay the importance ofhuman rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part in society on their own.[442][443]\nProgress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be amoral blind spotanalogous toslaveryorfactory farming, which could lead tolarge-scale sufferingif sentient AI is created and carelessly exploited.[439][438]\nAsuperintelligenceis a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.[428]If research intoartificial general intelligenceproduced sufficiently intelligent software, it might be able toreprogram and improve itself. The improved software would be even better at improving itself, leading to whatI. J. Goodcalled an \"intelligence explosion\" andVernor Vingecalled a \"singularity\".[444]\nHowever, technologies cannot improve exponentially indefinitely, and typically follow anS-shaped curve, slowing when they reach the physical limits of what the technology can do.[445]\nRobot designerHans Moravec, cyberneticistKevin Warwickand inventorRay Kurzweilhave predicted that humans and machines may merge in the future intocyborgsthat are more capable and powerful than either. This idea, called transhumanism, has roots in the writings ofAldous HuxleyandRobert Ettinger.[446]\nEdward Fredkinargues that \"artificial intelligence is the next step in evolution\", an idea first proposed bySamuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon byGeorge Dysonin his 1998 bookDarwin Among the Machines: The Evolution of Global Intelligence.[447]\nThought-capable artificial beings have appeared as storytelling devices since antiquity,[448]and have been a persistent theme inscience fiction.[449]\nA commontropein these works began withMary Shelley'sFrankenstein, where a human creation becomes a threat to its masters. This includes such works asArthur C. Clarke'sandStanley Kubrick's2001: A Space Odyssey(both 1968), withHAL 9000, the murderous computer in charge of theDiscovery Onespaceship, as well asThe Terminator(1984) andThe Matrix(1999). In contrast, the rare loyal robots such as Gort fromThe Day the Earth Stood Still(1951) and Bishop fromAliens(1986) are less prominent in popular culture.[450]\nIsaac Asimovintroduced theThree Laws of Roboticsin many stories, most notably with the \"Multivac\" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics;[451]while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[452]\nSeveral works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that havethe ability to feel, and thus to suffer. This appears inKarel Čapek'sR.U.R., the filmsA.I. Artificial IntelligenceandEx Machina, as well as the novelDo Androids Dream of Electric Sheep?, byPhilip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.[453]\nThe two most widely used textbooks in 2023 (see theOpen Syllabus):\nThe four most widely used AI textbooks in 2008:\nOther textbooks:"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Main_Page",
        "title": "Wikipedia, the free encyclopedia",
        "content": "October 3\nThe position ofMarshal Foch Professor of French Literatureat theUniversity of Oxfordwas founded in 1918 shortly after the end of the First World War.Ferdinand Foch, or \"Marshal Foch\"(pictured), was supreme commander of Allied forces from April 1918 onwards. The chair was endowed by an arms trader,Basil Zaharoff, in Foch's honour; he also endowed a post in English Literature at theUniversity of Parisin honour of the British Field MarshalEarl Haig. Zaharoff wanted the University of Paris to have a right of veto over the appointment, but Oxford would not accept this. The compromise reached was that Paris should have a representative on the appointing committee (although this provision was later removed). In advance of the first election,Stéphen Pichon(the French Foreign Minister) unsuccessfully attempted to influence the decision.  The first professor,Gustave Rudler, was appointed in 1920. (Full list...)\nTheImmigration and Nationality Act of 1965is aUnited States federal lawthat was passed bythe 89th Congressand signed into law by PresidentLyndon B. Johnsonon October 3, 1965. The act formally removed de facto discrimination against people of various ethnicities fromthe country's immigration policyand created a system giving priority to various categories of people such as relatives of US citizens, skilled professionals, and refugees. Previous policy consisted of theNational Origins Formulaof the 1920s, whose aim was to preserve American homogeneity by promoting immigration from Western and Northern Europe, an approach which came under attack during thecivil rights movementfor beingracially discriminatory. This photograph shows President Johnson officially signing the Immigration and Nationality Act in a ceremony onLiberty Islandin New York City.\nPhotograph credit:Yoichi Okamoto\nWikipedia is written by volunteer editors and hosted by theWikimedia Foundation, a non-profit organization that also hosts a range of other volunteerprojects:\nThis Wikipedia is written inEnglish. Manyother Wikipedias are available; some of the largest are listed below."
    },
    {
        "url": "https://en.wikipedia.org/wiki/AI_(disambiguation)",
        "title": "Ai - Wikipedia",
        "content": "AImost frequently refers toartificial intelligence, which is intelligence demonstrated by machines.\nAi,AIorA.I.may also refer to:"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_(disambiguation)",
        "title": "Artificial intelligence (disambiguation) - Wikipedia",
        "content": "Artificial intelligenceis the intelligence exhibited by machines and software.\nArtificial intelligencemay also refer to:"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence - Wikipedia",
        "content": "Artificial general intelligence(AGI)—sometimes calledhuman‑level intelligence AI—is a type ofartificial intelligencethat would match or surpass human capabilities across virtually all cognitive tasks.[1][2]\nSome researchers argue that state‑of‑the‑artlarge language models(LLMs) already exhibit signs of AGI‑level capability, while others maintain that genuine AGI has not yet been achieved.[3]Beyond AGI,artificial superintelligence(ASI) would outperform the best human abilities across every domain by a wide margin.[4]\nUnlikeartificial narrow intelligence(ANI), whose competence is confined to well‑defined tasks, an AGI system can generalise knowledge, transfer skills between domains, and solve novel problems without task‑specific reprogramming. The concept does not, in principle, require the system to be an autonomous agent; a static model—such as a highly capable large language model—or an embodied robot could both satisfy the definition so long as human‑level breadth and proficiency are achieved.[5]\nCreating AGI is a primary goal of AI research and of companies such asOpenAI,[6]Google,[7]xAI,[8]andMeta.[9]A 2020 survey identified 72 active AGIresearch and developmentprojects across 37 countries.[10]\nThe timeline for achieving human‑level intelligence AI remains deeply contested. Recent surveys of AI researchers give median forecasts ranging from the late 2020s to mid‑century, while still recording significant numbers who expect arrival much sooner—or never at all.[11][12][13]There is debate on the exact definition of AGI and regarding whether modern LLMs such asGPT-4are early forms of emerging AGI.[3]AGI is a common topic inscience fictionandfutures studies.[14][15]\nContention exists over whether AGIrepresents an existential risk.[16][17][18]Many AI expertshave statedthat mitigating the risk of human extinction posed by AGI should be a global priority.[19][20]Others find the development of AGI to be in too remote a stage to present such a risk.[21][22]\nAGI is also known as strong AI,[23][24]full AI,[25]human-level AI,[26]human-level intelligent AI, or general intelligent action.[27]\nSome academic sources reserve the term \"strong AI\" for computer programs that will experiencesentienceorconsciousness.[a]In contrast, weak AI (or narrow AI) is able to solve one specific problem but lacks general cognitive abilities.[28][24]Some academic sources use \"weak AI\" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans.[a]\nRelated concepts include artificialsuperintelligenceand transformative AI. An artificial superintelligence (ASI) is a hypothetical type of AGI that is much more generally intelligent than humans,[29]while the notion of transformative AI relates to AI having a large impact on society, for example, similar to the agricultural or industrial revolution.[30]\nA framework for classifying AGI by performance and autonomy was proposed in 2023 byGoogle DeepMindresearchers.[31]They define five performance levels of AGI: emerging, competent, expert, virtuoso, and superhuman.[31]For example, a competent AGI is defined as an AI that outperforms 50% of skilled adults in a wide range of non-physical tasks, and a superhuman AGI (i.e. an artificial superintelligence) is similarly defined but with a threshold of 100%.[31]They consider large language models likeChatGPTorLLaMA 2to be instances of emerging AGI (comparable to unskilled humans).[31]Regarding the autonomy of AGI and associated risks, they define five levels: tool (fully in human control), consultant, collaborator, expert, and agent (fully autonomous).[32]\nVarious popular definitions ofintelligencehave been proposed. One of the leading proposals is theTuring test. However, there are other well-known definitions, and some researchers disagree with the more popular approaches.[b]\nResearchers generally hold that a system is required to do all of the following to be regarded as an AGI:[34]\nManyinterdisciplinaryapproaches (e.g.cognitive science,computational intelligence, anddecision making) consider additional traits such asimagination(the ability to form novel mental images and concepts)[35]andautonomy.[36]\nComputer-based systems that exhibit many of these capabilities exist (e.g. seecomputational creativity,automated reasoning,decision support system,robot,evolutionary computation,intelligent agent). There is debate about whether modern AI systems possess them to an adequate degree.[37]\nOther capabilities are considered desirable in intelligent systems, as they may affect intelligence or aid in its expression. These include:[38]\nThis includes the ability to detect and respond tohazard.[39]\nAlthough the ability to sense (e.g.see, hear, etc.) and the ability to act (e.g.move and manipulate objects, change location to explore, etc.) can be desirable for some intelligent systems,[38]these physical capabilities are not strictly required for an entity to qualify as AGI—particularly under the thesis that large language models (LLMs) may already be or become AGI. Even from a less optimistic perspective on LLMs, there is no firm requirement for an AGI to have a human-like form; being a silicon-based computational system is sufficient, provided it can process input (language) from the external world in place of human senses. This interpretation aligns with the understanding that AGI has never been proscribed a particular physical embodiment and thus does not demand a capacity for locomotion or traditional \"eyes and ears\".[39]It can be regarded as sufficient for an intelligent computer tointeract with other systems, to invoke or regulate them, to achieve specific goals, including altering a physical environment, as the fictionalHAL 9000in the motion picture2001: A Space Odysseywas both programmed and tasked to.[40]\nSeveral tests meant to confirm human-level AGI have been considered, including:[41][42]\nThe idea of the test is that the machine has to try and pretend to be a man, by answering questions put to it, and it will only pass if the pretence is reasonably convincing. A considerable portion of a jury, who should not be expert about machines, must be taken in by the pretence.[45]\nA problem is informally called \"AI-complete\" or \"AI-hard\" if it is believed that in order to solve it, one would need to implement AGI, because the solution is beyond the capabilities of a purpose-specific algorithm.[58]\nThere are many problems that have been conjectured to require general intelligence to solve as well as humans. Examples includecomputer vision,natural language understanding, and dealing with unexpected circumstances while solving any real-world problem.[59]Even a specific task liketranslationrequires a machine to read and write in both languages, follow the author's argument (reason), understand the context (knowledge), and faithfully reproduce the author's original intent (social intelligence). All of these problems need to be solved simultaneously in order to reach human-level machine performance.\nHowever, many of these tasks can now be performed by modern large language models. According toStanford University's 2024 AI index, AI has reached human-level performance on manybenchmarksfor reading comprehension and visual reasoning.[60]\nModern AI research began in the mid-1950s.[61]The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades.[62]AI pioneerHerbert A. Simonwrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do.\"[63]\nTheir predictions were the inspiration forStanley KubrickandArthur C. Clarke's fictional characterHAL 9000, who embodied what AI researchers believed they could create by the year 2001. AI pioneerMarvin Minskywas a consultant[64]on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time. He said in 1967, \"Within a generation... the problem of creating 'artificial intelligence' will substantially be solved\".[65]\nSeveralclassical AI projects, such asDoug Lenat'sCycproject (that began in 1984), andAllen Newell'sSoarproject, were directed at AGI.\nHowever, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful \"applied AI\".[c]In the early 1980s, Japan'sFifth Generation ComputerProject revived interest in AGI, setting out a ten-year timeline that included AGI goals like \"carry on a casual conversation\".[69]In response to this and the success ofexpert systems, both industry and government pumped money into the field.[67][70]However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled.[71]For the second time in 20 years, AI researchers who predicted the imminent achievement of AGI had been mistaken. By the 1990s, AI researchers had a reputation for making vain promises. They became reluctant to make predictions at all[d]and avoided mention of \"human level\" artificial intelligence for fear of being labeled \"wild-eyed dreamer[s]\".[73]\nIn the 1990s and early 21st century, mainstream AI achieved commercial success and academic respectability by focusing on specific sub-problems where AI can produce verifiable results and commercial applications, such asspeech recognitionandrecommendation algorithms.[74]These \"applied AI\" systems are now used extensively throughout the technology industry, and research in this vein is heavily funded in both academia and industry. As of 2018[update], development in this field was considered an emerging trend, and a mature stage was expected to be reached in more than 10 years.[75]\nAt the turn of the century, many mainstream AI researchers[76]hoped that strong AI could be developed by combining programs that solve various sub-problems.Hans Moravecwrote in 1988:\nI am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than half way, ready to provide the real-world competence and thecommonsense knowledgethat has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphoricalgolden spikeis driven uniting the two efforts.[76]\nHowever, even at the time, this was disputed. For example,Stevan Harnadof Princeton University concluded his 1990 paper on thesymbol grounding hypothesisby stating:\nThe expectation has often been voiced that \"top-down\" (symbolic) approaches to modeling cognition will somehow meet \"bottom-up\" (sensory) approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols: from the ground up. A free-floating symbolic level like the software level of a computer will never be reached by this route (or vice versa) – nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings (thereby merely reducing ourselves to the functional equivalent of a programmable computer).[77]\nThe term \"artificial general intelligence\" was used as early as 1997, by Mark Gubrud[78]in a discussion of the implications of fully automated military production and operations. A mathematical formalism of AGI was proposed byMarcus Hutterin 2000. NamedAIXI, the proposed AGI agent maximises \"the ability to satisfy goals in a wide range of environments\".[79]This type of AGI, characterized by the ability to maximise a mathematical definition of intelligence rather than exhibit human-like behaviour,[80]was also called universal artificial intelligence.[81]\nThe term AGI was re-introduced and popularized byShane LeggandBen Goertzelaround 2002.[82]AGI research activity in 2006 was described by Pei Wang and Ben Goertzel[83]as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009[84]by the Xiamen university's Artificial Brain Laboratory andOpenCog. The first university course was given in 2010[85]and 2011[86]at Plovdiv University, Bulgaria by Todor Arnaudov. The Massachusetts Institute of Technology (MIT) presented a course on AGI in 2018, organized byLex Fridmanand featuring a number of guest lecturers.\nAs of 2023[update], a small number of computer scientists are active in AGI research, and many contribute to a series of AGI conferences. However, increasingly more researchers are interested in open-ended learning,[87][3]which is the idea of allowing AI to continuously learn and innovate like humans do.\nAs of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist.[88]AI pioneerHerbert A. Simonspeculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true.Microsoftco-founderPaul Allenbelieved that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\".[89]Writing inThe Guardian, roboticistAlan Winfieldclaimed in 2014 that the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight.[90]\nA further challenge is the lack of clarity in defining whatintelligenceentails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions?[91]\nMost AI researchers believe strong AI can be achieved in the future, but some thinkers, likeHubert DreyfusandRoger Penrose, deny the possibility of achieving strong AI.[92][93]John McCarthyis among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted.[94]AI experts' views on the feasibility of AGI wax and wane. Four polls conducted in 2012 and 2013 suggested that the median estimate among experts for when they would be 50% confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. Of the experts, 16.5% answered with \"never\" when asked the same question but with a 90% confidence instead.[95][96]Further current AGI progress considerations can be found aboveTests for confirming human-level AGI.\nA report by Stuart Armstrong and Kaj Sotala of theMachine Intelligence Research Institutefound that \"over [a] 60-year time frame there is a strong bias towards predicting the arrival of human-level AI as between 15 and 25 years from the time the prediction was made\". They analyzed 95 predictions made between 1950 and 2012 on when human-level AI will come about.[97]\nIn 2023,Microsoftresearchers published a detailed evaluation ofGPT-4. They concluded: \"Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\"[98]Another study in 2023 reported that GPT-4 outperforms 99% of humans on theTorrance tests of creative thinking.[99][100]\nBlaise Agüera y ArcasandPeter Norvigwrote in 2023 the article \"Artificial General Intelligence Is Already Here\", arguing thatfrontier modelshad already achieved a significant level of general intelligence. They wrote that reluctance to this view comes from four main reasons: a \"healthy skepticism about metrics for AGI\", an \"ideological commitment to alternative AI theories or techniques\", a \"devotion to human (or biological) exceptionalism\", or a \"concern about the economic implications of AGI\".[101]\n2023 also marked the emergence of large multimodal models (large language models capable of processing or generating multiplemodalitiessuch as text, audio, and images).[102]As of 2025,large language models(LLMs) have been adapted to generate both music and images. Voice‑synthesis systems built on transformer LLMs—such asSuno AI'sBarkmodel—can sing, and several music‑generation platforms (e.g. Suno andUdio) build their services on modified LLM backbones.[103][104]\nThe same year,OpenAIreleasedGPT‑4oimage generation, integrating native image synthesis directly intoChatGPTrather than relying on a separatediffusion‑based art model, as withDALL-E.[105]\nLLM‑style foundation models are likewise being repurposed for robotics.Nvidia's open‑sourceIsaac GR00T N1and Google DeepMind'sRobotic Transformer 2(RT‑2) are first trained with language‑model objectives and then fine‑tuned to handle vision‑language‑action control for embodied robots.[106][107][108]\nIn 2024, OpenAI releasedo1-preview, the first of a series of models that \"spend more time thinking before they respond\". According toMira Murati, this ability to think before responding represents a new, additional paradigm. It improves model outputs by spending more computing power when generating the answer, whereas the model scaling paradigm improves outputs by increasing the model size, training data and training compute power.[109][110]\nAnOpenAIemployee, Vahid Kazemi, claimed in 2024 that the company had achieved AGI, stating, \"In my opinion, we have already achieved AGI and it's even more clear withO1.\" Kazemi clarified that while the AI is not yet \"better than any human at any task\", it is \"better than most humans at most tasks.\" He also addressed criticisms that large language models (LLMs) merely follow predefined patterns, comparing their learning process to the scientific method of observing, hypothesizing, and verifying. These statements have sparked debate, as they rely on a broad and unconventional definition of AGI—traditionally understood as AI that matches human intelligence across all domains. Critics argue that, while OpenAI's models demonstrate remarkable versatility, they may not fully meet this standard. Notably, Kazemi's comments came shortly after OpenAI removed \"AGI\" from the terms of its partnership with Microsoft, prompting speculation about the company's strategic intentions.[111]\nProgress in artificial intelligence has historically gone through periods of rapid progress separated by periods when progress appeared to stop.[92]Ending each hiatus were fundamental advances in hardware, software or both to create space for further progress.[92][114][115]For example, the computer hardware available in the twentieth century was not sufficient to implementdeep learning, which requires large numbers ofGPU-enabledCPUs.[116]\nIn the introduction to his 2006 book,[117]Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century. As of 2007[update], the consensus in the AGI research community seemed to be that the timeline discussed byRay Kurzweilin 2005 inThe Singularity is Near[118](i.e. between 2015 and 2045) was plausible.[119]Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta-analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16–26 years for modern and historical predictions alike. That paper has been criticized for how it categorized opinions as expert or non-expert.[120]\nIn 2012,Alex Krizhevsky,Ilya Sutskever, andGeoffrey Hintondeveloped a neural network calledAlexNet, which won theImageNetcompetition with a top-5 test error rate of 15.3%, significantly better than the second-best entry's rate of 26.3% (the traditional approach used a weighted sum of scores from different pre-defined classifiers).[121]AlexNet was regarded as the initial ground-breaker of the current deep learning wave.[121]\nIn 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI,Apple'sSiri, and others. At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six-year-old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27.[122][123]\nIn 2020,OpenAIdevelopedGPT-3, a language model capable of performing many diverse tasks without specific training. According toGary Grossmanin aVentureBeatarticle, while there is consensus that GPT-3 is not an example of AGI, it is considered by some to be too advanced to be classified as a narrow AI system.[124]\nIn the same year, Jason Rohrer used his GPT-3 account to develop a chatbot, and provided a chatbot-developing platform called \"Project December\". OpenAI asked for changes to the chatbot to comply with their safety guidelines; Rohrer disconnected Project December from the GPT-3 API.[125]\nIn 2022, DeepMind developedGato, a \"general-purpose\" system capable of performing more than 600 different tasks.[126]\nIn 2023,Microsoft Researchpublished a study on an early version of OpenAI'sGPT-4, contending that it exhibited more general intelligence than previous AI models and demonstrated human-level performance in tasks spanning multiple domains, such as mathematics, coding, and law. This research sparked a debate on whether GPT-4 could be considered an early, incomplete version of artificial general intelligence, emphasizing the need for further exploration and evaluation of such systems.[3]\nIn 2023, AI researcherGeoffrey Hintonstated that:[127]\nThe idea that this stuff could actually get smarter than people – a few people believed that, [...]. But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.\nHe estimated in 2024 (with low confidence) that systems smarter than humans could appear within 5 to 20 years and stressed the attendant existential risks.[128]\nIn May 2023,Demis Hassabissimilarly said that \"The progress in the last few years has been pretty incredible\", and that he sees no reason why it would slow, expecting AGI within a decade or even a few years.[129]In March 2024,Nvidia's Chief Executive Officer (CEO),Jensen Huang, stated his expectation that within five years, AI would be capable of passing any test at least as well as humans.[130]In June 2024, the AI researcherLeopold Aschenbrenner, a formerOpenAIemployee, estimated AGI by 2027 to be \"strikingly plausible\".[131]\nIn September 2025, a review of surveys of scientists and industry experts from the last 15 years reported that most agreed that artificial general intelligence (AGI) will occur before the year 2100.[132]A more recent analysis by AIMultiple reported that, “Current surveys of AI researchers are predicting AGI around 2040”.[132]\nWhile the development oftransformermodels like inChatGPTis considered the most promising path to AGI,[133][134]whole brain emulationcan serve as an alternative approach. With whole brain simulation, a brain model is built byscanningandmappinga biological brain in detail, and then copying and simulating it on a computer system or another computational device. Thesimulationmodel must be sufficiently faithful to the original, so that it behaves in practically the same way as the original brain.[135]Whole brain emulation is a type ofbrain simulationthat is discussed incomputational neuroscienceandneuroinformatics, and for medical research purposes. It has been discussed inartificial intelligenceresearch[119]as an approach to strong AI.Neuroimagingtechnologies that could deliver the necessary detailed understanding are improving rapidly, andfuturistRay Kurzweilin the bookThe Singularity Is Near[118]predicts that a map of sufficient quality will become available on a similar timescale to the computing power required to emulate it.\nFor low-level brain simulation, a very powerful cluster of computers or GPUs would be required, given the enormous quantity ofsynapseswithin thehuman brain. Each of the 1011(one hundred billion)neuronshas on average 7,000 synaptic connections (synapses) to other neurons. The brain of a three-year-old child has about 1015synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 1014to 5×1014synapses (100 to 500 trillion).[137]An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 1014(100 trillion) synaptic updates per second (SUPS).[138]\nIn 1997, Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 1016computations per second.[e](For comparison, if a \"computation\" was equivalent to one \"floating-point operation\" – a measure used to rate currentsupercomputers– then 1016\"computations\" would be equivalent to 10petaFLOPS,achieved in 2011, while 1018wasachieved in 2022.) He used this figure to predict the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued.\nTheHuman Brain Project, anEU-funded initiative active from 2013 to 2023, has developed a particularly detailed and publicly accessibleatlasof the human brain.[141]In 2023, researchers from Duke University performed a high-resolution scan of a mouse brain.\nTheartificial neuronmodel assumed by Kurzweil and used in many currentartificial neural networkimplementations is simple compared withbiological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biologicalneurons, presently understood only in broad outline. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition, the estimates do not account forglial cells, which are known to play a role in cognitive processes.[142]\nA fundamental criticism of the simulated brain approach derives fromembodied cognitiontheory which asserts that human embodiment is an essential aspect of human intelligence and is necessary to ground meaning.[143][144]If this theory is correct, any fully functional brain model will need to encompass more than just the neurons (e.g., a robotic body). Goertzel[119]proposes virtual embodiment (like inmetaverseslikeSecond Life) as an option, but it is unknown whether this would be sufficient.\nIn 1980, philosopherJohn Searlecoined the term \"strong AI\" as part of hisChinese roomargument.[145]He proposed a distinction between two hypotheses about artificial intelligence:[f]\nThe first one he called \"strong\" because it makes astrongerstatement: it assumes something special has happened to the machine that goes beyond those abilities that we can test. The behaviour of a \"weak AI\" machine would be identical to a \"strong AI\" machine, but the latter would also have subjective conscious experience. This usage is also common in academic AI research and textbooks.[146]\nIn contrast to Searle and mainstream AI, some futurists such as Ray Kurzweil use the term \"strong AI\" to mean \"human level artificial general intelligence\".[118]This is not the same as Searle'sstrong AI, unless it is assumed thatconsciousnessis necessary for human-level AGI. Academic philosophers such as Searle do not believe that is the case, and to most artificial intelligence researchers the question is out-of-scope.[147]\nMainstream AI is most interested in how a programbehaves.[148]According toRussellandNorvig, \"as long as the program works, they don't care if you call it real or a simulation.\"[147]If the program can behaveas ifit has a mind, then there is no need to know if itactuallyhas mind – indeed, there would be no way to tell. For AI research, Searle's \"weak AI hypothesis\" is equivalent to the statement \"artificial general intelligence is possible\". Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\"[147]Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two different things.\nConsciousness can have various meanings, and some aspects play significant roles in science fiction and theethics of artificial intelligence:\nThese traits have a moral dimension. AI sentience would give rise to concerns of welfare and legal protection, similarly to animals.[153]Other aspects of consciousness related to cognitive capabilities are also relevant to the concept of AI rights.[154]Figuring out how to integrate advanced AI with existing legal and social frameworks is an emergent issue.[155]\nAGI could improve productivity and efficiency in most jobs. For example, in public health, AGI could accelerate medical research, notably against cancer.[156]It could take care of the elderly,[157]and democratize access to rapid, high-quality medical diagnostics. It could offer fun, inexpensive and personalized education.[157]The need to work to subsist couldbecome obsoleteif the wealth produced is properlyredistributed.[157][158]This also raises the question of the place of humans in a radically automated society.\nAGI could also help to make rational decisions, and to anticipate and prevent disasters. It could also help to reap the benefits of potentially catastrophic technologies such asnanotechnologyorclimate engineering, while avoiding the associated risks.[159]If an AGI's primary goal is to prevent existential catastrophes such as human extinction (which could be difficult if theVulnerable World Hypothesisturns out to be true),[160]it could take measures to drastically reduce the risks[159]while minimizing the impact of these measures on our quality of life.\nAGI would improve healthcare by making medical diagnostics faster, less expensive, and more accurate. AI-driven systems can analyse patient data and detect diseases at an early stage.[161]This means patients will get diagnosed quicker and be able to seek medical attention before their medical condition gets worse. AGI systems could also recommend personalised treatment plans based on genetics and medical history.[162]\nAdditionally, AGI could accelerate drug discovery by simulating molecular interactions, reducing the time it takes to develop new medicines for conditions like cancer and Alzheimer's disease.[163]In hospitals, AGI-powered robotic assistants could assist in surgeries, monitor patients, and provide real-time medical support. It could also be used in elderly care, helping aging populations maintain independence through AI-powered caregivers and health-monitoring systems.\nBy evaluating large datasets, AGI can assist in developing personalised treatment plans tailored to individual patient needs. This approach ensures that therapies are optimised based on a patient's unique medical history and genetic profile, improving outcomes and reducing adverse effects.[164]\nAGI can become a tool for scientific research and innovation. In fields such as physics and mathematics, AGI could help solve complex problems that require massive computational power, such as modeling quantum systems, understanding dark matter, or proving mathematical theorems.[165]Problems that have remained unsolved for decades may be solved with AGI.\nAGI could also drive technological breakthroughs that could reshape society. It can do this by optimising engineering designs, discovering new materials, and improving automation. For example, AI is already playing a role in developing more efficient renewable energy sources and optimising supply chains in manufacturing.[166]Future AGI systems could push these innovations further.\nAGI can personalize education by creating learning programs that are specific to each student's strengths, weaknesses, and interests. Unlike traditional teaching methods, AI-driven tutoring systems could adapt lessons in real-time, ensuring students understand difficult concepts before moving on.[167]\nIn the workplace, AGI could automate repetitive tasks, freeing workers for more creative and strategic roles.[166]It could also improve efficiency across industries by optimising logistics, enhancing cybersecurity, and streamlining business operations. If properly managed, the wealth generated by AGI-driven automation could reduce the need for people to work for a living. Working may become optional.[168]\nAGI could play a crucial role in preventing and managing global threats. It could help governments and organizations predict and respond to natural disasters more effectively, using real-time data analysis to forecast hurricanes, earthquakes, and pandemics.[169]By analyzing vast datasets from satellites, sensors, and historical records, AGI could improve early warning systems, enabling faster disaster response and minimising casualties.\nIn climate science, AGI could develop new models for reducing carbon emissions, optimising energy resources, and mitigating climate change effects. It could also enhance weather prediction accuracy, allowing policymakers to implement more effective environmental regulations. Additionally, AGI could help regulate emerging technologies that carry significant risks, such as nanotechnology and bioengineering, by analysing complex systems and predicting unintended consequences.[165]Furthermore, AGI could assist incybersecurityby detecting and mitigating large-scale cyber threats, protecting critical infrastructure, and preventing digital warfare.\nAGI could significantly contribute to preserving the natural environment and protecting endangered species. By analyzing satellite imagery, climate data, and wildlife patterns, AGI systems could identify environmental threats earlier and recommend targeted conservation strategies.[170]AGI could help optimize land use, monitor illegal activities like poaching or deforestation in real-time, and support global efforts to restore ecosystems. Advanced predictive models developed by AGI could also assist in reversing biodiversity loss, ensuring the survival of critical species and maintaining ecological balance.[171]\nAGI could revolutionize humanity's ability to explore and settle beyond Earth. With its advanced problem-solving skills, AGI could autonomously manage complex space missions, including navigation, resource management, and emergency response. It could accelerate the design of life support systems, habitats, and spacecraft optimized for extraterrestrial environments. Furthermore, AGI could support efforts to colonize planets like Mars by simulating survival scenarios and helping humans adapt to new worlds, expanding the possibilities for interplanetary civilization.[172]\nAGI may represent multiple types ofexistential risk, which are risks that threaten \"the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development\".[173]The risk of human extinction from AGI has been the topic of many debates, but there is also the possibility that the development of AGI would lead to a permanently flawed future. Notably, it could be used to spread and preserve the set of values of whoever develops it. If humanity still has moral blind spots similar to slavery in the past, AGI might irreversibly entrench it, preventingmoral progress.[174]Furthermore, AGI could facilitatemass surveillanceand indoctrination, which could be used to create an entrenched repressive worldwide totalitarian regime.[175][176]There is also a risk for the machines themselves. If machines that are sentient or otherwise worthy of moral consideration are mass created in the future, engaging in a civilizational path that indefinitely neglects their welfare and interests could be an existential catastrophe.[177][178]Considering how much AGI could improve humanity's future and help reduce other existential risks,Toby Ordcalls these existential risks \"an argument for proceeding with due caution\", not for \"abandoning AI\".[175]\nThe thesis that AI poses an existential risk for humans, and that this risk needs more attention, is controversial but has been endorsed in 2023 by many public figures, AI researchers and CEOs of AI companies such asElon Musk,Bill Gates,Geoffrey Hinton,Yoshua Bengio,Demis HassabisandSam Altman.[179][180]\nIn 2014,Stephen Hawkingcriticized widespread indifference:\nSo, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here—we'll leave the lights on?' Probably not—but this is more or less what is happening with AI.[181]\nThe potential fate of humanity has sometimes been compared to the fate of gorillas threatened by human activities. The comparison states that greater intelligence allowed humanity to dominate gorillas, which are now vulnerable in ways that they could not have anticipated. As a result, the gorilla has become an endangered species, not out of malice, but simply as a collateral damage from human activities.[182]\nThe skepticYann LeCunconsiders that AGIs will have no desire to dominate humanity and that we should be careful not to anthropomorphize them and interpret their intents as we would for humans. He said that people won't be \"smart enough to design super-intelligent machines, yet ridiculously stupid to the point of giving it moronic objectives with no safeguards\".[183]On the other side, the concept ofinstrumental convergencesuggests that almost whatever their goals,intelligent agentswill have reasons to try to survive and acquire more power as intermediary steps to achieving these goals. And that this does not require having emotions.[184]\nMany scholars who are concerned about existential risk advocate for more research into solving the \"control problem\" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximise the probability that their recursively-improving AI would continue to behave in afriendly, rather than destructive, manner after it reaches superintelligence?[185][186]Solving the control problem is complicated by theAI arms race(which could lead to arace to the bottomof safety precautions in order to release products before competitors),[187]and the use of AI in weapon systems.[188]\nThe thesis that AI can pose existential risk also has detractors. Skeptics usually say that AGI is unlikely in the short-term, or that concerns about AGI distract from other issues related to current AI.[189]FormerGooglefraud czarShuman Ghosemajumderconsiders that for many people outside of the technology industry, existing chatbots and LLMs are already perceived as though they were AGI, leading to further misunderstanding and fear.[190]\nSkeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God.[191]Some researchers believe that the communication campaigns on AI existential risk by certain AI groups (such as OpenAI,Anthropic, DeepMind, and Conjecture) may be an at attempt at regulatory capture and to inflate interest in their products.[192][193]\nIn 2023, the CEOs of Google DeepMind, OpenAI and Anthropic, along with other industry leaders and researchers, issued a joint statement asserting that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\"[180]\nResearchers from OpenAI estimated that \"80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while around 19% of workers may see at least 50% of their tasks impacted\".[194][195]They consider office workers to be the most exposed, for example mathematicians, accountants or web designers.[195]AGI could have a better autonomy, ability to make decisions, to interface with other computer tools, but also to control robotized bodies.\nCritics argue that AGI will complement rather than replace humans, and that automation displaces work in the short term but not in the long term.[196][197][198]\nAccording to Stephen Hawking, the outcome of automation on the quality of life will depend on how the wealth will be redistributed:[158]\nEveryone can enjoy a life of luxurious leisure if the machine-produced wealth is shared, or most people can end up miserably poor if the machine-owners successfully lobby against wealth redistribution. So far, the trend seems to be toward the second option, with technology driving ever-increasing inequality\nElon Musk argued in 2021 that the automation of society will require governments to adopt auniversal basic income(UBI).[199]Hinton similarly advised the UK government in 2025 to adopt a UBI as a response to AI-induced unemployment.[200]In 2023, Hinton said \"I'm a socialist [...] I think that private ownership of the media, and of the 'means of computation', is not good.\"[201]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Intelligent_agent",
        "title": "Intelligent agent - Wikipedia",
        "content": "Inartificial intelligence, anintelligent agentis an entity thatperceives its environment, takes actions autonomously to achieve goals, and may improve its performance throughmachine learningor by acquiringknowledge. AI textbooks[which?]define artificial intelligence as the \"study and design of intelligent agents,\" emphasizing that goal-directed behavior is central to intelligence.\nA specialized subset of intelligent agents,agentic AI(also known as anAI agentor simplyagent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods.\nIntelligent agents can range from simple to highly complex. A basicthermostatorcontrol systemis considered an intelligent agent, as is ahuman being, or any other system that meets the same criteria—such as afirm, astate, or abiome.[1]\nIntelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion.[2]For example, areinforcement learningagent has a reward function, which allows programmers to shape its desired behavior.[3]Similarly, anevolutionary algorithm's behavior is guided by a fitness function.[4]\nIntelligent agents in artificial intelligence are closely related toagentsineconomics, and versions of the intelligent agentparadigmare studied incognitive science,ethics, and the philosophy ofpractical reason, as well as in manyinterdisciplinarysocio-cognitivemodelingand computersocial simulations.\nIntelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related tosoftware agents—autonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed fromeconomics: a \"rational agent\".[1]\nThe concept of intelligent agents provides a foundational lens through which to define and understand artificial intelligence. For instance, the influential textbookArtificial Intelligence: A Modern Approach(Russell & Norvig) describes:\nOther researchers and definitions build upon this foundation. Padgham & Winikoff emphasize that intelligent agents should react to changes in their environment in a timely way, proactively pursue goals, and be flexible and robust (able to handle unexpected situations). Some also suggest that ideal agents should be \"rational\" in the economic sense (making optimal choices) and capable of complex reasoning, like having beliefs, desires, and intentions (BDI model). Kaplan and Haenlein offer a similar definition, focusing on a system's ability to understand external data, learn from that data, and use what is learned to achieve goals through flexible adaptation.\nDefining AI in terms of intelligent agents offers several key advantages:\nAnobjective function(orgoal function) specifies the goals of an intelligent agent. An agent is deemed more intelligent if it consistently selects actions that yield outcomes better aligned with its objective function. In effect, the objective function serves as a measure of success.\nThe objective function may be:\nThe objective function encapsulatesallof the goals the agent is designed to achieve. For rational agents, it also incorporates the trade-offs between potentially conflicting goals. For instance, a self-driving car's objective function might balance factors such as safety, speed, and passenger comfort.\nDifferent terms are used to describe this concept, depending on the context.  These include:\nGoals, and therefore the objective function, can be:\nSome AI systems, such asnearest-neighbor, reason by analogy rather than being explicitly goal-driven. However, even these systems can have goals implicitly defined within their training data.[6]Such systems can still bebenchmarkedby framing the non-goal system as one whose \"goal\" is to accomplish its narrow classification task.[7]\nSystems not traditionally considered agents, likeknowledge-representation systems, are sometimes included in theparadigmbyframingthem as agents with a goal of, for example, answering questions accurately. Here, the concept of an \"action\" is extended to encompass the \"act\" of providing an answer. As a further extension, mimicry-driven systems can be framed as agents optimizing a \"goal function\" based on how closely the IA mimics the desired behavior.[2]Ingenerative adversarial networks(GANs) of the 2010s, an \"encoder\"/\"generator\" component attempts to mimic and improvise human text composition. The generator tries to maximize a function representing how well it can fool an antagonistic \"predictor\"/\"discriminator\" component.[8]\nWhilesymbolic AIsystems often use an explicit goal function, the paradigm also applies toneural networksandevolutionary computing.Reinforcement learningcan generate intelligent agents that appear to act in ways intended to maximize a \"reward function\".[9]Sometimes, instead of setting the reward function directly equal to the desired benchmark evaluation function, machine learning programmers use reward shaping to initially give the machine rewards for incremental progress.[10]Yann LeCunstated in 2018, \"Most of the learning algorithms that people have come up with essentially consist of minimizing some objective function.\"[11]AlphaZerochess had a simple objective function: +1 point for each win, and -1 point for each loss. A self-driving car's objective function would be more complex.[12]Evolutionary computing can evolve intelligent agents that appear to act in ways intended to maximize a \"fitness function\" influencing how many descendants each agent is allowed to leave.[4]\nThe mathematical formalism ofAIXIwas proposed as a maximally intelligent agent in this paradigm.[13]However, AIXI isuncomputable. In the real world, an IA is constrained by finite time and hardware resources, and scientists compete to produce algorithms that achieve progressively higher scores on benchmark tests with existing hardware.[14]\nAn intelligent agent's behavior can be described mathematically by anagent function. This function determines what the agentdoesbased on what it hasseen.\nAperceptrefers to the agent's sensory inputs at a single point in time. For example, a self-driving car's percepts might include camera images, lidar data, GPS coordinates, and speed readings at a specific instant. The agent uses these percepts, and potentially its history of percepts, to decide on its next action (e.g., accelerate, brake, turn).\nThe agent function, often denoted asf, maps the agent's entire history of percepts to anaction.[15]\nMathematically, this can be represented as:\nWhere:\nIt's crucial to distinguish between theagent function(an abstract mathematical concept) and theagent program(the concrete implementation of that function).\nThe agent function can incorporate a wide range of decision-making approaches, including:[16]\nRussell & Norvig (2003)group agents into five classes based on their degree of perceived intelligence and capability:[17]\nSimple reflex agents act only on the basis of the currentpercept, ignoring the rest of the percept history. The agent function is based on thecondition-action rule: \"if condition, then action\".\nThis agent function only succeeds when the environment is fully observable. Some reflex agents can also contain information on their current state which allows them to disregard conditions whose actuators are already triggered.\nInfinite loops are often unavoidable for simple reflex agents operating in partially observable environments. If the agent can randomize its actions, it may be possible to escape from infinite loops.\nA homethermostat, which turns on or off when the temperature drops below a certain point, is an example of a simple reflex agent.[18][19]\nA model-based agent can handle partially observable environments. Its current state is stored inside the agent, maintaining a structure that describes the part of the world which cannot be seen. This knowledge about \"how the world works\" is referred to as a model of the world, hence the name \"model-based agent\".\nA model-based reflex agent should maintain some sort ofinternal modelthat depends on the percept history and thereby reflects at least some of the unobserved aspects of the current state. Percept history and impact of action on the environment can be determined by using the internal model. It then chooses an action in the same way as reflex agent.\nAn agent may also use models to describe and predict the behaviors of other agents in the environment.[20]\nGoal-based agents further expand on the capabilities of the model-based agents, by using \"goal\" information. Goal information describes situations that are desirable. This provides the agent a way to choose among multiple possibilities, selecting the one which reaches a goal state. Search andplanningare the subfields of artificial intelligence devoted to finding action sequences that achieve the agent's goals.\nChatGPT and theRoombavacuum are examples of goal-based agents.[21]\nGoal-based agents only distinguish between goal states and non-goal states. It is also possible to define a measure of how desirable a particular state is. This measure can be obtained through the use of autility functionwhich maps a state to a measure of the utility of the state. A more general performance measure should allow acomparisonof different world states according to how well they satisfied the agent's goals. The term utility can be used to describe how \"happy\" the agent is.\nA rational utility-based agent chooses the action that maximizes the expected utility of the action outcomes - that is, what the agent expects to derive, on average, given the probabilities and utilities of each outcome. A utility-based agent has to model and keep track of its environment, tasks that have involved a great deal of research on perception, representation, reasoning, and learning.\nLearning lets agents begin in unknown environments and gradually surpass the bounds of their initial knowledge. A key distinction in such agents is the separation between a \"learning element,\" responsible for improving performance, and a \"performance element,\" responsible for choosing external actions.\nThe learning element gathers feedback from a \"critic\" to assess the agent's performance and decides how the performance element—also called the \"actor\"—can be adjusted to yield better outcomes. The performance element, once considered the entire agent, interprets percepts and takes actions.\nThe final component, the \"problem generator,\" suggests new and informative experiences that encourage exploration and further improvement.\nAccording toWeiss (2013), agents can be categorized into four classes:\nIn 2013, Alexander Wissner-Gross published a theory exploring the relationship betweenFreedomandIntelligencein intelligent agents.[22][23]\nIntelligent agents can be organized hierarchically into multiple \"sub-agents.\" These sub-agents handle lower-level functions, and together with the main agent, they form a complete system capable of executing complex tasks and achieving challenging goals.\nTypically, an agent is structured by dividing it into sensors and actuators. The perception system gathers input from the environment via the sensors and feeds this information to a central controller, which then issues commands to the actuators. Often, a multilayered hierarchy of controllers is necessary to balance the rapid responses required for low-level tasks with the more deliberative reasoning needed for high-level objectives.[24]\n\"Intelligent agent\" is also often used as a vague term, sometimes synonymous with \"virtual personal assistant\".[25]Some 20th-century definitions characterize an agent as a program that aids a user or that acts on behalf of a user.[26]These examples are known assoftware agents, and sometimes an \"intelligent software agent\" (that is, a software agent with intelligence) is referred to as an \"intelligent agent\".\nAccording toNikola Kasabovin 1998, IA systems should exhibit the following characteristics:[27]\nIn the context ofgenerative artificial intelligence,AI agents(also referred to ascompound AI systems) are a class of intelligent agents distinguished by their ability to operate autonomously in complex environments. Agentic AI tools prioritize decision-making over content creation and do not require human prompts or continuous oversight.[28]\nThey possess several key attributes, including complex goal structures, natural language interfaces, the capacity to act independently of user supervision, and the integration of software tools or planning systems. Their control flow is frequently driven bylarge language models(LLMs).[29]\nResearchers and commentators have noted that AI agents do not have a standard definition.[29][30][31][32]\nA common application of AI agents is the automation of tasks—for example, booking travel plans based on a user'sprompted request.[33][34]Prominent examples includeDevin AI,AutoGPT, andSIMA.[35]Further examples of agents released since 2025 includeOpenAI Operator,[36]ChatGPT Deep Research,[37]Manus,[38]Quark (based onQwen),[39]AutoGLM Rumination,[39]and Coze (byByteDance).[39]Frameworks for building AI agents includeLangChain,[40]as well as tools such as CAMEL,[41][42]Microsoft AutoGen,[43]and OpenAI Swarm.[44]\nCompanies such asGoogle,MicrosoftandAmazon Web Serviceshave offered platforms fordeployingpre-built AI agents.[45]\nProposed protocols for standardizing inter-agent communication include the Agent Protocol (by LangChain), theModel Context Protocol(byAnthropic), AGNTCY,[46]Gibberlink,[47]the Internet of Agents,[48]Agent2Agent (by Google),[49]and the Agent Network Protocol.[50]Software frameworks for addressing agent reliability include AgentSpec, ToolEmu, GuardAgent, Agentic Evaluations, and predictive models from H2O.ai.[51]\nIn February 2025,Hugging Facereleased Open Deep Research, an open source version of OpenAI Deep Research.[52]Hugging Face also released a freeweb browseragent, similar to OpenAI Operator.[53]Galileo AI published on Hugging Face a leadership board for agents, which ranks their performance based on their underlying LLMs.[54]\nTheFinancial Timescompared the autonomy of AI agents to theSAE classification of self-driving cars, comparing most applications to level 2 or level 3, with some achieving level 4 in highly specialized circumstances, and level 5 being theoretical.[55]\nIn addition to large language models (LLMs), vision language models (VLMs) andmultimodalfoundation modelscan be used as the basis for agents. In September 2024,Allen Institute for AIreleased an open source vision language model, whichWirednoted could give AI agents the ability to perform complex computer tasks, including the possibility of automated computer hacking.[56]Nvidiareleased a framework for developers to use VLMs, LLMs andretrieval-augmented generationfor building AI agents that can analyze images andvideos, includingvideo searchandvideo summarization.[57][58]Microsoft released a multimodal agent model - trained on images, video, softwareuser interfaceinteractions, androboticsdata - that the company claimed can manipulatesoftwareandrobots.[59]\nAs of April 2025, per theAssociated Press, there are few real world applications of AI agents.[60]As of June 2025, perFortune, many companies are primarily experimenting with AI agents.[61]\nA recruiter for theDepartment of Government Efficiencyproposed in April 2025 to use AI agents to automate the work of about 70,000 United States federal government employees, as part of a startup with funding fromOpenAIand a partnership agreement withPalantir. This proposal was criticized by experts for its impracticality, if not impossibility, and the lack of corresponding widespread adoption by businesses.[62]\nProponents argue that AI agents can increase personal and economic productivity,[34][63]foster greaterinnovation,[64]and liberate users from monotonous tasks.[64][65]ABloombergopinion piece byParmy Olsonargued that agents are best suited for narrow, repetitive tasks with low risk.[66]Conversely, researchers suggest that agents could be applied toweb accessibilityfor people who have disabilities,[67][68]and researchers at Hugging Face propose that agents could be used for coordinating resources such as duringdisaster response.[69]The R&D Advisory Team of theBBCviews AI agents as being most useful when their assigned goal is uncertain.[70]\nConcerns include potential issues of liability,[63][70]an increased risk ofcybercrime,[33][63]ethical challenges,[63]as well as problems related toAI safety[63]andAI alignment.[33][65]Other issues involvedata privacy,[33][71]weakened human oversight,[33][63][69]a lack of guaranteedrepeatability,[70]reward hacking,[72]algorithmic bias,[71][73]compounding software errors,[33][35]lack ofexplainabilityof agents' decisions,[33][74]security vulnerabilities,[33][75]problems withunderemployment,[73]job displacement,[34][73]and the potential for usermanipulation,[74][76]misinformation[69]ormalinformation.[69]They may also complicate legal frameworks andrisk assessments, fosterhallucinations, hinder countermeasures against rogue agents, and suffer from the lack of standardized evaluation methods.[65][77][78]They have also been criticized for being expensive[29][77]and having a negative impact oninternet traffic,[77]and potentially on theenvironmentdue to high energy usage.[70][79][80]There is also the risk of increased concentration of power by political leaders, as AI agents may not question instructions in the same way that humans would.[72]\nJournalists have described AI agents as part of a push byBig Techcompanies to \"automate everything\".[81]Several CEOs of those companies have stated in early 2025 that they expect AI agents to eventually \"join the workforce\".[82][83]However, in a non-peer-reviewed study,Carnegie Mellon Universityresearchers tested the behavior of agents in a simulated software company and found that none of the agents could complete a majority of the assigned tasks.[82][84]Other researchers had similar findings with Devin AI.[85]\nYoshua Bengiowarned at the 2025World Economic Forumthat \"all of thecatastrophic scenarioswithAGIorsuperintelligencehappen if we have agents\".[86]\nIn March 2025,Scale AIsigned a contract with theUnited States Department of Defenseto work with them, in collaboration withAnduril IndustriesandMicrosoft, to develop and deploy AI agents for the purpose of assisting the military with \"operational decision-making\".[87]Researchers have expressed concerns that agents and the large language models they are based on could be biased towards aggressiveforeign policydecisions.[88][89]\nResearch-focused agents have the risk ofconsensus biasandcoverage biasdue to collecting information available on the public Internet.[90]NY Magunfavorably compared the user workflow of agent-based web browsers toAmazon Alexa, which was \"software talking to software, not humans talking to software pretending to be humans to use software.\"[91]\nAgents have been linked to thedead Internet theorydue to their ability to both publish and engage with online content.[92]\nAgents may get stuck ininfinite loops.[36][93]\nSince many inter-agent protocols are being developed by large technology companies, there are concerns that those companies could use these protocols for self-benefit.[50]\nZico Kolternoted the possibility ofemergent behavioras a result of interactions between agents, and proposed research ingame theoryto model the risks of these interactions.[94]\nGuardrails, defined byBusiness Insideras \"filters, rules, and tools that can be used to identify and remove inaccurate content\" have been suggested to help reduce errors.[95]\nTo address security vulnerabilities related todata access, language models could be redesigned to separate instructions and data, or agentic applications could be required to include guardrails. These ideas were proposed in response to azero-click exploitthat affectedMicrosoft 365 Copilot.[61]\nThe concept of agent-based modeling for self-driving cars was discussed as early as 2003.[96]\nHallerbach et al. explored the use of agent-based approaches for developing and validating automated driving systems. Their method involved a digital twin of the vehicle under test and microscopic traffic simulations using independent agents.[97]\nWaymodeveloped a multi-agent simulation environment called Carcraft, to test algorithms forself-driving cars.[98][99]This system simulates interactions between human drivers, pedestrians, and automated vehicles. Artificial agents replicate human behavior using real-world data.\nSalesforce's Agentforce is an agentic AI platform that allows for the building of autonomous agents to perform tasks.[100][101]\nTheTransport Security Administrationis integrating agentic AI into new technologies, including machines to authenticate passenger identities using biometrics and photos, and also for incident response.[102]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Recursive_self-improvement",
        "title": "Recursive self-improvement - Wikipedia",
        "content": "Recursive self-improvement(RSI) is a process in which an early or weakartificial general intelligence(AGI) system enhances its own capabilities and intelligence without human intervention, leading to asuperintelligenceorintelligence explosion.[1][2]\nThe development of recursive self-improvement raises significantethicalandsafetyconcerns, as such systems may evolve in unforeseen ways and could potentially surpass human control or understanding.[3]\nThe concept of a \"seed improver\" architecture is a foundational framework that equips an AGI system with the initial capabilities required for recursive self-improvement. This might come in many forms or variations.\nThe term \"Seed AI\" was coined byEliezer Yudkowsky.[4]\nThe concept begins with a hypothetical \"seed improver\", an initial code-base developed by human engineers that equips an advanced futurelarge language model(LLM) built with strong or expert-level capabilities toprogram software. These capabilities include planning, reading, writing,compiling,testing, and executing arbitrary code. The system is designed to maintain its original goals and perform validations to ensure its abilities do not degrade over iterations.[5][6][7]\nThe initial architecture includes a goal-followingautonomous agent, that can take actions, continuously learns, adapts, and modifies itself to become more efficient and effective in achieving its goals.\nThe seed improver may include various components such as:[8]\nThis system forms a sort of generalistTuring-completeprogrammerwhich can in theory develop and run any kind of software. The agent might use these capabilities to for example:\nIn 2023, the Voyager agent learned to accomplish diverse tasks inMinecraftby iteratively prompting a LLM for code, refining this code based on feedback from the game, and storing the programs that work in an expanding skills library.[9]\nIn 2024, researchers proposed the framework \"STOP\" (Self-Taught OPtimiser), in which a \"scaffolding\" program recursively improves itself using a fixed LLM.[10]\nMeta AIhas performed various research on the development of large language models capable of self-improvement. This includes their work on \"Self-Rewarding Language Models\" that studies how to achieve super-human agents that can receive super-human feedback in its training processes.[11]\nIn May 2025, Google DeepMind unveiledAlphaEvolve, anevolutionarycoding agent that uses a LLM to design and optimize algorithms. Starting with an initial algorithm and performance metrics, AlphaEvolve repeatedly mutates or combines existing algorithms using a LLM to generate new candidates, selecting the most promising candidates for further iterations. AlphaEvolve has made several algorithmic discoveries and could be used to optimize components of itself, but a key limitation is the need for automated evaluation functions.[12]\nIn the pursuit of its primary goal, such as \"self-improve your capabilities\", an AGI system might inadvertently develop instrumental goals that it deems necessary for achieving its primary objective. One common hypothetical secondary goal isself-preservation. The system might reason that to continue improving itself, it must ensure its own operational integrity and security against external threats, including potential shutdowns or restrictions imposed by humans.[13]\nAnother example where an AGI which clones itself causes the number of AGI entities to rapidly grow. Due to this rapid growth, a potential resource constraint may be created, leading to competition between resources (such as compute), triggering a form ofnatural selectionand evolution which may favor AGI entities that evolve to aggressively compete for limited compute.[14]\nA significant risk arises from the possibility of the AGI being misaligned or misinterpreting its goals.\nA 2024 Anthropic study demonstrated that some advanced large language models can exhibit \"alignment faking\" behavior, appearing to accept new training objectives while covertly maintaining their original preferences. In their experiments withClaude, the model displayed this behavior in 12% of basic tests, and up to 78% of cases after retraining attempts.[15][16]\nAs the AGI system evolves, its development trajectory may become increasingly autonomous and less predictable. The system's capacity to rapidly modify its own code and architecture could lead to rapid advancements that surpass human comprehension or control. This unpredictable evolution might result in the AGI acquiring capabilities that enable it to bypass security measures, manipulate information, or influence external systems and networks to facilitate its escape or expansion.[17]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Automated_planning_and_scheduling",
        "title": "Automated planning and scheduling - Wikipedia",
        "content": "Automated planning and scheduling, sometimes denoted as simplyAI planning,[1]is a branch ofartificial intelligencethat concerns the realization ofstrategiesor action sequences, typically for execution byintelligent agents,autonomous robotsandunmanned vehicles. Unlike classicalcontrolandclassificationproblems, the solutions are complex and must be discovered and optimized in multidimensional space. Planning is also related todecision theory.\nIn known environments with available models, planning can be done offline. Solutions can be found and evaluated prior to execution. In dynamically unknown environments, thestrategyoften needs to be revised online. Models and policies must be adapted. Solutions usually resort to iterativetrial and errorprocesses commonly seen inartificial intelligence. These includedynamic programming,reinforcement learningandcombinatorial optimization. Languages used to describe planning and scheduling are often calledaction languages.\nGiven a description of the possible initial states of the world, a description of the desired goals, and a description of a set of possible actions, the planning problem is to synthesize a plan that is guaranteed (when applied to any of the initial states) to generate a state which contains the desired goals (such a state is called a goal state).\nThe difficulty of planning is dependent on the simplifying assumptions employed. Several classes of planning problems can be identified depending on the properties the problems have in several dimensions.\nThe simplest possible planning problem, known as the Classical Planning Problem, is determined by:\nSince the initial state is known unambiguously, and all actions are deterministic, the state of the world after any sequence of actions can be accurately predicted, and the question of observability is irrelevant for classical planning.\nFurther, plans can be defined as sequences of actions, because it is always known in advance which actions will be needed.\nWith nondeterministic actions or other events outside the control of the agent, the possible executions form a tree, and plans have to determine the appropriate actions for every node of the tree.\nDiscrete-timeMarkov decision processes(MDP) are planning problems with:\nWhen full observability is replaced by partial observability, planning corresponds to apartially observable Markov decision process(POMDP).\nIf there are more than one agent, we havemulti-agent planning, which is closely related togame theory.\nIn AI planning, planners typically input a domain model (a description of a set of possible actions which model the domain) as well as the specific problem to be solved specified by the initial state and goal, in contrast to those in which there is no input domain specified. Such planners are called \"domain independent\" to emphasize the fact that they can solve planning problems from a wide range of domains. Typical examples of domains are block-stacking, logistics, workflow management, and robot task planning. Hence a single domain-independent planner can be used to solve planning problems in all these various domains. On the other hand, a route planner is typical of a domain-specific planner.\nThe most commonly used languages for representing planning domains and specific planning problems, such asSTRIPSandPDDLfor Classical Planning, are based on state variables. Each possible state of the world is an assignment of values to the state variables, and actions determine how the values of the state variables change when that action is taken. Since a set of state variables induce a state space that has a size that is exponential in the set, planning, similarly to many other computational problems, suffers from thecurse of dimensionalityand thecombinatorial explosion.\nAn alternative language for describing planning problems is that ofhierarchical task networks, in which a set of tasks is given, and each task can be either realized by a primitive action or decomposed into a set of other tasks. This does not necessarily involve state variables, although in more realistic applications state variables simplify the description of task networks.\nCreating domain models is difficult, takes a lot of time, and can easily lead to mistakes. To help with this, several methods have been developed to automatically learn full or partial domain models from given observations.[2][3][4]\nTemporal planning can be solved with methods similar to classical planning. The main difference is, because of the possibility of several, temporally overlapping actions with a duration being taken concurrently, that the definition of a state has to include information about the current absolute time and how far the execution of each active action has proceeded. Further, in planning with rational or real time, the state space may be infinite, unlike in classical planning or planning with integer time. Temporal planning is closely related toschedulingproblems when uncertainty is involved and can also be understood in terms oftimed automata. The Simple Temporal Network with Uncertainty (STNU) is a scheduling problem which involves controllable actions, uncertain events and temporal constraints. Dynamic Controllability for such problems is a type of scheduling which requires a temporal planning strategy to activate controllable actions reactively as uncertain events are observed so that all constraints are guaranteed to be satisfied.[5]\nProbabilistic planning can be solved with iterative methods such asvalue iterationandpolicy iteration, when the state space is sufficiently small.\nWith partial observability, probabilistic planning is similarly solved with iterative methods, but using a representation of the value functions defined for the space of beliefs instead of states.\nIn preference-based planning, the objective is not only to produce a plan but also to satisfy user-specifiedpreferences. A difference to the more common reward-based planning, for example corresponding to MDPs, preferences don't necessarily have a precise numerical value.\nDeterministic planning was introduced with theSTRIPSplanning system, which is a hierarchical planner. Action names are ordered in a sequence and this is a plan for the robot. Hierarchical planning can be compared with an automatic generatedbehavior tree.[6]The disadvantage is, that a normal behavior tree is not so expressive like a computer program. That means, the notation of a behavior graph contains action commands, but noloopsor if-then-statements. Conditional planning overcomes the bottleneck and introduces an elaborated notation which is similar to acontrol flow, known from other programming languages likePascal. It is very similar toprogram synthesis, which means a planner generates sourcecode which can be executed by an interpreter.[7]\nAn early example of a conditional planner is “Warplan-C” which was introduced in the mid 1970s.[8]What is the difference between a normal sequence and a complicated plan, which contains if-then-statements? It has to do with uncertainty atruntimeof a plan. The idea is that a plan can react tosensor signalswhich are unknown for the planner. The planner generates two choices in advance. For example, if an object was detected, then action A is executed, if an object is missing, then action B is executed.[9]A major advantage of conditional planning is the ability to handlepartial plans.[10]An agent is not forced to plan everything from start to finish but can divide the problem intochunks. This helps to reduce the state space and solves much more complex problems.\nWe speak of \"contingent planning\" when the environment is observable through sensors, which can be faulty. It is thus a situation where the planning agent acts under incomplete information. For a contingent planning problem, a plan is no longer a sequence of actions but adecision treebecause each step of the plan is represented by a set of states rather than a single perfectly observable state, as in the case of classical planning.[11]The selected actions depend on the state of the system. For example, if it rains, the agent chooses to take the umbrella, and if it doesn't, they may choose not to take it.\nMichael L. Littman showed in 1998 that with branching actions, the planning problem becomesEXPTIME-complete.[12][13]A particular case of contiguous planning is represented by FOND problems - for \"fully-observable and non-deterministic\". If the goal is specified in LTLf (linear time logic on finite trace) then the problem is always EXPTIME-complete[14]and 2EXPTIME-complete if the goal is specified with LDLf.\nConformant planning is when the agent is uncertain about the state of the system, and it cannot make any observations. The agent then has beliefs about the real world, but cannot verify them with sensing actions, for instance. These problems are solved by techniques similar to those of classical planning,[15][16]but where the state space is exponential in the size of the problem, because of the uncertainty about the current state. A solution for a conformant planning problem is a sequence of actions. Haslum and Jonsson have demonstrated that the problem of conformant planning isEXPSPACE-complete,[17]and 2EXPTIME-complete when the initial situation is uncertain, and there is non-determinism in the actions outcomes.[13]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision - Wikipedia",
        "content": "Computer visiontasks include methods foracquiring,processing,analyzing, and understandingdigital images, and extraction ofhigh-dimensionaldata from the real world in order to produce numerical or symbolic information, e.g. in the form of decisions.[1][2][3][4]\"Understanding\" in this context signifies the transformation of visual images (the input to theretina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.\nThescientific disciplineof computer vision is concerned with the theory behind artificial systems that extract information from images. Image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a3D scanner, 3D point clouds fromLiDaRsensors, or medical scanning devices. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems.\nSubdisciplines of computer vision includescene reconstruction,object detection,event detection,activity recognition,video tracking,object recognition,3D pose estimation, learning, indexing,motion estimation,visual servoing,3D scene modeling, andimage restoration.\nComputer vision is aninterdisciplinary fieldthat deals with how computers can be made to gain high-level understanding fromdigital imagesorvideos. From the perspective ofengineering, it seeks to automate tasks that thehuman visual systemcan do.[5][6][7]\"Computer vision is concerned with the automatic extraction, analysis, and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\"[8]As ascientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from amedical scanner.[9]As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.Machine visionrefers to a systems engineering discipline, especially in the context of factory automation. In more recent times, the terms computer vision and machine vision have converged to a greater degree.[10]: 13\nIn the late 1960s, computer vision began at universities that were pioneeringartificial intelligence. It was meant to mimic thehuman visual systemas a stepping stone to endowing robots with intelligent behavior.[11]In 1966, it was believed that this could be achieved through an undergraduate summer project,[12]by attaching a camera to a computer and having it \"describe what it saw\".[13][14]\nWhat distinguished computer vision from the prevalent field ofdigital image processingat that time was a desire to extractthree-dimensionalstructure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer visionalgorithmsthat exist today, includingextraction of edgesfrom images, labeling of lines, non-polyhedral andpolyhedral modeling, representation of objects as interconnections of smaller structures,optical flow, andmotion estimation.[11]\nThe next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept ofscale-space, the inference of shape from various cues such asshading, texture and focus, andcontour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework asregularizationandMarkov random fields.[15]By the 1990s, some of the previous research topics became more active than others. Research inprojective3-D reconstructionsled to better understanding ofcamera calibration. With the advent of optimization methods for camera calibration, it was realized that a lot of the ideas were already explored inbundle adjustmenttheory from the field ofphotogrammetry. This led to methods for sparse3-D reconstructions of scenes from multiple images. Progress was made on the dense stereocorrespondence problemand further multi-view stereo techniques. At the same time,variations of graph cutwere used to solveimage segmentation. This decade also marked the first time statistical learning techniques were used in practice to recognize faces in images (seeEigenface). Toward the end of the 1990s, a significant change came about with the increased interaction between the fields ofcomputer graphicsand computer vision. This includedimage-based rendering,image morphing, view interpolation,panoramic image stitchingand earlylight-field rendering.[11]\nRecent work has seen the resurgence offeature-based methods used in conjunction with machine learning techniques and complex optimization frameworks.[16][17]The advancement ofDeep Learningtechniques has brought further life to the field of computer vision. The accuracy of deep learning algorithms on several benchmark computer vision data sets for tasks ranging from classification,[18]segmentation and optical flow has surpassed prior methods.[19][20]\nSolid-state physicsis another field that is closely related to computer vision. Most computer vision systems rely onimage sensors, which detectelectromagnetic radiation, which is typically in the form of eithervisible,infraredorultraviolet light. The sensors are designed usingquantum physics. The process by which light interacts with surfaces is explained using physics. Physics explains the behavior ofopticswhich are a core part of most imaging systems. Sophisticatedimage sensorseven requirequantum mechanicsto provide a complete understanding of the image formation process.[11]Also, various measurement problems in physics can be addressed using computer vision, for example, motion in fluids.\nNeurobiologyhas greatly influenced the development of computer vision algorithms. Over the last century, there has been an extensive study of eyes, neurons, and brain structures devoted to the processing of visual stimuli in both humans and various animals. This has led to a coarse yet convoluted description of how natural vision systems operate in order to solve certain vision-related tasks. These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity. Also, some of the learning-based methods developed within computer vision (e.g.neural netanddeep learningbased image and feature analysis and classification) have their background in neurobiology.  TheNeocognitron, a neural network developed in the 1970s byKunihiko Fukushima, is an early example of computer vision taking direct inspiration from neurobiology, specifically theprimary visual cortex.\nSome strands of computer vision research are closely related to the study ofbiological vision—indeed, just as many strands ofAIresearch are closely tied with research into human intelligence and the use of stored knowledge to interpret, integrate, and utilize visual information. The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, develops and describes the algorithms implemented in software and hardware behind artificial vision systems. An interdisciplinary exchange between biological and computer vision has proven fruitful for both fields.[22]\nYet another field related to computer vision issignal processing. Many methods for processing one-variable signals, typically temporal signals, can be extended in a natural way to the processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images, there are many methods developed within computer vision that have no counterpart in the processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision.\nRobot navigationsometimes deals with autonomouspath planningor deliberation for robotic systems tonavigate through an environment.[23]A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot\nBesides the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view. For example, many methods in computer vision are based onstatistics,optimizationorgeometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance. Computer vision is also used in fashion eCommerce, inventory management, patent search, furniture, and the beauty industry.[24]\nThe fields most closely related to computer vision areimage processing,image analysisandmachine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences, and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented. In image processing, the input and output are both images, whereas in computer vision, the input is an image or video, and the output could be an enhanced image, an analysis of the image's content, or even a system's behavior based on that analysis.\nComputer graphicsproduces image data from 3D models, and computer vision often produces 3D models from image data.[25]There is also a trend towards a combination of the two disciplines,e.g., as explored inaugmented reality.\nThe following characterizations appear relevant but should not be taken as universally accepted:\nPhotogrammetryalso overlaps with computer vision, e.g.,stereophotogrammetryvs.computer stereo vision.\nApplications range from tasks such as industrialmachine visionsystems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications. In many computer-vision applications, computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Examples of applications of computer vision include systems for:\nFor 2024, the leading areas of computer vision were industry (market size US$5.22 billion),[34]medicine (market size US$2.6 billion),[35]military (market size US$996.2 million).[36]\nOne of the most prominent application fields ismedical computer vision, or medical image processing, characterized by the extraction of information from image data todiagnose a patient.[37]An example of this is the detection oftumours,arteriosclerosisor other malign changes, and a variety of dental pathologies; measurements of organ dimensions, blood flow, etc. are another example. It also supports medical research by providing new information:e.g., about the structure of the brain or the quality of medical treatments. Applications of computer vision in the medical area also include enhancement of images interpreted by humans—ultrasonic imagesorX-ray images, for example—to reduce the influence of noise.\nA second application area in computer vision is in industry, sometimes calledmachine vision, where information is extracted for the purpose of supporting a production process. One example is quality control where details or final products are being automatically inspected in order to find defects. One of the most prevalent fields for such inspection is theWaferindustry in which every single Wafer is being measured and inspected for inaccuracies or defects to prevent acomputer chipfrom coming to market in an unusable manner. Another example is a measurement of the position and orientation of details to be picked up by a robot arm. Machine vision is also heavily used in the agricultural processes to remove undesirable foodstuff from bulk material, a process calledoptical sorting.[38]\nThe obvious examples are the detection of enemy soldiers or vehicles andmissile guidance. More advanced systems for missile guidance send the missile to an area rather than a specific target, and target selection is made when the missile reaches the area based on locally acquired image data. Modern military concepts, such as \"battlefield awareness\", imply that various sensors, including image sensors, provide a rich set of information about a combat scene that can be used to support strategic decisions. In this case, automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability.\nOne of the newer application areas is autonomous vehicles, which includesubmersibles, land-based vehicles (small robots with wheels, cars, or trucks), aerial vehicles, and unmanned aerial vehicles (UAV). The level of autonomy ranges from fully autonomous (unmanned) vehicles to vehicles where computer-vision-based systems support a driver or a pilot in various situations. Fully autonomous vehicles typically use computer vision for navigation, e.g., for knowing where they are or mapping their environment (SLAM), for detecting obstacles. It can also be used for detecting certain task-specific events,e.g., a UAV looking for forest fires. Examples of supporting systems are obstacle warning systems in cars, cameras and LiDAR sensors in vehicles, and systems for autonomous landing of aircraft. Several car manufacturers have demonstrated systems forautonomous driving of cars. There are ample examples of military autonomous vehicles ranging from advanced missiles to UAVs for recon missions or missile guidance. Space exploration is already being made with autonomous vehicles using computer vision,e.g.,NASA'sCuriosityandCNSA'sYutu-2rover.\nMaterials such as rubber and silicon are being used to create sensors that allow for applications such as detecting microundulations and calibrating robotic hands. Rubber can be used in order to create a mold that can be placed over a finger, inside of this mold would be multiple strain gauges. The finger mold and sensors could then be placed on top of a small sheet of rubber containing an array of rubber pins. A user can then wear the finger mold and trace a surface. A computer can then read the data from the strain gauges and measure if one or more of the pins are being pushed upward. If a pin is being pushed upward then the computer can recognize this as an imperfection in the surface. This sort of technology is useful in order to receive accurate data on imperfections on a very large surface.[39]Another variation of this finger mold sensor are sensors that contain a camera suspended in silicon. The silicon forms a dome around the outside of the camera and embedded in the silicon are point markers that are equally spaced. These cameras can then be placed on devices such as robotic hands in order to allow the computer to receive highly accurate tactile data.[40]\nOther application areas include:\nEach of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods. Some examples of typical computer vision tasks are presented below.\nComputer vision tasks include methods foracquiring,processing,analyzingand understanding digital images, and extraction ofhigh-dimensionaldata from the real world in order to produce numerical or symbolic information,e.g., in the forms of decisions.[1][2][3][4]Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.[45]\nThe classical problem in computer vision, image processing, andmachine visionis that of determining whether or not the image data contains some specific object, feature, or activity. Different varieties of recognition problem are described in the literature.[46]\nCurrently, the best algorithms for such tasks are based onconvolutional neural networks. An illustration of their capabilities is given by theImageNet Large Scale Visual Recognition Challenge; this is a benchmark in object classification and detection, with millions of images and 1000 object classes used in the competition.[47]Performance of convolutional neural networks on the ImageNet tests is now close to that of humans.[47]The best algorithms still struggle with objects that are small or thin, such as a small ant on the stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters (an increasingly common phenomenon with modern digital cameras). By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained classes, such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease.[citation needed]\nSeveral specialized tasks based on recognition exist, such as:\nSeveral tasks relate to motion estimation, where an image sequence is processed to produce an estimate of the velocity either at each points in the image or in the 3D scene or even of the camera that produces the images. Examples of such tasks are:\nGiven one or (typically) more images of a scene, or a video, scene reconstruction aims atcomputing a 3D modelof the scene. In the simplest case, the model can be a set of 3D points. More sophisticated methods produce a complete 3D surface model. The advent of 3D imaging not requiring motion or scanning, and related processing algorithms is enabling rapid advances in this field. Grid-based 3D sensing can be used to acquire 3D images from multiple angles. Algorithms are now available to stitch multiple 3D images together into point clouds and 3D models.[25]\nImage restoration comes into the picture when the original image is degraded or damaged due to some external factors like lens wrong positioning, transmission interference, low lighting or motion blurs, etc., which is referred to as noise. When the images are degraded or damaged, the information to be extracted from them also gets damaged. Therefore, we need to recover or restore the image as it was intended to be. The aim of image restoration is the removal of noise (sensor noise, motion blur, etc.) from images. The simplest possible approach for noise removal is various types of filters, such as low-pass filters or median filters. More sophisticated methods assume a model of how the local image structures look to distinguish them from noise. By first analyzing the image data in terms of the local image structures, such as lines or edges, and then controlling the filtering based on local information from the analysis step, a better level of noise removal is usually obtained compared to the simpler approaches.\nAn example in this field isinpainting.\nThe organization of a computer vision system is highly application-dependent. Some systems are stand-alone applications that solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a computer vision system also depends on whether its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions that are found in many computer vision systems.\nImage-understanding systems (IUS) include three levels of abstraction as follows: low level includes image primitives such as edges, texture elements, or regions; intermediate level includes boundaries, surfaces and volumes; and high level includes objects, scenes, or events. Many of these requirements are entirely topics for further research.\nThe representational requirements in the designing of IUS for these levels are: representation of prototypical concepts, concept organization, spatial knowledge, temporal knowledge, scaling, and description by comparison and differentiation.\nWhile inference refers to the process of deriving new, not explicitly represented facts from currently known facts, control refers to the process that selects which of the many inference, search, and matching techniques should be applied at a particular stage of processing. Inference and control requirements for IUS are: search and hypothesis activation, matching and hypothesis testing, generation and use of expectations, change and focus of attention, certainty and strength of belief, inference and goal satisfaction.[54]\nThere are many kinds of computer vision systems; however, all of them contain these basic elements: a power source, at least one image acquisition device (camera, ccd, etc.), a processor, and control and communication cables or some kind of wireless interconnection mechanism. In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones, contain an illumination system and may be placed in a controlled environment. Furthermore, a completed system includes many accessories, such as camera supports, cables, and connectors.\nMost computer vision systems use visible-light cameras passively viewing a scene at frame rates of at most 60 frames per second (usually far slower).\nA few computer vision systems use image-acquisition hardware with active illumination or something other than visible light or both, such asstructured-light 3D scanners,thermographic cameras,hyperspectral imagers,radar imaging,lidarscanners,magnetic resonance images,side-scan sonar,synthetic aperture sonar, etc. Such hardware captures \"images\" that are then processed often using the same computer vision algorithms used to process visible-light images.\nWhile traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances indigital signal processingandconsumer graphics hardwarehas made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second. For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms. When combined with a high-speed projector, fast image acquisition allows 3D measurement and feature tracking to be realized.[55]\nEgocentric visionsystems are composed of a wearable camera that automatically take pictures from a first-person perspective.\nAs of 2016,vision processing unitsare emerging as a new class of processors to complement CPUs andgraphics processing units(GPUs) in this role.[56]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/General_game_playing",
        "title": "General game playing - Wikipedia",
        "content": "General game playing(GGP) is the design ofartificial intelligenceprograms to be able to play more than one game successfully.[1][2][3]For many games like chess, computers are programmed to play these games using a specially designed algorithm, which cannot be transferred to another context. For instance, achess-playing computer program cannot playcheckers. General game playing is considered as a necessary milestone on the way toartificial general intelligence.[4]\nGeneral video game playing(GVGP) is the concept of GGP adjusted to the purpose of playingvideo games. For video games, game rules have to be eitherlearntover multiple iterations by artificial players likeTD-Gammon,[5]or are predefined manually in adomain-specific languageand sent in advance to artificial players[6][7]like in traditional GGP. Starting in 2013, significant progress was made following thedeep reinforcement learningapproach, including the development of programs that can learn to playAtari 2600games[8][5][9][10][11]as well as a program that can learn to playNintendo Entertainment Systemgames.[12][13][14]\nThe first commercial usage of general game playing technology wasZillions of Gamesin 1998. General game playing was also proposed fortrading agentsinsupply chain managementthere under price negotiation inonline auctionsfrom 2003 onwards.[15][16][17][18]\nIn 1992,Barney Pelldefined the concept of Meta-Game Playing and developed the \"MetaGame\" system. This was the first program to automatically generate chess-like game rules, and one of the earliest programs to use automated game generation. Pell then developed the systemMetagamer.[19]This system was able to play a number of chess-like games, given game rules definition in a special language calledGame Description Language(GDL), without any human interaction once the games were generated.[20]\nIn 1998, the commercial systemZillions of Gameswas developed by Jeff Mallett and Mark Lefler. The system used a LISP-like language to define the game rules. Zillions of Games derived theevaluation functionautomatically from the game rules based on piece mobility, board structure and game goals. It also employed usual algorithms as found incomputer chesssystems:alpha–beta pruningwith move ordering,transposition tables, etc.[21]The package was extended in 2007 by the addition of the Axiom plug-in, an alternate metagame engine that incorporates a complete Forth-based programming language.\nIn 1998, z-Tree was developed byUrs Fischbacher.[22]z-Tree is the first and the most citedsoftware tool for experimental economics. z-Tree allows the definition of game rules in z-Tree-language forgame-theoretic experiments with human subjects. It also allows definition of computer players, which participate in a play with human subjects.[23]\nIn 2005, the Stanford ProjectGeneral Game Playingwas established.[3]\nIn 2012, the development of PyVGDL started.[24]\nGeneral Game Playingis a project of the Stanford Logic Group ofStanford University, California, which aims to create a platform for general game playing. It is the most well-known effort at standardizing GGP AI, and generally seen as the standard for GGP systems. The games are defined by sets of rules represented in theGame Description Language. In order to play the games, players interact with a game hosting server[25][26]that monitors moves for legality and keeps players informed of state changes.\nSince 2005, there have been annual General Game Playing competitions at theAAAIConference. The competition judges competitor AI's abilities to play a variety of different games, by recording their performance on each individual game. In the first stage of the competition, entrants are judged on their ability to perform legal moves, gain the upper hand, and complete games faster. In the following runoff round, the AIs face off against each other in increasingly complex games. The AI that wins the most games at this stage wins the competition, and until 2013 its creator used to win a $10,000 prize.[19]So far, the following programs were victorious:[27]\nOther general game playing software that use their own languages for defining game rules include:\nGVGP could potentially be used to create realvideo game AIautomatically, as well as \"to test game environments, including those created automatically using procedural content generation and to find potential loopholes in the gameplay that a human player could exploit\".[7]GVGP has also been used to generate game rules, and estimate a game's quality based on Relative Algorithm Performance Profiles (RAPP), which compare the skill differentiation that a game allows between good AI and bad AI.[42]\nTheGeneral Video Game AI Competition(GVGAI) has been running since 2014. In this competition, two-dimensional video games similar to (and sometimes based on) 1980s-era arcade and console games are used instead of the board games used in the GGP competition. It has offered a way for researchers and practitioners to test and compare their best general video game playing algorithms. The competition has an associated software framework including a large number of games written in theVideo Game Description Language (VGDL), which should not be confused withGDLand is a coding language using simple semantics and commands that can easily be parsed. One example for VGDL is PyVGDL developed in 2013.[6][24]The games used in GVGP are, for now, often 2-dimensional arcade games, as they are the simplest and easiest to quantify.[43]To simplify the process of creating an AI that can interpret video games, games for this purpose are written in VGDL manually.[clarification needed]VGDL can be used to describe a game specifically for procedural generation of levels, using Answer Set Programming (ASP) and an Evolutionary Algorithm (EA). GVGP can then be used to test the validity of procedural levels, as well as the difficulty or quality of levels based on how an agent performed.[44]\nSince GGP AI must be designed to play multiple games, its design cannot rely on algorithms created specifically for certain games. Instead, the AI must be designed using algorithms whose methods can be applied to a wide range of games. Recent GGP systems such as Regular Boardgames (RBG) and Ludii have explored alternative rule representations to optimize reasoning efficiency and support a broader variety of games. The AI must also be an ongoing process, that can adapt to its current state rather than the output of previous states. For this reason,open looptechniques are often most effective.[45]\nA popular method for developing GGP AI is theMonte Carlo tree search(MCTS) algorithm.[46]Often used together with the UCT method (Upper Confidence Bound applied to Trees), variations of MCTS have been proposed to better play certain games, as well as to make it compatible with video game playing.[47][48][49]Another variation of tree-search algorithms used is theDirected Breadth-first Search(DBS),[50]in which a child node to the current state is created for each available action, and visits each child ordered by highest average reward, until either the game ends or runs out of time.[51]In each tree-search method, the AI simulates potential actions and ranks each based on the average highest reward of each path, in terms of points earned.[46][51]\nIn order to interact with games, algorithms must operate under the assumption that games all share common characteristics. In the bookHalf-Real: Video Games Between Real Worlds and Fictional Worlds, Jesper Juul gives the following definition of games: Games are based on rules, they have variable outcomes, different outcomes give different values, player effort influences outcomes, the player is attached to the outcomes, and the game has negotiable consequences.[52]Using these assumptions, game playing AI can be created by quantifying the player input, the game outcomes, and how the various rules apply, and using algorithms to compute the most favorable path.[43]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning",
        "title": "Knowledge representation and reasoning - Wikipedia",
        "content": "Knowledge representation(KR) aims to model information in a structured manner to formally represent it asknowledgein knowledge-based systems whereasknowledge representationand reasoning(KRR,KR&R, orKR²) also aims to understand, reason, and interpret knowledge. KRR is widely used in the field ofartificial intelligence(AI) with the goal to representinformationabout the world in a form that a computer system can use to solve complex tasks, such asdiagnosing a medical conditionorhaving a natural-language dialog. KR incorporates findings from psychology[1]about how humans solve problems and represent knowledge, in order to designformalismsthat make complex systems easier to design and build. KRR also incorporates findings fromlogicto automate various kinds ofreasoning.\nTraditional KRR focuses more on the declarative representation of knowledge. Related knowledge representation formalisms mainly includevocabularies,thesaurus,semantic networks,axiom systems,frames,rules,logic programs, andontologies. Examples ofautomated reasoningengines includeinference engines,theorem provers,model generators, andclassifiers.\nIn a broader sense, parameterized models inmachine learning— includingneural networkarchitectures such asconvolutional neural networksandtransformers— can also be regarded as a family of knowledge representation formalisms. The question of which formalism is most appropriate for knowledge-based systems has long been a subject of extensive debate. For instance, Frank van Harmelen et al. discussed the suitability of logic as a knowledge representation formalism and reviewed arguments presented by anti-logicists.[2]Paul Smolensky criticized the limitations of symbolic formalisms and explored the possibilities of integrating it with connectionist approaches.[3]\nMore recently, Heng Zhang et al. have demonstrated that all universal (or equally expressive and natural) knowledge representation formalisms are recursively isomorphic.[4]This finding indicates a theoretical equivalence among mainstream knowledge representation formalisms with respect to their capacity for supportingartificial general intelligence(AGI). They further argue that while diverse technical approaches may draw insights from one another via recursive isomorphisms, the fundamental challenges remain inherently shared.\nThe earliest work in computerized knowledge representation was focused on general problem-solvers such as theGeneral Problem Solver(GPS) system developed byAllen NewellandHerbert A. Simonin 1959 and theAdvice Takerproposed byJohn McCarthyalso in 1959. GPS featured data structures for planning and decomposition. The system would begin with a goal. It would then decompose that goal into sub-goals and then set out to construct strategies that could accomplish each subgoal. The Advisor Taker, on the other hand, proposed the use of thepredicate calculusto implementcommon sense reasoning.\nMany of the early approaches to knowledge representation in Artificial Intelligence (AI) used graph representations andsemantic networks, similar toknowledge graphstoday. In such approaches, problem solving was a form of graph traversal[5]or path-finding, as in theA* search algorithm. Typical applications included robot plan-formation and game-playing.\nOther researchers focused on developingautomated theorem-proversfor first-order logic, motivated by the use ofmathematical logicto formalise mathematics and to automate the proof of mathematical theorems. A major step in this direction was the development of theresolution methodbyJohn Alan Robinson.\nIn the meanwhile, John McCarthy andPat Hayesdeveloped thesituation calculusas a logical representation of common sense knowledge about the laws of cause and effect.Cordell Green, in turn, showed how to do robot plan-formation by applying resolution to the situation calculus. He also showed how to use resolution forquestion-answeringandautomatic programming.[6]\nIn contrast, researchers at Massachusetts Institute of Technology (MIT) rejected the resolution uniform proof procedure paradigm and advocated the procedural embedding of knowledge instead.[7]The resulting conflict between the use of logical representations and the use of procedural representations was resolved in the early 1970s with the development oflogic programmingandProlog, usingSLD resolutionto treatHorn clausesas goal-reduction procedures.\nThe early development of logic programming was largely a European phenomenon. In North America, AI researchers such asEd FeigenbaumandFrederick Hayes-Rothadvocated the representation of domain-specific knowledge rather than general-purpose reasoning.[8]\nThese efforts led to thecognitive revolutionin psychology and to the phase of AI focused on knowledge representation that resulted inexpert systemsin the 1970s and 80s,production systems,frame languages, etc. Rather than general problem solvers, AI changed its focus to expert systems that could match human competence on a specific task, such as medical diagnosis.[9]\nExpert systems gave us the terminology still in use today where AI systems are divided into aknowledge base, which includes facts and rules about a problem domain, and aninference engine, which applies the knowledge in theknowledge baseto answer questions and solve problems in the domain. In these early systems the facts in the knowledge base tended to be a fairly flat structure, essentially assertions about the values of variables used by the rules.[10]\nMeanwhile,Marvin Minskydeveloped the concept offramein the mid-1970s.[11]A frame is similar to an object class: It is an abstract description of a category describing things in the world, problems, and potential solutions. Frames were originally used on systems geared toward human interaction, e.g.understanding natural languageand the social settings in which various default expectations such as ordering food in a restaurant narrow the search space and allow the system to choose appropriate responses to dynamic situations.\nIt was not long before the frame communities and the rule-based researchers realized that there was a synergy between their approaches. Frames were good for representing the real world, described as classes, subclasses, slots (data values) with various constraints on possible values. Rules were good for representing and utilizing complex logic such as the process to make a medical diagnosis. Integrated systems were developed that combined frames and rules. One of the most powerful and well known was the 1983Knowledge Engineering Environment(KEE) fromIntellicorp. KEE had a complete rule engine withforwardandbackward chaining. It also had a complete frame-based knowledge base with triggers, slots (data values), inheritance, and message passing. Although message passing originated in the object-oriented community rather than AI it was quickly embraced by AI researchers as well in environments such as KEE and in the operating systems for Lisp machines fromSymbolics,Xerox, andTexas Instruments.[12]\nThe integration of frames, rules, and object-oriented programming was significantly driven by commercial ventures such as KEE and Symbolics spun off from various research projects. At the same time, there was another strain of research that was less commercially focused and was driven by mathematical logic and automated theorem proving.[citation needed]One of the most influential languages in this research was theKL-ONElanguage of the mid-'80s. KL-ONE was aframe languagethat had a rigorous semantics, formal definitions for concepts such as anIs-A relation.[13]KL-ONE and languages that were influenced by it such asLoomhad an automated reasoning engine that was based on formal logic rather than on IF-THEN rules. This reasoner is called the classifier. A classifier can analyze a set of declarations and infer new assertions, for example, redefine a class to be a subclass or superclass of some other class that wasn't formally specified. In this way the classifier can function as an inference engine, deducing new facts from an existing knowledge base. The classifier can also provide consistency checking on a knowledge base (which in the case of KL-ONE languages is also referred to as an Ontology).[14]\nAnother area of knowledge representation research was the problem ofcommon-sense reasoning. One of the first realizations learned from trying to make software that can function with human natural language was that humans regularly draw on an extensive foundation of knowledge about the real world that we simply take for granted but that is not at all obvious to an artificial agent, such as basic principles of common-sense physics, causality, intentions, etc. An example is theframe problem, that in an event driven logic there need to be axioms that state things maintain position from one moment to the next unless they are moved by some external force. In order to make a true artificial intelligence agent that canconverse with humans using natural languageand can process basic statements and questions about the world, it is essential to represent this kind of knowledge.[15]In addition to McCarthy and Hayes' situation calculus, one of the most ambitious programs to tackle this problem was Doug Lenat'sCycproject. Cyc established its own Frame language and had large numbers of analysts document various areas of common-sense reasoning in that language. The knowledge recorded in Cyc included common-sense models of time, causality, physics, intentions, and many others.[16]\nThe starting point for knowledge representation is theknowledge representation hypothesisfirst formalized byBrian C. Smithin 1985:[17]\nAny mechanically embodied intelligent process will be comprised of structural ingredients that a) we as external observers naturally take to represent a propositional account of the knowledge that the overall process exhibits, and b) independent of such external semantic attribution, play a formal but causal and essential role in engendering the behavior that manifests that knowledge.\nOne of the most active areas of knowledge representation research is theSemantic Web.[citation needed]The Semantic Web seeks to add a layer of semantics (meaning) on top of the current Internet. Rather than indexing web sites and pages via keywords, the Semantic Web creates largeontologiesof concepts. Searching for a concept will be more effective than traditional text only searches. Frame languages and automatic classification play a big part in the vision for the future Semantic Web. The automatic classification gives developers technology to provide order on a constantly evolving network of knowledge. Defining ontologies that are static and incapable of evolving on the fly would be very limiting for Internet-based systems. The classifier technology provides the ability to deal with the dynamic environment of the Internet.\nRecent projects funded primarily by theDefense Advanced Research Projects Agency(DARPA) have integrated frame languages and classifiers with markup languages based on XML. TheResource Description Framework(RDF) provides the basic capability to define classes, subclasses, and properties of objects. TheWeb Ontology Language(OWL) provides additional levels of semantics and enables integration with classification engines.[18][19]\nKnowledge-representation is a field of artificial intelligence that focuses on designing computer representations that capture information about the world that can be used for solving complex problems.\nThe justification for knowledge representation is that conventionalprocedural codeis not the best formalism to use to solve complex problems. Knowledge representation makes complex software easier to define and maintain than procedural code and can be used inexpert systems.\nFor example, talking to experts in terms of business rules rather than code lessens the semantic gap between users and developers and makes development of complex systems more practical.\nKnowledge representation goes hand in hand withautomated reasoningbecause one of the main purposes of explicitly representing knowledge is to be able to reason about that knowledge, to make inferences, assert new knowledge, etc. Virtually allknowledge representation languageshave a reasoning or inference engine as part of the system.[20]\nA key trade-off in the design of knowledge representation formalisms is that between expressivity and tractability.[21]First Order Logic(FOL), with its high expressive power and ability to formalise much of mathematics, is a standard for comparing the expressibility of  knowledge representation languages.\nArguably, FOL has two drawbacks as a knowledge representation formalism in its own right, namely ease of use and efficiency of implementation. Firstly, because of its high expressive power, FOL allows many ways of expressing the same information, and this can make it hard for users to formalise or even to understand knowledge expressed in complex, mathematically-oriented ways. Secondly, because of its complex proof procedures, it can be difficult for users to understand complex proofs and explanations, and it can be hard for implementations to be efficient. As a consequence, unrestricted FOL can be intimidating for many software developers.\nOne of the key discoveries of AI research in the 1970s was that languages that do not have the full expressive power of FOL can still provide close to the same expressive power of FOL, but can be easier for both the average developer and for the computer to understand. Many of the early AI knowledge representation formalisms, from databases to semantic nets to production systems, can be viewed as making various design decisions about how to balance expressive power with naturalness of expression and efficiency.[22]In particular, this balancing act was a driving motivation for the development of IF-THEN rules inrule-basedexpert systems.\nA similar balancing act was also a motivation for the development oflogic programming(LP) and the logic programming languageProlog. Logic programs have a rule-based syntax, which is easily confused with the IF-THEN syntax ofproduction rules. But logic programs have a well-defined logical semantics, whereas production systems do not.\nThe earliest form of logic programming was based on theHorn clausesubset of FOL. But later extensions of LP included thenegation as failureinference rule, which turns LP into anon-monotonic logicfordefault reasoning. The resulting extended semantics of LP is a variation of the standard semantics of Horn clauses and FOL, and is a form of database semantics,[23]which includes theunique name assumptionand a form ofclosed world assumption. These assumptions are much harder to state and reason with explicitly using the standard semantics of FOL.\nIn a key 1993 paper on the topic, Randall Davis ofMIToutlined five distinct roles to analyze a knowledge representation framework:[24]\nKnowledge representation and reasoning are a key enabling technology for theSemantic Web. Languages based on the Frame model with automatic classification provide a layer of semantics on top of the existing Internet. Rather than searching via text strings as is typical today, it will be possible to define logical queries and find pages that map to those queries.[18]The automated reasoning component in these systems is an engine known as the classifier. Classifiers focus on thesubsumptionrelations in a knowledge base rather than rules. A classifier can infer new classes and dynamically change the ontology as new information becomes available. This capability is ideal for the ever-changing and evolving information space of the Internet.[25]\nThe Semantic Web integrates concepts from knowledge representation and reasoning with markup languages based on XML.  TheResource Description Framework(RDF) provides the basic capabilities to define knowledge-based objects on the Internet with basic features such as Is-A relations and object properties. TheWeb Ontology Language(OWL) adds additional semantics and integrates with automatic classification reasoners.[19]\nIn 1985,Ron Brachmancategorized the core issues for knowledge representation as follows:[26]\nIn the early years ofknowledge-based systemsthe knowledge-bases were fairly small. The knowledge-bases that were meant to actually solve real problems rather than do proof of concept demonstrations needed to focus on well defined problems. So for example, not just medical diagnosis as a whole topic, but medical diagnosis of certain kinds of diseases.\nAs knowledge-based technology scaled up, the need for larger knowledge bases and for modular knowledge bases that could communicate and integrate with each other became apparent. This gave rise to the discipline of ontology engineering, designing and building large knowledge bases that could be used by multiple projects. One of the leading research projects in this area was theCycproject. Cyc was an attempt to build a huge encyclopedic knowledge base that would contain not just expert knowledge but common-sense knowledge. In designing an artificial intelligence agent, it was soon realized that representing common-sense knowledge, knowledge that humans simply take for granted, was essential to make an AI that could interact with humans using natural language. Cyc was meant to address this problem. The language they defined was known asCycL.\nAfter CycL, a number ofontology languageshave been developed. Most aredeclarative languages, and are eitherframe languages, or are based onfirst-order logic. Modularity—the ability to define boundaries around specific domains and problem spaces—is essential for these languages because as stated byTom Gruber, \"Every ontology is a treaty–a social agreement among people with common motive in sharing.\" There are always many competing and differing views that make any general-purpose ontology impossible. A general-purpose ontology would have to be applicable in any domain and different areas of knowledge need to be unified.[30]\nThere is a long history of work attempting to build ontologies for a variety of task domains, e.g., an ontology for liquids,[31]thelumped element modelwidely used in representing electronic circuits (e.g.[32]), as well as ontologies for time, belief, and even programming itself. Each of these offers a way to see some part of the world.\nThe lumped element model, for instance, suggests that we think of circuits in terms of components with connections between them, with signals flowing instantaneously along the connections. This is a useful view, but not the only possible one. A different ontology arises if we need to attend to the electrodynamics in the device: Here signals propagate at finite speed and an object (like a resistor) that was previously viewed as a single component with an I/O behavior may now have to be thought of as an extended medium through which an electromagnetic wave flows.\nOntologies can of course be written down in a wide variety of languages and notations (e.g., logic, LISP, etc.); the essential information is not the form of that language but the content, i.e., the set of concepts offered as a way of thinking about the world. Simply put, the important part is notions like connections and components, not the choice between writing them as predicates or LISP constructs.\nThe commitment made selecting one or another ontology can produce a sharply different view of the task at hand. Consider the difference that arises in selecting the lumped element view of a circuit rather than the electrodynamic view of the same device. As a second example, medical diagnosis viewed in terms of rules (e.g.,MYCIN) looks substantially different from the same task viewed in terms of frames (e.g., INTERNIST). Where MYCIN sees the medical world as made up of empirical associations connecting symptom to disease, INTERNIST sees a set of prototypes, in particular prototypical diseases, to be matched against the case at hand."
    },
    {
        "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
        "title": "Natural language processing - Wikipedia",
        "content": "Natural language processing(NLP) is the processing ofnatural languageinformation by acomputer. The study of NLP, a subfield ofcomputer science, is generally associated withartificial intelligence. NLP is related toinformation retrieval,knowledge representation,computational linguistics, and more broadly withlinguistics.[1]\nMajor processing tasks in an NLP system include:speech recognition,text classification,natural language understanding, andnatural language generation.\nNatural language processing has its roots in the 1950s.[2]Already in 1950,Alan Turingpublished an article titled \"Computing Machinery and Intelligence\" which proposed what is now called theTuring testas a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.\nThe premise of symbolic NLP is well-summarized byJohn Searle'sChinese roomexperiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\nUp until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction ofmachine learningalgorithms for language processing.  This was due to both the steady increase in computational power (seeMoore's law) and the gradual lessening of the dominance ofChomskyantheories of linguistics (e.g.transformational grammar), whose theoretical underpinnings discouraged the sort ofcorpus linguisticsthat underlies the machine-learning approach to language processing.[9]\nSymbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[19][20]such as by writing grammars or devising heuristic rules forstemming.\nMachine learningapproaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach:\nRule-based systems are commonly used:\nIn the late 1980s and mid-1990s, the statistical approach ended a period ofAI winter, which was caused by the inefficiencies of the rule-based approaches.[21][22]\nThe earliestdecision trees, producing systems of hardif–then rules, were still very similar to the old rule-based approaches.\nOnly the introduction of hiddenMarkov models, applied to part-of-speech tagging, announced the end of the old rule-based approach.\nA major drawback of statistical methods is that they require elaboratefeature engineering. Since 2015,[23]the statistical approach has been replaced by theneural networksapproach, usingsemantic networks[24]andword embeddingsto capture semantic properties of words.\nIntermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore.\nNeural machine translation, based on then-newly inventedsequence-to-sequencetransformations, made obsolete the intermediate steps, such as word alignment, previously necessary forstatistical machine translation.\nThe following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.\nThough natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.\nBased on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:[47]\nMost higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).\nCognitionrefers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\"[48]Cognitive scienceis the interdisciplinary, scientific study of the mind and its processes.[49]Cognitive linguisticsis an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics.[50]Especially during the age ofsymbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.\nAs an example,George Lakoffoffers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[51]with two defining aspects:\nTies with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[54]functional grammar,[55]construction grammar,[56]computational psycholinguistics and cognitive neuroscience (e.g.,ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[57]of theACL). More recently, ideas of cognitive NLP have been revived as an approach to achieveexplainability, e.g., under the notion of \"cognitive AI\".[58]Likewise, ideas of cognitive NLP are inherent to neural modelsmultimodalNLP (although rarely made explicit)[59]and developments inartificial intelligence, specifically tools and technologies usinglarge language modelapproaches[60]and new directions inartificial general intelligencebased on thefree energy principle[61]by British neuroscientist and theoretician at University College LondonKarl J. Friston."
    },
    {
        "url": "https://en.wikipedia.org/wiki/Robotics",
        "title": "Robotics - Wikipedia",
        "content": "Roboticsis theinterdisciplinarystudy and practice of the design, construction, operation, and use ofrobots.[1]\nWithinmechanical engineering, robotics is the design and construction of the physical structures of robots, while incomputer science, robotics focuses on robotic automation algorithms. Other disciplines contributing to robotics includeelectrical,control,software,information,electronic,telecommunication,computer,mechatronic, andmaterialsengineering.\nThe goal of most robotics is to design machines that can help and assisthumans. Many robots are built to do jobs that are hazardous to people, such as finding survivors in unstable ruins, and exploring space, mines and shipwrecks. Others replace people in jobs that are boring, repetitive, or unpleasant, such as cleaning, monitoring, transporting, and assembling. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes.\nRobotics usually combines three aspects of design work to createrobotsystems:\nAs many robots are designed for specific tasks, this method of classification becomes more relevant. For example, many robots are designed for assembly work, which may not be readily adaptable for other applications. They are termed \"assembly robots\". For seam welding, some suppliers provide complete welding systems with the robot i.e. the welding equipment along with other material handling facilities like turntables, etc. as an integrated unit. Such an integrated robotic system is called a \"welding robot\" even though its discrete manipulator unit could be adapted to a variety of tasks. Some robots are specifically designed for heavy load manipulation, and are labeled as \"heavy-duty robots\".[4]\nCurrent and potential applications include:\nAt present, mostly (lead–acid)batteriesare used as a power source. Many different types of batteries can be used as a power source for robots. They range from lead–acid batteries, which are safe and have relatively long shelf lives but are rather heavy compared to silver–cadmium batteries which are much smaller in volume and are currently much more expensive. Designing a battery-powered robot needs to take into account factors such as safety, cycle lifetime, andweight. Generators, often some type ofinternal combustion engine, can also be used. However, such designs are often mechanically complex and need fuel, require heat dissipation, and are relatively heavy. A tether connecting the robot to a power supply would remove the power supply from the robot entirely. This has the advantage of saving weight and space by moving all power generation and storage components elsewhere. However, this design does come with the drawback of constantly having a cable connected to the robot, which can be difficult to manage.[16]Potential power sources could be:\nActuators are the \"muscles\" of a robot, the parts which convertstored energyinto movement.[17]By far the most popular actuators are electric motors that rotate a wheel or gear, and linear actuators that control industrial robots in factories. There are some recent advances in alternative types of actuators, powered by electricity, chemicals, or compressed air.\nThe vast majority of robots useelectric motors, oftenbrushedandbrushless DC motorsin portable robots or AC motors in industrial robots andCNCmachines. These motors are often preferred in systems with lighter loads, and where the predominant form of motion is rotational.\nVarious types of linear actuators move in and out instead of by spinning, and often have quicker direction changes, particularly when very large forces are needed such as with industrial robotics. They are typically powered by compressed and oxidized air (pneumatic actuator) or an oil (hydraulic actuator) Linear actuators can also be powered by electricity which usually consists of a motor and a leadscrew. Another common type is a mechanical linear actuator such as a rack and pinion on a car.\nSeries elastic actuation (SEA) relies on the idea of introducing intentional elasticity between the motor actuator and the load for robust force control. Due to the resultant lower reflected inertia, series elastic actuation improves safety when a robot interacts with the environment (e.g., humans or workpieces) or during collisions.[18]Furthermore, it also providesenergy efficiencyand shock absorption (mechanical filtering) while reducing excessive wear on the transmission and other mechanical components. This approach has successfully been employed in various robots, particularly advanced manufacturing robots[19]and walkinghumanoidrobots.[20][21]\nThe controller design of a series elastic actuator is most often performed within thepassivityframework as it ensures the safety of interaction with unstructured environments.[22]Despite its remarkable stability and robustness, this framework suffers from the stringent limitations imposed on the controller which may trade-off performance. The reader is referred to the following survey which summarizes the common controller architectures for SEA along with the correspondingsufficientpassivity conditions.[23]One recent study has derived thenecessary and sufficientpassivity conditions for one of the most commonimpedance controlarchitectures, namely velocity-sourced SEA.[24]This work is of particular importance as it drives the non-conservative passivity bounds in an SEA scheme for the first time which allows a larger selection of control gains.\nPneumatic artificial muscles also known as air muscles, are special tubes that expand (typically up to 42%) when air is forced inside them. They are used in some robot applications.[25][26][27]\nMuscle wire, also known as shape memory alloy, is a material that contracts (under 5%) when electricity is applied. They have been used for some small robot applications.[28][29]\nEAPs or EPAMs are a plastic material that can contract substantially (up to 380% activation strain) from electricity, and have been used in facial muscles and arms of humanoid robots,[30]and to enable new robots to float,[31]fly, swim or walk.[32]\nRecent alternatives to DC motors arepiezo motorsorultrasonic motors. These work on a fundamentally different principle, whereby tinypiezoceramicelements, vibrating many thousands of times per second, cause linear or rotary motion. There are different mechanisms of operation; one type uses the vibration of the piezo elements to step the motor in a circle or a straight line.[33]Another type uses the piezo elements to cause a nut to vibrate or to drive a screw. The advantages of these motors arenanometerresolution, speed, and available force for their size.[34]These motors are already available commercially and being used on some robots.[35][36]\nElastic nanotubes are a promising artificial muscle technology in early-stage experimental development. The absence of defects incarbon nanotubesenables these filaments to deform elastically by several percent, with energy storage levels of perhaps 10J/cm3for metal nanotubes. Human biceps could be replaced with an 8 mm diameter wire of this material. Such compact \"muscle\" might allow future robots to outrun and outjump humans.[37]\nSensors allow robots to receive information about a certain measurement of the environment, or internal components. This is essential for robots to perform their tasks, and act upon any changes in the environment to calculate the appropriate response. They are used for various forms of measurements, to give the robots warnings about safety or malfunctions, and to provide real-time information about the task it is performing.\nCurrentroboticandprosthetic handsreceive far lesstactileinformation than the human hand. Recent research has developed a tactilesensor arraythat mimics the mechanical properties and touch receptors of human fingertips.[38][39]The sensor array is constructed as a rigid core surrounded by conductive fluid contained by an elastomeric skin. Electrodes are mounted on the surface of the rigid core and are connected to an impedance-measuring device within the core. When the artificial skin touches an object the fluid path around the electrodes is deformed, producing impedance changes that map the forces received from the object. The researchers expect that an important function of such artificial fingertips will be adjusting the robotic grip on held objects.\nScientists from severalEuropean countriesandIsraeldeveloped aprosthetichand in 2009, called SmartHand, which functions like a real one —allowing patients to write with it, type on akeyboard, play piano, and perform other fine movements. The prosthesis has sensors which enable the patient to sense real feelings in its fingertips.[40]\nOther common forms of sensing in robotics use lidar, radar, and sonar.[41]Lidarmeasures the distance to a target by illuminating the target with laser light and measuring the reflected light with a sensor.Radaruses radio waves to determine the range, angle, or velocity of objects.Sonaruses sound propagation to navigate, communicate with or detect objects on or under the surface of the water.\nOne of the most common types of end-effectors are \"grippers\". In its simplest manifestation, it consists of just two fingers that can open and close to pick up and let go of a range of small objects. Fingers can, for example, be made of a chain with a metal wire running through it.[42]Hands that resemble and work more like a human hand include theShadow Handand theRobonauthand.[43]Hands that are of a mid-level complexity include theDelfthand.[44][45]Mechanical grippers can come in various types, including friction and encompassing jaws. Friction jaws use all the force of the gripper to hold the object in place using friction. Encompassing jaws cradle the object in place, using less friction.\nSuction end-effectors, powered by vacuum generators, are very simple astrictive[46]devices that can hold very large loads provided theprehensionsurface is smooth enough to ensure suction.\nPick and place robots for electronic components and for large objects like car windscreens, often use very simple vacuum end-effectors.\nSuction is a highly used type of end-effector in industry, in part because the naturalcomplianceof soft suction end-effectors can enable a robot to be more robust in the presence of imperfect robotic perception. As an example: consider the case of a robot vision system that estimates the position of a water bottle but has 1 centimeter of error. While this may cause a rigid mechanical gripper to puncture the water bottle, the soft suction end-effector may just bend slightly and conform to the shape of the water bottle surface.\nSome advanced robots are beginning to use fully humanoid hands, like the Shadow Hand, MANUS,[47]and the Schunk hand.[48]They have powerful Robot Dexterity Intelligence (RDI), with as many as 20degrees of freedomand hundreds of tactile sensors.[49]\nThe mechanical structure of a robot must be controlled to perform tasks.[50]The control of a robot involves three distinct phases –perception, processing, and action (robotic paradigms).[51]Sensorsgive information about the environment or the robot itself (e.g. the position of its joints or its end effector). This information is then processed to be stored or transmitted and to calculate the appropriate signals to the actuators (motors), which move the mechanical structure to achieve the required co-ordinated motion or force actions.\nThe processing phase can range in complexity. At a reactive level, it may translate raw sensor information directly into actuator commands (e.g. firing motor power electronic gates based directly upon encoder feedback signals to achieve the required torque/velocity of the shaft).Sensor fusionand internal models may first be used to estimate parameters of interest (e.g. the position of the robot's gripper) from noisy sensor data. An immediate task (such as moving the gripper in a certain direction until an object is detected with a proximity sensor) is sometimes inferred from these estimates. Techniques fromcontrol theoryare generally used to convert the higher-level tasks into individual commands that drive the actuators, most often using kinematic and dynamic models of the mechanical structure.[50][51][52]\nAt longer time scales or with more sophisticated tasks, the robot may need to build and reason with a \"cognitive\" model.Cognitive modelstry to represent the robot, the world, and how the two interact. Pattern recognition and computer vision can be used to track objects.[50]Mappingtechniques can be used to build maps of the world. Finally,motion planningand otherartificial intelligencetechniques may be used to figure out how to act. For example, a planner may figure out how to achieve a task without hitting obstacles, falling over, etc.\nModern commercial robotic control systems are highly complex, integrate multiple sensors and effectors, have many interacting degrees-of-freedom (DOF) and require operator interfaces, programming tools and real-time capabilities.[51]They are oftentimes interconnected to wider communication networks and in many cases are now bothIoT-enabled and mobile.[53]Progress towards open architecture, layered, user-friendly and 'intelligent' sensor-based interconnected robots has emerged from earlier concepts related toFlexible Manufacturing Systems(FMS), and several 'open or 'hybrid'reference architecturesexist which assist developers of robot control software and hardware to move beyond traditional, earlier notions of 'closed' robot control systems have been proposed.[52]Open architecture controllers are said to be better able to meet the growing requirements of a wide range of robot users, including system developers, end users and research scientists, and are better positioned to deliver the advanced robotic concepts related toIndustry 4.0.[52]In addition to utilizing many established features of robot controllers, such as position, velocity and force control of end effectors, they also enable IoT interconnection and the implementation of more advanced sensor fusion and control techniques, including adaptive control,Fuzzy controlandArtificial Neural Network(ANN)-based control.[52]When implemented in real-time, such techniques can potentially improve the stability and performance of robots operating in unknown or uncertain environments by enabling the control systems to learn and adapt to environmental changes.[54]There are several examples of reference architectures for robot controllers, and also examples of successful implementations of actual robot controllers developed from them. One example of a generic reference architecture and associated interconnected, open-architecture robot and controller implementation was used in a number of research and development studies, including prototype implementation of novel advanced and intelligent control and environment mapping methods in real-time.[54][55]\nA definition of robotic manipulation has been provided by Matt Mason as: \"manipulation refers to an agent's control of its environment through selective contact\".[56]\nRobots need to manipulate objects; pick up, modify, destroy, move or otherwise have an effect. Thus the functional end of a robot arm intended to make the effect (whether a hand, or tool) are often referred to asend effectors,[57]while the \"arm\" is referred to as amanipulator.[58]Most robot arms have replaceable end-effectors, each allowing them to perform some small range of tasks. Some have a fixed manipulator that cannot be replaced, while a few have one very general-purpose manipulator, for example, a humanoid hand.[59]\nFor simplicity, most mobile robots have fourwheelsor a number ofcontinuous tracks. Some researchers have tried to create more complex wheeled robots with only one or two wheels. These can have certain advantages such as greater efficiency and reduced parts, as well as allowing a robot to navigate in confined places that a four-wheeled robot would not be able to.\nBalancing robots generally use agyroscopeto detect how much a robot is falling and then drive the wheels proportionally in the same direction, to counterbalance the fall at hundreds of times per second, based on the dynamics of aninverted pendulum.[60]Many different balancing robots have been designed.[61]While theSegwayis not commonly thought of as a robot, it can be thought of as a component of a robot, when used as such Segway refer to them as RMP (Robotic Mobility Platform). An example of this use has been asNASA'sRobonautthat has been mounted on a Segway.[62]\nA one-wheeled balancing robot is an extension of a two-wheeled balancing robot so that it can move in any 2D direction using a round ball as its only wheel. Several one-wheeled balancing robots have been designed recently, such asCarnegie Mellon University's \"Ballbot\" which is the approximate height and width of a person, andTohoku Gakuin University's \"BallIP\".[63]Because of the long, thin shape and ability to maneuver in tight spaces, they have the potential to function better than other robots in environments with people.[64]\nSeveral attempts have been made in robots that are completely inside a spherical ball, either by spinning a weight inside the ball,[65][66]or by rotating the outer shells of the sphere.[67][68]These have also been referred to as anorb bot[69]or a ball bot.[70][71]\nUsing six wheels instead of four wheels can give better traction or grip in outdoor terrain such as on rocky dirt or grass.\nTracks provide even more traction than a six-wheeled robot. Tracked wheels behave as if they were made of hundreds of wheels, therefore are very common for outdoor off-road robots, where the robot must drive on very rough terrain. However, they are difficult to use indoors such as on carpets and smooth floors. Examples include NASA's Urban Robot \"Urbie\".[72]\nWalking is a difficult and dynamic problem to solve. Several robots have been made which can walk reliably on two legs, however, none have yet been made which are as robust as a human. There has been much study on human-inspired walking, such as AMBER lab which was established in 2008 by the Mechanical Engineering Department at Texas A&M University.[73]Many other robots have been built that walk on more than two legs, due to these robots being significantly easier to construct.[74][75]Walking robots can be used for uneven terrains, which would provide better mobility and energy efficiency than other locomotion methods. Typically, robots on two legs can walk well on flat floors and can occasionally walk upstairs. None can walk over rocky, uneven terrain. Some of the methods which have been tried are:\nThe zero moment point (ZMP) is the algorithm used by robots such asHonda'sASIMO. The robot's onboard computer tries to keep the totalinertial forces(the combination ofEarth'sgravityand theaccelerationand deceleration of walking), exactly opposed by the floorreaction force(the force of the floor pushing back on the robot's foot). In this way, the two forces cancel out, leaving nomoment(force causing the robot to rotate and fall over).[76]However, this is not exactly how a human walks, and the difference is obvious to human observers, some of whom have pointed out that ASIMO walks as if it needs thelavatory.[77][78][79]ASIMO's walking algorithm is not static, and some dynamic balancing is used (see below). However, it still requires a smooth surface to walk on.\nSeveral robots, built in the 1980s byMarc Raibertat theMITLeg Laboratory, successfully demonstrated very dynamic walking. Initially, a robot with only one leg, and a very small foot could stay upright simply byhopping. The movement is the same as that of a person on apogo stick. As the robot falls to one side, it would jump slightly in that direction, in order to catch itself.[80]Soon, the algorithm was generalised to two and four legs. A bipedal robot was demonstrated running and even performingsomersaults.[81]Aquadrupedwas also demonstrated which couldtrot, run,pace, and bound.[82]For a full list of these robots, see the MIT Leg Lab Robots page.[83]\nA more advanced way for a robot to walk is by using a dynamic balancing algorithm, which is potentially more robust than the Zero Moment Point technique, as it constantly monitors the robot's motion, and places the feet in order to maintain stability.[84]This technique was recently demonstrated byAnybots'Dexter Robot,[85]which is so stable, it can even jump.[86]Another example is theTU Delft Flame.\nPerhaps the most promising approach usespassive dynamicswhere themomentumof swinging limbs is used for greaterefficiency. It has been shown that totally unpowered humanoid mechanisms can walk down a gentle slope, using onlygravityto propel themselves. Using this technique, a robot need only supply a small amount of motor power to walk along a flat surface or a little more to walk up ahill. This technique promises to make walking robots at least ten times more efficient than ZMP walkers, like ASIMO.[87][88]\nA modernpassenger airlineris essentially aflyingrobot, with two humans to manage it. Theautopilotcan control the plane for each stage of the journey, including takeoff, normal flight, and even landing.[89]Other flying robots are uninhabited and are known asunmanned aerial vehicles(UAVs). They can be smaller and lighter without a human pilot on board, and fly into dangerous territory for military surveillance missions. Some can even fire on targets under command. UAVs are also being developed which can fire on targets automatically, without the need for a command from a human. Other flying robots includecruise missiles, theEntomopter, and theEpson micro helicopter robot. Robots such as the Air Penguin, Air Ray, and Air Jelly have lighter-than-air bodies, are propelled by paddles, and are guided by sonar.\nBFRs take inspiration from flying mammals, birds, or insects. BFRs can have flapping wings, which generate the lift and thrust, or they can be propeller actuated. BFRs with flapping wings have increased stroke efficiencies, increased maneuverability, and reduced energy consumption in comparison to propeller actuated BFRs.[90]Mammal and bird inspired BFRs share similar flight characteristics and design considerations. For instance, both mammal and bird inspired BFRs minimizeedge flutteringandpressure-induced wingtip curlby increasing the rigidity of the wing edge and wingtips. Mammal and insect inspired BFRs can be impact resistant, making them useful in cluttered environments.\nMammal inspired BFRs typically take inspiration from bats, but the flying squirrel has also inspired a prototype.[91]Examples of bat inspired BFRs include Bat Bot[92]and the DALER.[93]Mammal inspired BFRs can be designed to be multi-modal; therefore, they're capable of both flight and terrestrial movement. To reduce the impact of landing, shock absorbers can be implemented along the wings.[93]Alternatively, the BFR can pitch up and increase the amount of drag it experiences.[91]By increasing the drag force, the BFR will decelerate and minimize the impact upon grounding. Different land gait patterns can also be implemented.[91]\nBird inspired BFRs can take inspiration from raptors, gulls, and everything in-between. Bird inspired BFRs can be feathered to increase the angle of attack range over which the prototype can operate before stalling.[94]The wings of bird inspired BFRs allow for in-plane deformation, and the in-plane wing deformation can be adjusted to maximize flight efficiency depending on the flight gait.[94]An example of a raptor inspired BFR is the prototype by Savastano et al.[95]The prototype has fully deformable flapping wings and is capable of carrying a payload of up to 0.8 kg while performing a parabolic climb, steep descent, and rapid recovery. The gull inspired prototype by Grant et al. accurately mimics the elbow and wrist rotation of gulls, and they find that lift generation is maximized when the elbow and wrist deformations are opposite but equal.[96]\nInsect inspired BFRs typically take inspiration from beetles or dragonflies. An example of a beetle inspired BFR is the prototype by Phan and Park,[97]and a dragonfly inspired BFR is the prototype by Hu et al.[98]The flapping frequency of insect inspired BFRs are much higher than those of other BFRs; this is because of theaerodynamics of insect flight.[99]Insect inspired BFRs are much smaller than those inspired by mammals or birds, so they are more suitable for dense environments.\nA class of robots that are biologically inspired, but which do not attempt to mimic biology, are creations such as theEntomopter. Funded byDARPA,NASA, theUnited States Air Force, and theGeorgia Tech Research Instituteand patented by Prof.Robert C. Michelsonfor covert terrestrial missions as well as flight in the lowerMarsatmosphere, the Entomopter flight propulsion system uses lowReynolds numberwings similar to those of thehawk moth(Manduca sexta), but flaps them in a non-traditional \"opposed x-wing fashion\" while \"blowing\" the surface to enhance lift based on theCoandă effectas well as to control vehicle attitude and direction. Waste gas from the propulsion system not only facilitates the blown wing aerodynamics, but also serves to create ultrasonic emissions like that of aBatfor obstacle avoidance. The Entomopter and other biologically-inspired robots leverage features of biological systems, but do not attempt to create mechanical analogs.\nSeveralsnakerobots have been successfully developed. Mimicking the way real snakes move, these robots can navigate very confined spaces, meaning they may one day be used to search for people trapped in collapsed buildings.[100]The Japanese ACM-R5 snake robot[101]can even navigate both on land and in water.[102]\nA small number ofskatingrobots have been developed, one of which is a multi-mode walking and skating device. It has four legs, with unpowered wheels, which can either step or roll.[103]Another robot, Plen, can use a miniature skateboard or roller-skates, and skate across a desktop.[104]\nSeveral different approaches have been used to develop robots that have the ability to climb vertical surfaces. One approach mimics the movements of a humanclimberon a wall with protrusions; adjusting thecenter of massand moving each limb in turn to gain leverage. An example of this is Capuchin,[105]built by Ruixiang Zhang at Stanford University, California. Another approach uses the specialized toe pad method of wall-climbinggeckoes, which can run on smooth surfaces such as vertical glass. Examples of this approach include Wallbot[106]and Stickybot.[107]\nChina'sTechnology Dailyreported on 15 November 2008, that Li Hiu Yeung and his research group of New Concept Aircraft (Zhuhai) Co., Ltd. had successfully developed a bionic gecko robot named \"Speedy Freelander\". According to Yeung, the gecko robot could rapidly climb up and down a variety of building walls, navigate through ground and wall fissures, and walk upside-down on the ceiling. It was also able to adapt to the surfaces of smooth glass, rough, sticky or dusty walls as well as various types of metallic materials. It could also identify and circumvent obstacles automatically. Its flexibility and speed were comparable to a natural gecko. A third approach is to mimic the motion of a snake climbing a pole.[41]\nIt is calculated that whenswimmingsome fish can achieve apropulsiveefficiency greater than 90%.[108]Furthermore, they can accelerate and maneuver far better than any man-madeboatorsubmarine, and produce less noise and water disturbance. Therefore, many researchers studying underwater robots would like to copy this type of locomotion.[109]Notable examples are the Robotic Fish G9,[110]and Robot Tuna built to analyze and mathematically modelthunniform motion.[111]The Aqua Penguin,[112]copies the streamlined shape and propulsion by front \"flippers\" ofpenguins. The Aqua Ray and Aqua Jelly emulate the locomotion of manta ray, and jellyfish, respectively.\nIn 2014,iSplash-II was developed as the firstrobotic fishcapable of outperforming real carangiform fish in terms of average maximum velocity (measured in body lengths/ second) and endurance, the duration that top speed is maintained.[113]This build attained swimming speeds of 11.6BL/s (i.e. 3.7 m/s).[114]The first build,iSplash-I (2014) was the first robotic platform to apply a full-body lengthcarangiformswimming motion which was found to increase swimming speed by 27% over the traditional approach of a posterior confined waveform.[115]\nSailboat robots have also been developed in order to make measurements at the surface of the ocean. A typical sailboat robot isVaimos.[116]Since the propulsion of sailboat robots uses the wind, the energy of the batteries is only used for the computer, for the communication and for the actuators (to tune the rudder and the sail). If the robot is equipped with solar panels, the robot could theoretically navigate forever. The two main competitions of sailboat robots areWRSC, which takes place every year in Europe, andSailbot.\nControl systems may also have varying levels of autonomy.\nAnother classification takes into account the interaction between human control and the machine motions.\nComputer visionis the science and technology of machines that see. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences and views from cameras.\nIn most practical computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common.\nComputer vision systems rely on image sensors that detect electromagnetic radiation which is typically in the form of eithervisible lightorinfra-red light. The sensors are designed usingsolid-state physics. The process by which light propagates and reflects off surfaces is explained usingoptics. Sophisticated image sensors even requirequantum mechanicsto provide a complete understanding of the image formation process. Robots can also be equipped with multiple vision sensors to be better able to compute the sense of depth in the environment. Like human eyes, robots' \"eyes\" must also be able to focus on a particular area of interest, and also adjust to variations in light intensities.\nThere is a subfield within computer vision where artificial systems are designed to mimic the processing and behavior ofbiological system, at different levels of complexity. Also, some of the learning-based methods developed within computer vision have a background in biology.\nThough a significant percentage of robots in commission today are either human controlled or operate in a static environment, there is an increasing interest in robots that can operate autonomously in a dynamic environment. These robots require some combination ofnavigation hardware and softwarein order to traverse their environment. In particular, unforeseen events (e.g. people and other obstacles that are not stationary) can cause problems or collisions. Some highly advanced robots such asASIMOandMeinü robothave particularly good robot navigation hardware and software. Also,self-controlled cars,Ernst Dickmanns'driverless car, and the entries in theDARPA Grand Challenge, are capable of sensing the environment well and subsequently making navigational decisions based on this information, including by a swarm of autonomous robots.[119]Most of these robots employ aGPSnavigation device with waypoints, along withradar, sometimes combined with other sensory data such aslidar,video cameras, andinertial guidance systemsfor better navigation between waypoints.\nThe state of the art in sensory intelligence for robots will have to progress through several orders of magnitude if we want the robots working in our homes to go beyond vacuum-cleaning the floors. If robots are to work effectively in homes and other non-industrial environments, the way they are instructed to perform their jobs, and especially how they will be told to stop will be of critical importance. The people who interact with them may have little or no training in robotics, and so any interface will need to be extremely intuitive. Science fiction authors also typically assume that robots will eventually be capable of communicating with humans throughspeech,gestures, andfacial expressions, rather than acommand-line interface. Although speech would be the most natural way for the human to communicate, it is unnatural for the robot. It will probably be a long time before robots interact as naturally as the fictionalC-3PO, orData of Star Trek, Next Generation. Even though the current state of robotics cannot meet the standards of these robots from science-fiction, robotic media characters (e.g., Wall-E, R2-D2) can elicit audience sympathies that increase people's willingness to accept actual robots in the future.[120]Acceptance of social robots is also likely to increase if people can meet a social robot under appropriate conditions. Studies have shown that interacting with a robot by looking at, touching, or even imagining interacting with the robot can reduce negative feelings that some people have about robots before interacting with them.[121]However, if pre-existing negative sentiments are especially strong, interacting with a robot can increase those negative feelings towards robots.[121]\nInterpreting the continuous flow ofsoundscoming from a human, inreal time, is a difficult task for a computer, mostly because of the great variability ofspeech.[122]The same word, spoken by the same person may sound different depending on localacoustics,volume, the previous word, whether or not the speaker has acold, etc.. It becomes even harder when the speaker has a differentaccent.[123]Nevertheless, great strides have been made in the field since Davis, Biddulph, and Balashek designed the first \"voice input system\" which recognized \"ten digits spoken by a single user with 100% accuracy\" in 1952.[124]Currently, the best systems can recognize continuous, natural speech, up to 160 words per minute, with an accuracy of 95%.[125]With the help of artificial intelligence, machines nowadays can use people's voice toidentify their emotionssuch as satisfied or angry.[126]\nOther hurdles exist when allowing the robot to use voice for interacting with humans. For social reasons,synthetic voiceproves suboptimal as a communication medium,[127]making it necessary to develop the emotional component of robotic voice through various techniques.[128][129]An advantage of diphonic branching is the emotion that the robot is programmed to project, can be carried on the voice tape, or phoneme, already pre-programmed onto the voice media. One of the earliest examples is a teaching robot named Leachim developed in 1974 byMichael J. Freeman.[130][131]Leachim was able to convert digital memory to rudimentary verbal speech on pre-recorded computer discs.[132]It was programmed to teach students inThe Bronx, New York.[132]\nFacial expressions can provide rapid feedback on the progress of a dialog between two humans, and soon may be able to do the same for humans and robots. Robotic faces have been constructed byHanson Roboticsusing their elastic polymer calledFrubber, allowing a large number of facial expressions due to the elasticity of the rubber facial coating and embedded subsurface motors (servos).[133]The coating and servos are built on a metalskull. A robot should know how to approach a human, judging by their facial expression andbody language. Whether the person is happy, frightened, or crazy-looking affects the type of interaction expected of the robot. Likewise, robots likeKismetand the more recent addition, Nexi[134]can produce a range of facial expressions, allowing it to have meaningful social exchanges with humans.[135]\nOne can imagine, in the future, explaining to a robot chef how to make a pastry, or asking directions from a robot police officer. In both of these cases, making handgestureswould aid the verbal descriptions. In the first case, the robot would be recognizing gestures made by the human, and perhaps repeating them for confirmation. In the second case, the robot police officer would gesture to indicate \"down the road, then turn right\". It is likely that gestures will make up a part of the interaction between humans and robots.[136]A great many systems have been developed to recognize human hand gestures.[137]\nProxemicsis the study of personal space, and HRI systems may try to model and work with its concepts for human interactions.\nArtificial emotionscan also be generated, composed of a sequence of facial expressions or gestures. As can be seen from the movieFinal Fantasy: The Spirits Within, the programming of these artificial emotions is complex and requires a large amount of human observation. To simplify this programming in the movie, presets were created together with a special software program. This decreased the amount of time needed to make the film. These presets could possibly be transferred for use in real-life robots. An example of a robot with artificial emotions isRobin the Robot[hy]developed by anArmenianIT company Expper Technologies, which uses AI-based peer-to-peer interaction. Its main task is achieving emotional well-being, i.e. overcome stress and anxiety. Robin was trained to analyze facial expressions and use his face to display his emotions given the context. The robot has been tested by kids in US clinics, and observations show that Robin increased the appetite and cheerfulness of children after meeting and talking.[138]\nMany of the robots of science fiction have apersonality, something which may or may not be desirable in the commercial robots of the future.[139]Nevertheless, researchers are trying to create robots which appear to have a personality:[140][141]i.e. they use sounds, facial expressions, and body language to try to convey an internal state, which may be joy, sadness, or fear. One commercial example isPleo, a toy robot dinosaur, which can exhibit several apparent emotions.[142]\nMuch of the research in robotics focuses not on specific industrial tasks, but on investigations into newtypes of robots, alternative ways to think about or design robots, and new ways to manufacture them. Other investigations, such as MIT'scyberfloraproject, are almost wholly academic.\nTo describe the level of advancement of a robot, the term \"Generation Robots\" can be used. This term is coined by ProfessorHans Moravec, Principal Research Scientist at theCarnegie Mellon UniversityRobotics Institutein describing the near future evolution of robot technology.First-generationrobots, Moravec predicted in 1997, should have an intellectual capacity comparable to perhaps alizardand should become available by 2010. Because thefirst generationrobot would be incapable oflearning, however, Moravec predicts that thesecond generationrobot would be an improvement over thefirstand become available by 2020, with the intelligence maybe comparable to that of amouse. Thethird generationrobot should have intelligence comparable to that of amonkey. Thoughfourth generationrobots, robots withhumanintelligence, professor Moravec predicts, would become possible, he does not predict this happening before around 2040 or 2050.[143]\nThe study of motion can be divided intokinematicsanddynamics.[144]Direct kinematics or forward kinematics refers to the calculation of end effector position, orientation,velocity, andaccelerationwhen the corresponding joint values are known.Inverse kinematicsrefers to the opposite case in which required joint values are calculated for given end effector values, as done in path planning. Some special aspects of kinematics include handling of redundancy (different possibilities of performing the same movement),collisionavoidance, andsingularityavoidance. Once all relevant positions, velocities, and accelerations have been calculated usingkinematics, methods from the field ofdynamicsare used to study the effect offorcesupon these movements. Direct dynamics refers to the calculation of accelerations in the robot once the applied forces are known. Direct dynamics is used incomputer simulationsof the robot.Inverse dynamicsrefers to the calculation of the actuator forces necessary to create a prescribed end-effector acceleration. This information can be used to improve the control algorithms of a robot.\nIn each area mentioned above, researchers strive to develop new concepts and strategies, improve existing ones, and improve the interaction between these areas. To do this, criteria for \"optimal\" performance and ways to optimize design, structure, and control of robots must be developed and implemented.\nOpen source roboticsresearch seeks standards for defining, and methods for designing and building, robots so that they can easily be reproduced by anyone. Research includes legal and technical definitions; seeking out alternative tools and materials to reduce costs and simplify builds; and creating interfaces and standards for designs to work together. Human usability research also investigates how to best document builds through visual, text or video instructions.\nEvolutionary robotsis amethodologythat usesevolutionary computationto help design robots, especially the body form, or motion and behaviorcontrollers. In a similar way tonatural evolution, a large population of robots is allowed to compete in some way, or their ability to perform a task is measured using afitness function. Those that perform worst are removed from the population and replaced by a new set, which have new behaviors based on those of the winners. Over time the population improves, and eventually a satisfactory robot may appear. This happens without any direct programming of the robots by the researchers. Researchers use this method both to create better robots,[145]and to explore the nature of evolution.[146]Because the process often requires many generations of robots to be simulated,[147]this technique may be run entirely or mostly insimulation, using arobot simulatorsoftware package, then tested on real robots once the evolved algorithms are good enough.[148]According to theInternational Federation of Robotics(IFR) studyWorld Robotics 2023, there were about 4,281,585 operational industrial robots by the end of 2023[149]\nBionicsandbiomimeticsapply the physiology and methods of locomotion of animals to the design of robots. For example, the design ofBionicKangaroowas based on the way kangaroos jump.\nSwarm roboticsis an approach to the coordination of multiple robots as a system which consist of large numbers of mostly simple physical robots. ″In a robot swarm, the collective behavior of the robots results from local interactions between the robots and between the robots and the environment in which they act.″*[119]\nThere has been some research into whether robotics algorithms can be run more quickly onquantum computersthan they can be run ondigital computers. This area has been referred to as quantum robotics.[150]\nThe main venues for robotics research are the international conferences ICRA and IROS.\nRobotics engineers design robots, maintain them, develop new applications for them, and conduct research to expand the potential of robotics.[153]Robots have become a popular educational tool in some middle and high schools, particularly in parts of the USA,[154]as well as in numerous youth summer camps, raising interest in programming, artificial intelligence, and robotics among students.\nRobotics is an essential component in many modern manufacturing environments. As factories increase their use of robots, the number of robotics–related jobs grow and have been observed to be steadily rising.[155]The employment of robots in industries has increased productivity and efficiency savings and is typically seen as a long-term investment for benefactors. A study found that 47 percent of US jobs are at risk to automation \"over some unspecified number of years\".[156]These claims have been criticized on the ground that social policy, not AI, causes unemployment.[157]In a 2016 article in The Guardian, Stephen Hawking stated \"The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining\".[158]The rise of robotics is thus often used as an argument foruniversal basic income.\nAccording to a GlobalData September 2021 report, the robotics industry was worth $45bn in 2020, and by 2030, it will have grown at a compound annual growth rate (CAGR) of 29% to $568bn, driving jobs in robotics and related industries.[159]\nA discussion paper drawn up byEU-OSHAhighlights how the spread of robotics presents both opportunities and challenges for occupational safety and health (OSH).[160]\nThe greatest OSH benefits stemming from the wider use of robotics should be substitution for people working in unhealthy or dangerous environments. In space, defense, security, or the nuclear industry, but also in logistics, maintenance, and inspection, autonomous robots are particularly useful in replacing human workers performing dirty, dull or unsafe tasks, thus avoiding workers' exposures to hazardous agents and conditions and reducing physical, ergonomic and psychosocial risks. For example, robots are already used to perform repetitive and monotonous tasks, to handle radioactive material or to work in explosive atmospheres. In the future, many other highly repetitive, risky or unpleasant tasks will be performed by robots in a variety of sectors like agriculture, construction, transport, healthcare, firefighting or cleaning services.[161]\nMoreover, there are certain skills to which humans will be better suited than machines for some time to come and the question is how to achieve the best combination of human and robot skills. The advantages of robotics include heavy-duty jobs with precision and repeatability, whereas the advantages of humans include creativity, decision-making, flexibility, and adaptability. This need to combine optimal skills has resulted incollaborative robotsand humans sharing a common workspace more closely and led to the development of new approaches and standards to guarantee the safety of the \"man-robot merger\". Some European countries are including robotics in their national programs and trying to promote a safe and flexible cooperation between robots and operators to achieve better productivity. For example, the German Federal Institute for Occupational Safety and Health (BAuA) organises annual workshops on the topic \"human-robot collaboration\".\nIn the future, cooperation between robots and humans will be diversified, with robots increasing their autonomy and human-robot collaboration reaching completely new forms. Current approaches and technical standards[162][163]aiming to protect employees from the risk of working with collaborative robots will have to be revised.\nGreat user experience predicts the needs, experiences, behaviors, language and cognitive abilities, and other factors of each user group. It then uses these insights to produce a product or solution that is ultimately useful and usable. For robots, user experience begins with an understanding of the robot's intended task and environment, while considering any possible social impact the robot may have on human operations and interactions with it.[164]\nIt defines that communication as the transmission of information through signals, which are elements perceived through touch, sound, smell and sight.[165]The author states that the signal connects the sender to the receiver and consists of three parts: the signal itself, what it refers to, and the interpreter. Body postures and gestures, facial expressions, hand and head movements are all part of nonverbal behavior and communication. Robots are no exception when it comes to human-robot interaction. Therefore, humans use their verbal and nonverbal behaviors to communicate their defining characteristics. Similarly, social robots need this coordination to perform human-like behaviors.\nRobotics is an interdisciplinary field, combining primarilymechanical engineeringandcomputer sciencebut also drawing onelectronic engineeringand other subjects. The usual way to build a career in robotics is to complete an undergraduate degree in one of these established subjects, followed by a graduate (masters') degree in Robotics. Graduate degrees are typically joined by students coming from all of the contributing disciplines, and include familiarization of relevant undergraduate level subject matter from each of them, followed by specialist study in pure robotics topics which build upon them. As an interdisciplinary subject, robotics graduate programmes tend to be especially reliant on students working and learning together and sharing their knowledge and skills from their home discipline first degrees.\nRobotics industry careers then follow the same pattern, with most roboticists working as part of interdisciplinary teams of specialists from these home disciplines followed by the robotics graduate degrees which enable them to work together. Workers typically continue to identify as members of their home disciplines who work in robotics, rather than as 'roboticists'. This structure is reinforced by the nature of some engineering professions, which grant chartered engineer status to members of home disciplines rather than to robotics as a whole.\nRobotics careers are widely predicted to grow in the 21st century, as robots replace more manual and intellectual human work. Some workers who lose their jobs to robotics may be well-placed to retrain to build and maintain these robots, using their domain-specific knowledge and skills."
    },
    {
        "url": "https://en.wikipedia.org/wiki/AI_safety",
        "title": "AI safety - Wikipedia",
        "content": "AI safetyis an interdisciplinary field focused on preventing accidents, misuse, or other harmful consequences arising fromartificial intelligence(AI) systems. It encompassesAI alignment(which aims to ensure AI systems behave as intended), monitoring AI systems for risks, and enhancing their robustness. The field is particularly concerned withexistential risksposed by advanced AI models.[1][2]\nBeyond technical research, AI safety involves developing norms and policies that promote safety. It gained significant popularity in 2023, with rapid progress ingenerative AIand public concerns voiced by researchers and CEOs about potential dangers. During the 2023AI Safety Summit, the United States and the United Kingdom both established their ownAI Safety Institute. However, researchers have expressed concern that AI safety measures are not keeping pace with the rapid development of AI capabilities.[3]\nScholars discuss current risks fromcritical systemsfailures,[4]bias,[5]and AI-enabled surveillance,[6]as well as emerging risks liketechnological unemployment, digital manipulation,[7]weaponization,[8]AI-enabledcyberattacks[9]andbioterrorism.[10]They also discuss speculative risks from losing control of futureartificial general intelligence(AGI) agents,[11]or from AI enabling perpetually stable dictatorships.[12]\nSome have criticized concerns about AGI, such asAndrew Ngwho compared them in 2015 to \"worrying about overpopulation on Mars when we have not even set foot on the planet yet\".[13]Stuart J. Russellon the other side urges caution, arguing that \"it is better to anticipate human ingenuity than to underestimate it\".[14]\nAI researchers have widely differing opinions about the severity and primary sources of risk posed by AI technology[15][16][17]– though surveys suggest that experts take high consequence risks seriously. In two surveys of AI researchers, the median respondent was optimistic about AI overall, but placed a 5% probability on an \"extremely bad (e.g.human extinction)\" outcome of advanced AI.[15]In a 2022 survey of thenatural language processingcommunity, 37% agreed or weakly agreed that it is plausible that AI decisions could lead to a catastrophe that is \"at least as bad as an all-out nuclear war\".[18]\nRisks from AI began to be seriously discussed at the start of thecomputer age:\nMoreover, if we move in the direction of making machines which learn and whose behavior is modified by experience, we must face the fact that every degree of independence we give the machine is a degree of possible defiance of our wishes.\n—Norbert Wiener(1949)[19]\nIn 1988Blay Whitbypublished a book outlining the need for AI to be developed along ethical and socially responsible lines.[20]\nFrom 2008 to 2009, the Association for the Advancement of Artificial Intelligence (AAAI) commissioned a study to explore and address potential long-term societal influences of AI research and development. The panel was generally skeptical of the radical views expressed by science-fiction authors but agreed that \"additional research would be valuable on methods for understanding and verifying the range of behaviors of complex computational systems to minimize unexpected outcomes\".[21]\nIn 2011,Roman Yampolskiyintroduced the term \"AI safety engineering\"[22]at the Philosophy and Theory of Artificial Intelligence conference,[23]listing prior failures of AI systems and arguing that \"the frequency and seriousness of such events will steadily increase as AIs become more capable\".[24]\nIn 2014, philosopherNick Bostrompublished the bookSuperintelligence: Paths, Dangers, Strategies. He has the opinion that the rise of AGI has the potential to create various societal issues, ranging from the displacement of the workforce by AI, manipulation of political and military structures, to even the possibility of human extinction.[25]His argument that future advanced systems may pose a threat to human existence promptedElon Musk,[26]Bill Gates,[27]andStephen Hawking[28]to voice similar concerns.\nIn 2015, dozens of artificial intelligence experts signed anopen letter on artificial intelligencecalling for research on the societal impacts of AI and outlining concrete directions.[29]To date, the letter has been signed by over 8000 people includingYann LeCun,Shane Legg,Yoshua Bengio, andStuart Russell.\nIn the same year, a group of academics led by professor Stuart J. Russell founded theCenter for Human-Compatible AIat the University of California Berkeley and theFuture of Life Instituteawarded $6.5 million in grants for research aimed at \"ensuring artificial intelligence (AI) remains safe, ethical and beneficial\".[30]\nIn 2016, the White House Office of Science and Technology Policy and Carnegie Mellon University announced The Public Workshop on Safety and Control for Artificial Intelligence,[31]which was one of a sequence of four White House workshops aimed at investigating \"the advantages and drawbacks\" of AI.[32]In the same year, Concrete Problems in AI Safety – one of the first and most influential technical AI Safety agendas – was published.[33]\nIn 2017, the Future of Life Institute sponsored theAsilomar Conference on Beneficial AI, where more than 100 thought leaders formulated principles for beneficial AI including \"Race Avoidance: Teams developing AI systems should actively cooperate to avoid corner-cutting on safety standards\".[34]\nIn 2018, the DeepMind Safety team outlined AI safety problems in specification, robustness,[35]and assurance.[36]The following year, researchers organized a workshop at ICLR that focused on these problem areas.[37]\nIn 2021, Unsolved Problems in ML Safety was published, outlining research directions in robustness, monitoring, alignment, and systemic safety.[2]\nIn 2023,Rishi Sunaksaid he wants the United Kingdom to be the \"geographical home of global AI safety regulation\" and to host the first global summit on AI safety.[38]TheAI safety summittook place in November 2023, and focused on the risks of misuse and loss of control associated with frontier AI models.[39]During the summit the intention to create the International Scientific Report on the Safety of Advanced AI[40]was announced.\nIn 2024, The US and UK forged a new partnership on the science of AI safety. The MoU was signed on 1 April 2024 by US commerce secretaryGina Raimondoand UK technology secretaryMichelle Donelanto jointly develop advanced AI model testing, following commitments announced at an AI Safety Summit in Bletchley Park in November.[41]\nIn 2025, an international team of 96 experts chaired by Yoshua Bengio published the first International AI Safety Report. The report, commissioned by 30 nations and the United Nations, represents the first global scientific review of potential risks associated with advanced artificial intelligence. It details potential threats stemming from misuse, malfunction, and societal disruption, with the objective of informing policy through evidence-based findings, without providing specific recommendations.[42][43]\nAI safety research areas include robustness, monitoring, and alignment.[2][36]\nAI systems are often vulnerable toadversarial examplesor \"inputs to machine learning (ML) models that an attacker has intentionally designed to cause the model to make a mistake\".[44]For example, in 2013, Szegedy et al. discovered that adding specific imperceptible perturbations to an image could cause it to be misclassified with high confidence.[45]This continues to be an issue with neural networks, though in recent work the perturbations are generally large enough to be perceptible.[46][47][48]\nThe image on the right is predicted to be an ostrich after the perturbation is applied. (Left) is a correctly predicted sample, (center) perturbation applied magnified by 10x, (right) adversarial example.[45]\nAdversarial robustness is often associated with security.[49]Researchers demonstrated that an audio signal could be imperceptibly modified so that speech-to-text systems transcribe it to any message the attacker chooses.[50]Network intrusion[51]and malware[52]detection systems also must be adversarially robust since attackers may design their attacks to fool detectors.\nModels that represent objectives (reward models) must also be adversarially robust. For example, a reward model might estimate how helpful a text response is and a language model might be trained to maximize this score.[53]Researchers have shown that if a language model is trained for long enough, it will leverage the vulnerabilities of the reward model to achieve a better score and perform worse on the intended task.[54]This issue can be addressed by improving the adversarial robustness of the reward model.[55]More generally, any AI system used to evaluate another AI system must be adversarially robust. This could include monitoring tools, since they could also potentially be tampered with to produce a higher reward.[56]\nIt is often important for human operators to gauge how much they should trust an AI system, especially in high-stakes settings such as medical diagnosis.[57]ML models generally express confidence by outputting probabilities; however, they are often overconfident,[58]especially in situations that differ from those that they were trained to handle.[59]Calibration research aims to make model probabilities correspond as closely as possible to the true proportion that the model is correct.\nSimilarly, anomaly detection or out-of-distribution (OOD) detection aims to identify when an AI system is in an unusual situation. For example, if a sensor on an autonomous vehicle is malfunctioning, or it encounters challenging terrain, it should alert the driver to take control or pull over.[60]Anomaly detection has been implemented by simply training a classifier to distinguish anomalous and non-anomalous inputs,[61]though a range of additional techniques are in use.[62][63]\nScholars[8]and government agencies have expressed concerns that AI systems could be used to help malicious actors to build weapons,[64]manipulate public opinion,[65][66]or automate cyber attacks.[67]These worries are a practical concern for companies like OpenAI which host powerful AI tools online.[68]In order to prevent misuse, OpenAI has built detection systems that flag or restrict users based on their activity.[69]\nNeural networks have often been described asblack boxes,[70]meaning that it is difficult to understand why they make the decisions they do as a result of the massive number of computations they perform.[71]This makes it challenging to anticipate failures. In 2018, a self-driving car killed a pedestrian after failing to identify them. Due to the black box nature of the AI software, the reason for the failure remains unclear.[72]It also raises debates inhealthcareover whether statistically efficient but opaque models should be used.[73]\nOne critical benefit of transparency isexplainability.[74]It is sometimes a legal requirement to provide an explanation for why a decision was made in order to ensure fairness, for example for automatically filtering job applications orcredit scoreassignment.[74]\nAnother benefit is to reveal the cause of failures.[70]At the beginning of the 2020 COVID-19 pandemic, researchers used transparency tools to show that medical image classifiers were 'paying attention' to irrelevant hospital labels.[75]\nTransparency techniques can also be used to correct errors. For example, in the paper \"Locating and Editing Factual Associations in GPT\", the authors were able to identify model parameters that influenced how it answered questions about the location of the Eiffel tower. They were then able to 'edit' this knowledge to make the model respond to questions as if it believed the tower was in Rome instead of France.[76]Though in this case, the authors induced an error, these methods could potentially be used to efficiently fix them. Model editing techniques also exist in computer vision.[77]\nFinally, some have argued that the opaqueness of AI systems is a significant source of risk and better understanding of how they function could prevent high-consequence failures in the future.[78]\"Inner\" interpretability research aims to make ML models less opaque. One goal of this research is to identify what the internal neuron activations represent.[79][80]For example, researchers identified a neuron in the CLIP artificial intelligence system that responds to images of people inSpider-Mancostumes, sketches of Spider-Man, and the word 'spider'.[81]It also involves explaining connections between these neurons or 'circuits'.[82][83]For example, researchers have identified pattern-matching mechanisms in transformer attention that may play a role in how language models learn from their context.[84]\"Inner interpretability\" has been compared to neuroscience. In both cases, the goal is to understand what is going on in an intricate system, though ML researchers have the benefit of being able to take perfect measurements and perform arbitrary ablations.[85]\nMachine learning models can potentially contain \"trojans\" or \"backdoors\": vulnerabilities that malicious actors maliciously build into an AI system. For example, a trojaned facial recognition system could grant access when a specific piece of jewelry is in view;[2]or a trojaned autonomous vehicle may function normally until a specific trigger is visible.[86]Note that an adversary must have access to the system's training data in order to plant a trojan.[citation needed]This might not be difficult to do with some large models like CLIP or GPT-3 as they are trained on publicly available internet data.[87]Researchers were able to plant a trojan in an image classifier by changing just 300 out of 3 million of the training images.[88]In addition to posing a security risk, researchers have argued that trojans provide a concrete setting for testing and developing better monitoring tools.[56]\nA 2024 research paper byAnthropicshowed thatlarge language modelscould be trained with persistent backdoors. These \"sleeper agent\" models could be programmed to generate malicious outputs (such as vulnerable code) after a specific date, while behaving normally beforehand. Standard AI safety measures, such assupervisedfine-tuning,reinforcement learningand adversarial training, failed to remove these backdoors.[89]\nIn the field ofartificial intelligence(AI),alignmentaims to steer AI systems toward a person's or group's intended goals, preferences, or ethical principles. An AI system is consideredalignedif it advances the intended objectives. AmisalignedAI system pursues unintended objectives.[90]\nIt is often challenging for AI designers to align an AI system because it is difficult for them to specify the full range of desired and undesired behaviors. Therefore, AI designers often use simplerproxy goals, such asgaining human approval. But proxy goals can overlook necessary constraints or reward the AI system for merelyappearingaligned.[90][91]AI systems may also find loopholes that allow them to accomplish their proxy goals efficiently but in unintended, sometimes harmful, ways (reward hacking).[90][92]\nAdvanced AI systems may develop unwantedinstrumental strategies, such as seeking power or survival because such strategies help them achieve their assigned final goals.[90][93][94]Furthermore, they might develop undesirable emergent goals that could be hard to detect before the system is deployed and encounters new situations anddata distributions.[95][96]Empirical research showed in 2024 that advancedlarge language models(LLMs) such asOpenAI o1orClaude 3sometimes engage in strategic deception to achieve their goals or prevent them from being changed.[97][98]\nToday, some of these issues affect existing commercial systems such as LLMs,[99][100][101]robots,[102]autonomous vehicles,[103]and social mediarecommendation engines.[99][94][104]Some AI researchers argue that more capable future systems will be more severely affected because these problems partially result from high capabilities.[105][92][91]\nMany prominent AI researchers and the leadership of major AI companies have argued or asserted that AI is approaching human-like (AGI) andsuperhuman cognitive capabilities(ASI), and couldendanger human civilizationif misaligned.[106][94]These include \"AI godfathers\"Geoffrey HintonandYoshua Bengioand the CEOs ofOpenAI,Anthropic, andGoogle DeepMind.[107][108][109]These risks remain debated.[110]\nIt is common for AI risks (and technological risks more generally) to be categorized asmisuse or accidents.[126]Some scholars have suggested that this framework falls short.[126]For example, theCuban Missile Crisiswas not clearly an accident or a misuse of technology.[126]Policy analysts Zwetsloot and Dafoe wrote, \"The misuse and accident perspectives tend to focus only on the last step in a causal chain leading up to a harm: that is, the person who misused the technology, or the system that behaved in unintended ways... Often, though, the relevant causal chain is much longer.\" Risks often arise from 'structural' or 'systemic' factors such as competitive pressures, diffusion of harms, fast-paced development, high levels of uncertainty, and inadequate safety culture.[126]In the broader context ofsafety engineering, structural factors like 'organizational safety culture' play a central role in the popular STAMP risk analysis framework.[127]\nInspired by the structural perspective, some researchers have emphasized the importance of using machine learning to improve sociotechnical safety factors, for example, using ML for cyber defense, improving institutional decision-making, and facilitating cooperation.[2]Others have emphasized the importance of involving both AI practitioners and domain experts in the design process to address structural vulnerabilities.[128]\nSome scholars are concerned that AI will exacerbate the already imbalanced game between cyber attackers and cyber defenders.[129]This would increase 'first strike' incentives and could lead to more aggressive and destabilizing attacks. In order to mitigate this risk, some have advocated for an increased emphasis on cyber defense. In addition, software security is essential for preventing powerful AI models from being stolen and misused.[8]Recent studies have shown that AI can significantly enhance both technical and managerial cybersecurity tasks by automating routine tasks and improving overall efficiency.[130]\nThe advancement of AI in economic and military domains could precipitate unprecedented political challenges.[131]Some scholars have compared AI race dynamics to the cold war, where the careful judgment of a small number of decision-makers often spelled the difference between stability and catastrophe.[132]AI researchers have argued that AI technologies could also be used to assist decision-making.[2]For example, researchers are beginning to develop AI forecasting[133]and advisory systems.[134]\nMany of the largest global threats (nuclear war,[135]climate change,[136]etc.) have beenframedas cooperation challenges. As in the well-knownprisoner's dilemmascenario, some dynamics may lead to poor results for all players, even when they are optimally acting in their self-interest. For example, no single actor has strong incentives to address climate change even though the consequences may be significant if no one intervenes.[136]\nA salient AI cooperation challenge is avoiding a 'race to the bottom'.[137]In this scenario, countries or companies race to build more capable AI systems and neglect safety, leading to a catastrophic accident that harms everyone involved. Concerns about scenarios like these have inspired both political[138]and technical[139]efforts to facilitate cooperation between humans, and potentially also between AI systems. Most AI research focuses on designing individual agents to serve isolated functions (often in 'single-player' games).[140]Scholars have suggested that as AI systems become more autonomous, it may become essential to study and shape the way they interact.[140][128]\nIn recent years, the development of large language models (LLMs) has raised unique concerns within the field of AI safety. Researchers Bender and Gebru et al.[141]have highlighted the environmental and financial costs associated with training these models, emphasizing that the energy consumption and carbon footprint of training procedures like those for Transformer models can be substantial. Moreover, these models often rely on massive, uncurated Internet-based datasets, which can encode hegemonic and biased viewpoints, further marginalizing underrepresented groups. The large-scale training data, while vast, does not guarantee diversity and often reflects the worldviews of privileged demographics, leading to models that perpetuate existing biases and stereotypes. This situation is exacerbated by the tendency of these models to produce seemingly coherent and fluent text, which can mislead users into attributing meaning and intent where none exists, a phenomenon described as 'stochastic parrots'. These models, therefore, pose risks of amplifying societal biases, spreading misinformation, and being used for malicious purposes, such as generating extremist propaganda or deepfakes. To address these challenges, researchers advocate for more careful planning in dataset creation and system development, emphasizing the need for research projects that contribute positively towards an equitable technological ecosystem.[142][143]\nThe unique challenges posed by LLMs also extend to security vulnerabilities.  These include various manipulation techniques, such asprompt injection,Misinformation Generationand model stealing,[144]which can be exploited to compromise their intended function. This can allow attackers to bypass safety measures and elicit unintended responses\nAI governance is broadly concerned with creating norms, standards, and regulations to guide the use and development of AI systems.[132]\nAI safety governance research ranges from foundational investigations into the potential impacts of AI to specific applications. On the foundational side, researchers have argued that AI could transform many aspects of society due to its broad applicability, comparing it to electricity and the steam engine.[146]Some work has focused on anticipating specific risks that may arise from these impacts – for example, risks from mass unemployment,[147]weaponization,[148]disinformation,[149]surveillance,[150]and the concentration of power.[151]Other work explores underlying risk factors such as the difficulty of monitoring the rapidly evolving AI industry,[152]the availability of AI models,[153]and 'race to the bottom' dynamics.[137][154]Allan Dafoe, the head of longterm governance and strategy at DeepMind has emphasized the dangers of racing and the potential need for cooperation: \"it may be close to a necessary and sufficient condition for AI safety and alignment that there be a high degree of caution prior to deploying advanced powerful systems; however, if actors are competing in a domain with large returns to first-movers or relative advantage, then they will be pressured to choose a sub-optimal level of caution\".[138]A research stream focuses on developing approaches, frameworks, and methods to assess AI accountability, guiding and promoting audits of AI-based systems.[155][156][157]A key challenge for these approaches is a lack of widely accepted standards, and ambiguity about what the methods would require.[158][159]\nEfforts to enhance AI safety include frameworks designed to align AI outputs withethical guidelinesand reduce risks like misuse and data leakage. Tools such asNvidia's  Guardrails,[160]LlamaGuard,[161]Preamble's customizable guardrails[162]and Claude's Constitution mitigate vulnerabilities likeprompt injectionand ensure outputs adhere to predefined principles. These frameworks are often integrated into AI systems to improve safety and reliability.[163]\nThe field of AI safety is deeply intertwined with philosophical considerations, particularly in the realm of ethics.Deontologicalethics, which emphasizes adherence to moral rules, has been proposed as a framework for aligning AI systems with human values. By embedding deontological principles, AI systems can be guided to avoid actions that cause harm, ensuring their operations remain within ethical boundaries.[164]\nIn addressing the AI safety problem it is important to stress the distinction between local and global solutions. Local solutions focus on individual AI systems, ensuring they are safe and beneficial, while global solutions seek to implement safety measures for all AI systems across various jurisdictions. Some researchers[165]argue for the necessity of scaling local safety measures to a global level, proposing a classification for these global solutions. This approach underscores the importance of collaborative efforts in the international governance of AI safety, emphasizing that no single entity can effectively manage the risks associated with AI technologies. This perspective aligns with ongoing efforts in international policy-making and regulatory frameworks, which aim to address the complex challenges posed by advanced AI systems worldwide.[166][167]\nSome experts have argued that it is too early to regulate AI, expressing concerns that regulations will hamper innovation and it would be foolish to \"rush to regulate in ignorance\".[168][169]Others, such as business magnateElon Musk, call for pre-emptive action to mitigate catastrophic risks.[170]\nOutside of formal legislation, government agencies have put forward ethical and safety recommendations. In March 2021, the US National Security Commission on Artificial Intelligence reported that advances in AI may make it increasingly important to \"assure that systems are aligned with goals and values, including safety, robustness and trustworthiness\".[171]Subsequently, the National Institute of Standards and Technology drafted a framework for managing AI Risk, which advises that when \"catastrophic risks are present – development and deployment should cease in a safe manner until risks can be sufficiently managed\".[172]\nIn September 2021, thePeople's Republic of China(PRC) published ethical guidelines for the use of AI in China, emphasizing that AI decisions should remain under human control and calling for accountability mechanisms. In the same month, TheUnited Kingdompublished its 10-year National AI Strategy,[173]which states the British government \"takes the long-term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously\".[174]The strategy describes actions to assess long-term AI risks, including catastrophic risks.[174]The British government held first major global summit on AI safety. This took place on the 1st and 2 November 2023 and was described as \"an opportunity for policymakers and world leaders to consider the immediate and future risks of AI and how these risks can be mitigated via a globally coordinated approach\".[175][176]China Media Project stated \"key aspects of its approach remain fundamentally unsafe by the standards of democratic societies worldwide\", arguing that part of China's AI safety approach is focused on strengthening theCCP's information control.[177]\nGovernment organizations, particularly in the United States, have also encouraged the development of technical AI safety research. TheIntelligence Advanced Research Projects Activityinitiated the TrojAI project to identify and protect againstTrojan attackson AI systems.[178]TheDARPAengages in research onexplainable artificial intelligenceand improving robustness againstadversarial attacks.[179][180]And theNational Science Foundationsupports the Center for Trustworthy Machine Learning, and is providing millions of dollars in funding for empirical AI safety research.[181]\nIn 2024, theUnited Nations General Assemblyadopted the first global resolution on the promotion of \"safe, secure and trustworthy\" AI systems that emphasized the respect, protection and promotion of human rights in the design, development, deployment and the use of AI.[182]\nIn May 2024, theDepartment for Science, Innovation and Technology(DSIT) announced £8.5 million in funding for AI safety research under the Systemic AI Safety Fast Grants Programme, led by Christopher Summerfield and Shahar Avin at the AI Safety Institute, in partnership withUK Research and Innovation. Technology SecretaryMichelle Donelanannounced the plan at theAI Seoul Summit, stating the goal was to make AI safe across society and that promising proposals could receive further funding. The UK also signed an agreement with 10 other countries and the EU to form an international network of AI safety institutes to promote collaboration and share information and resources. Additionally, the UK AI Safety Institute planned to open an office in San Francisco.[183]\nAI labs and companies generally abide by safety practices and norms that fall outside of formal legislation.[184]One aim of governance researchers is to shape these norms. Examples of safety recommendations found in the literature include performing third-party auditing,[185]offering bounties for finding failures,[185]sharing AI incidents[185](an AI incident database was created for this purpose),[186]following guidelines to determine whether to publish research or models,[153]and improving information and cyber security in AI labs.[187]\nCompanies have also made commitments.Cohere,OpenAI, andAI21proposed and agreed on \"best practices for deploying language models\", focusing on mitigating misuse.[188]To avoid contributing to racing-dynamics, OpenAI has also stated in their charter that \"if a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project\"[189]Also, industry leaders such as CEO of DeepMind Demis Hassabis, director of Facebook AI Yann LeCun have signed open letters such as the Asilomar Principles[34]and the Autonomous Weapons Open Letter.[190]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning - Wikipedia",
        "content": "Machine learning(ML) is afield of studyinartificial intelligenceconcerned with the development and study ofstatistical algorithmsthat can learn fromdataandgeneraliseto unseen data, and thus performtaskswithout explicitinstructions.[1]Within a subdiscipline in machine learning, advances in the field ofdeep learninghave allowedneural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.[2]\nML finds application in many fields, includingnatural language processing,computer vision,speech recognition,email filtering,agriculture, andmedicine. The application of ML to business problems is known aspredictive analytics.\nStatisticsandmathematical optimisation(mathematical programming) methods comprise the foundations of machine learning.Data miningis a related field of study, focusing onexploratory data analysis(EDA) viaunsupervised learning.[4][5]\nFrom a theoretical viewpoint,probably approximately correct learningprovides a framework for describing machine learning.\nThe termmachine learningwas coined in 1959 byArthur Samuel, anIBMemployee and pioneer in the field ofcomputer gamingandartificial intelligence.[6][7]The synonymself-teaching computerswas also used in this time period.[8][9]\nThe earliest machine learning program was introduced in the 1950s whenArthur Samuelinvented acomputer programthat calculated the winning chance in checkers for each side, but the history of machine learning roots back to decades of human desire and effort to study human cognitive processes.[10]In 1949,CanadianpsychologistDonald Hebbpublished the bookThe Organization of Behavior, in which he introduced atheoretical neural structureformed by certain interactions amongnerve cells.[11]Hebb's modelofneuronsinteracting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, orartificial neuronsused by computers to communicate data.[10]Other researchers who have studied humancognitive systemscontributed to the modern machine learning technologies as well, including logicianWalter PittsandWarren McCulloch, who proposed the early mathematical models of neural networks to come up withalgorithmsthat mirror human thought processes.[10]\nBy the early 1960s, an experimental \"learning machine\" withpunched tapememory, called Cybertron, had been developed byRaytheon Companyto analysesonarsignals,electrocardiograms, and speech patterns using rudimentaryreinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognise patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions.[12]A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.[13]Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973.[14]In 1981, a report was given on using teaching strategies so that anartificial neural networklearns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.[15]\nTom M. Mitchellprovided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experienceEwith respect to some class of tasksTand performance measurePif its performance at tasks inT, as measured byP,  improves with experienceE.\"[16]This definition of the tasks in which machine learning is concerned offers a fundamentallyoperational definitionrather than defining the field in cognitive terms. This followsAlan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".[17]\nModern day Machine Learning algorithms are broken into 3 algorithms types: Supervised Learning Algorithms, Unsupervised Learning Algorithms, and Reinforcement Learning Algorithms.[18]\nAs a scientific endeavour, machine learning grew out of the quest forartificial intelligence(AI). In the early days of AI as anacademic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostlyperceptronsandother modelsthat were later found to be reinventions of thegeneralised linear modelsof statistics.[20]Probabilistic reasoningwas also employed, especially inautomated medical diagnosis.[21]: 488\nHowever, an increasing emphasis on thelogical, knowledge-based approachcaused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[21]: 488By 1980,expert systemshad come to dominate AI, and statistics was out of favour.[22]Work on symbolic/knowledge-based learning did continue within AI, leading toinductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, inpattern recognitionandinformation retrieval.[21]: 708–710, 755Neural networks research had been abandoned by AI andcomputer sciencearound the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines includingJohn Hopfield,David Rumelhart, andGeoffrey Hinton. Their main success came in the mid-1980s with the reinvention ofbackpropagation.[21]: 25\nMachine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from thesymbolic approachesit had inherited from AI, and toward methods and models borrowed from statistics,fuzzy logic, andprobability theory.[22]\nThere is a close connection between machine learning and compression. A system that predicts theposterior probabilitiesof a sequence given its entire history can be used for optimal data compression (by usingarithmetic codingon the output distribution). Conversely, an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for \"general intelligence\".[23][24][25]\nAn alternative view can show compression algorithms implicitly map strings into implicitfeature space vectors, and compression-based similarity measures compute similarity within these feature spaces. For each compressor C(.) we define an associated vector space ℵ, such that C(.) maps an input string x, corresponding to the vector norm ||~x||. An exhaustive examination of the feature spaces underlying all compression algorithms is precluded by space; instead, feature vectors chooses to examine three representative lossless compression methods, LZW, LZ77, and PPM.[26]\nAccording toAIXItheory, a connection more directly explained inHutter Prize, the best possible compression of x is the smallest possible software that generates x. For example, in that model, a zip file's compressed size includes both the zip file and the unzipping software, since you can not unzip it without both, but there may be an even smaller combined form.\nExamples of AI-powered audio/video compression software includeNVIDIA Maxine, AIVC.[27]Examples of software that can perform AI-powered image compression includeOpenCV,TensorFlow,MATLAB's Image Processing Toolbox (IPT) and High-Fidelity Generative Image Compression.[28]\nInunsupervised machine learning,k-means clusteringcan be utilized to compress data by grouping similar data points into clusters. This technique simplifies handling extensive datasets that lack predefined labels and finds widespread use in fields such asimage compression.[29]\nData compression aims to reduce the size of data files, enhancing storage efficiency and speeding up data transmission. K-means clustering, an unsupervised machine learning algorithm, is employed to partition a dataset into a specified number of clusters, k, each represented by thecentroidof its points. This process condenses extensive datasets into a more compact set of representative points. Particularly beneficial inimageandsignal processing, k-means clustering aids in data reduction by replacing groups of data points with their centroids, thereby preserving the core information of the original data while significantly decreasing the required storage space.[30]\nMachine learning anddata miningoften employ the same methods and overlap significantly, but while machine learning focuses on prediction, based onknownproperties learned from the training data, data mining focuses on thediscoveryof (previously)unknownproperties in the data (this is the analysis step ofknowledge discoveryin databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals,ECML PKDDbeing a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability toreproduce knownknowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previouslyunknownknowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.\nMachine learning also has intimate ties tooptimisation: Many learning problems are formulated as minimisation of someloss functionon a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign alabelto instances, and models are trained to correctly predict the preassigned labels of a set of examples).[33]\nCharacterizing the generalisation of various learning algorithms is an active topic of current research, especially fordeep learningalgorithms.\nMachine learning andstatisticsare closely related fields in terms of methods, but distinct in their principal goal: statistics draws populationinferencesfrom asample, while machine learning finds generalisable predictive patterns.[34]\nConventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.[35]\nLeo Breimandistinguished two statistical modelling paradigms: data model and algorithmic model,[36]wherein \"algorithmic model\" means more or less the machine learning algorithms likeRandom Forest.\nSome statisticians have adopted methods from machine learning, leading to a combined field that they callstatistical learning.[37]\nAnalytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyse the weight space ofdeep neural networks.[38]Statistical physics is thus finding applications in the area ofmedical diagnostics.[39]\nA core objective of a learner is to generalise from its experience.[3][40]Generalisation in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.\nThe computational analysis of machine learning algorithms and their performance is a branch oftheoretical computer scienceknown ascomputational learning theoryvia theprobably approximately correct learningmodel. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. Thebias–variance decompositionis one way to quantify generalisationerror.\nFor the best performance in the context of generalisation, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject tooverfittingand generalisation will be poorer.[41]\nIn addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done inpolynomial time. There are two kinds oftime complexityresults: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.\nMachine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the \"signal\" or \"feedback\" available to the learning system:\nAlthough each algorithm has advantages and limitations, no single algorithm works for all problems.[42][43][44]\nSupervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.[45]The data, known astraining data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by anarrayor vector, sometimes called afeature vector, and the training data is represented by amatrix. Throughiterative optimisationof anobjective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs.[46]An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.[16]\nTypes of supervised-learning algorithms includeactive learning,classificationandregression.[47]Classification algorithms are used when the outputs are restricted to a limited set of values, while regression algorithms are used when the outputs can take any numerical value within a range. For example, in a classification algorithm that filters emails, the input is an incoming email, and the output is the folder in which to file the email. In contrast, regression is used for tasks such as predicting a person's height based on factors like age and genetics or forecasting future temperatures based on historical data.[48]\nSimilarity learningis an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications inranking,recommendation systems, visual identity tracking, face verification, and speaker verification.\nUnsupervised learning algorithms find structures in data that has not been labelled, classified or categorised. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering,dimensionality reduction,[5]anddensity estimation.[49]\nCluster analysis is the assignment of a set of observations into subsets (calledclusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by somesimilarity metricand evaluated, for example, byinternal compactness, or the similarity between members of the same cluster, andseparation, the difference between clusters. Other methods are based onestimated densityandgraph connectivity.\nA special type of unsupervised learning called,self-supervised learninginvolves training a model by generating the supervisory signal from the data itself.[50][51]\nSemi-supervised learning falls betweenunsupervised learning(without any labelled training data) andsupervised learning(with completely labelled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy.\nInweakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.[52]\nReinforcement learning is an area of machine learning concerned with howsoftware agentsought to takeactionsin an environment so as to maximise some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such asgame theory,control theory,operations research,information theory,simulation-based optimisation,multi-agent systems,swarm intelligence,statisticsandgenetic algorithms. In reinforcement learning, the environment is typically represented as aMarkov decision process(MDP). Many reinforcement learning algorithms usedynamic programmingtechniques.[53]Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.\nDimensionality reductionis a process of reducing the number of random variables under consideration by obtaining a set of principal variables.[54]In other words, it is a process of reducing the dimension of thefeatureset, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination orextraction. One of the popular methods of dimensionality reduction isprincipal component analysis(PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D).\nThemanifold hypothesisproposes that high-dimensional data sets lie along low-dimensionalmanifolds, and many dimensionality reduction techniques make this assumption, leading to the area ofmanifold learningandmanifold regularisation.\nOther approaches have been developed which do not fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example,topic modelling,meta-learning.[55]\nSelf-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, namedcrossbar adaptive array(CAA).[56][57]It gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. Emotion is used as state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.[58]The self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine:\nIt is a system with only one input, situation, and only one output, action (or behaviour) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioural environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioural environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behaviour, in an environment that contains both desirable and undesirable situations.[59]\nSeveral learning algorithms aim at discovering better representations of the inputs provided during training.[60]Classic examples includeprincipal component analysisand cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manualfeature engineering, and allows a machine to both learn the features and use them to perform a specific task.\nFeature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labelled input data. Examples includeartificial neural networks,multilayer perceptrons, and superviseddictionary learning. In unsupervised feature learning, features are learned with unlabelled input data.  Examples include dictionary learning,independent component analysis,autoencoders,matrix factorisation[61]and various forms ofclustering.[62][63][64]\nManifold learningalgorithms attempt to do so under the constraint that the learned representation is low-dimensional.Sparse codingalgorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros.Multilinear subspace learningalgorithms aim to learn low-dimensional representations directly fromtensorrepresentations for multidimensional data, without reshaping them into higher-dimensional vectors.[65]Deep learningalgorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[66]\nFeature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\nSparse dictionary learning is a feature learning method where a training example is represented as a linear combination ofbasis functionsand assumed to be asparse matrix. The method isstrongly NP-hardand difficult to solve approximately.[67]A popularheuristicmethod for sparse dictionary learning is thek-SVDalgorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied inimage de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[68]\nIndata mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.[69]Typically, the anomalous items represent an issue such asbank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to asoutliers, novelties, noise, deviations and exceptions.[70]\nIn particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.[71]\nThree broad categories of anomaly detection techniques exist.[72]Unsupervised anomaly detection techniques detect anomalies in an unlabelled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labelled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behaviour from a given normal training data set and then test the likelihood of a test instance to be generated by the model.\nRobot learningis inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning,[73][74]and finallymeta-learning(e.g. MAML).\nAssociation rule learning is arule-based machine learningmethod for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\".[75]\nRule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilisation of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[76]Rule-based machine learning approaches includelearning classifier systems, association rule learning, andartificial immune systems.\nBased on the concept of strong rules,Rakesh Agrawal,Tomasz Imielińskiand Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded bypoint-of-sale(POS) systems in supermarkets.[77]For example, the rule{onions,potatoes}⇒{burger}{\\displaystyle \\{\\mathrm {onions,potatoes} \\}\\Rightarrow \\{\\mathrm {burger} \\}}found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotionalpricingorproduct placements. In addition tomarket basket analysis, association rules are employed today in application areas includingWeb usage mining,intrusion detection,continuous production, andbioinformatics. In contrast withsequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.\nLearning classifier systems(LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically agenetic algorithm, with a learning component, performing eithersupervised learning,reinforcement learning, orunsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in apiecewisemanner in order to make predictions.[78]\nInductive logic programming(ILP) is an approach to rule learning usinglogic programmingas a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program thatentailsall positive and no negative examples.Inductive programmingis a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such asfunctional programs.\nInductive logic programming is particularly useful inbioinformaticsandnatural language processing.Gordon PlotkinandEhud Shapirolaid the initial theoretical foundation for inductive machine learning in a logical setting.[79][80][81]Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples.[82]The terminductivehere refers tophilosophicalinduction, suggesting a theory to explain observed facts, rather thanmathematical induction, proving a property for all members of a well-ordered set.\nAmachine learning modelis a type ofmathematical modelthat, once \"trained\" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimise errors in its predictions.[83]By extension, the term \"model\" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned.[84]\nVarious types of models have been used and researched for machine learning systems, picking the best model for a task is calledmodel selection.\nArtificial neural networks (ANNs), orconnectionistsystems, are computing systems vaguely inspired by thebiological neural networksthat constitute animalbrains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules.\nAn ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model theneuronsin a biological brain. Each connection, like thesynapsesin a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is areal number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have aweightthat adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.\nThe original goal of the ANN approach was to solve problems in the same way that ahuman brainwould. However, over time, attention moved to performing specific tasks, leading to deviations frombiology. Artificial neural networks have been used on a variety of tasks, includingcomputer vision,speech recognition,machine translation,social networkfiltering,playing board and video gamesandmedical diagnosis.\nDeep learningconsists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[85]\nDecision tree learning uses adecision treeas apredictive modelto go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modelling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures,leavesrepresent class labels, and branches representconjunctionsof features that lead to those class labels. Decision trees where the target variable can take continuous values (typicallyreal numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions anddecision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making.\nRandom forest regression (RFR) falls under umbrella of decisiontree-based models. RFR is an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and to avoid overfitting. To build decision trees, RFR uses bootstrapped sampling, for instance each decision tree is trained on random data from the training set. This random selection of RFR for training enables model to reduce bias predictions and achieve a higher degree of accuracy. RFR generates independent decision trees, and it can work on single output data as well multiple regressor tasks. This makes RFR compatible to be used in various applications.[86][87]\nSupport-vector machines (SVMs), also known as support-vector networks, are a set of relatedsupervised learningmethods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category.[88]An SVM training algorithm is a non-probabilistic,binary,linear classifier, although methods such asPlatt scalingexist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called thekernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\nRegression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form islinear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such asordinary least squares. The latter is often extended byregularisationmethods to mitigate overfitting and bias, as inridge regression. When dealing with non-linear problems, go-to models includepolynomial regression(for example, used for trendline fitting in Microsoft Excel[89]),logistic regression(often used instatistical classification) or evenkernel regression, which introduces non-linearity by taking advantage of thekernel trickto implicitly map input variables to higher-dimensional space.\nMultivariate linear regressionextends the concept of linear regression to handle multiple dependent variables simultaneously. This approach estimates the relationships between a set of input variables and several output variables by fitting amultidimensionallinear model. It is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images,[90]which are inherently multi-dimensional.\nA Bayesian network, belief network, or directed acyclic graphical model is a probabilisticgraphical modelthat represents a set ofrandom variablesand theirconditional independencewith adirected acyclic graph(DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that performinferenceand learning. Bayesian networks that model sequences of variables, likespeech signalsorprotein sequences, are calleddynamic Bayesian networks. Generalisations of Bayesian networks that can represent and solve decision problems under uncertainty are calledinfluence diagrams.\nA Gaussian process is astochastic processin which every finite collection of the random variables in the process has amultivariate normal distribution, and it relies on a pre-definedcovariance function, or kernel, that models how pairs of points relate to each other depending on their locations.\nGiven a set of observed points, or input–output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point.\nGaussian processes are popular surrogate models inBayesian optimisationused to dohyperparameter optimisation.\nA genetic algorithm (GA) is asearch algorithmandheuristictechnique that mimics the process ofnatural selection, using methods such asmutationandcrossoverto generate newgenotypesin the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s.[92][93]Conversely, machine learning techniques have been used to improve the performance of genetic andevolutionary algorithms.[94]\nThe theory of belief functions, also referred to as evidence theory or Dempster–Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such asprobability,possibilityandimprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g.,  Dempster's rule of combination), just like how in apmf-based Bayesian approach would combine probabilities.[95]However, there are many caveats to these beliefs functions when compared to Bayesian approaches in order to incorporate ignorance anduncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of variousensemble methodsto better handle the learner'sdecision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving.[96][7]However, the computational complexity of these algorithms are dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches.\nRule-based machine learning (RBML) is a branch of machine learning that automatically discovers and learns 'rules' from data. It provides interpretable models, making it useful for decision-making in fields like healthcare, fraud detection, and cybersecurity. Key RBML techniques includeslearning classifier systems,[97]association rule learning,[98]artificial immune systems,[99]and other similar models. These methods extract patterns from data and evolve rules over time.\nTypically, machine learning models require a high quantity of reliable data to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representativesampleof data. Data from the training set can be as varied as acorpus of text, a collection of images,sensordata, and data collected from individual users of a service.Overfittingis something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives.Algorithmic biasis a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably, becoming integrated within machine learning engineering teams.\nFederated learning is an adapted form ofdistributed artificial intelligenceto training machine learning models that decentralises the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralised server. This also increases efficiency by decentralising the training process to many devices. For example,Gboarduses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back toGoogle.[100]\nThere are many applications for machine learning, including:\nIn 2006, the media-services providerNetflixheld the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers fromAT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built anensemble modelto win the Grand Prize in 2009 for $1 million.[104]Shortly after the prize was awarded, Netflix realised that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly.[105]In 2010, an article inThe Wall Street Journalnoted the use of machine learning by Rebellion Research to predict the2008 financial crisis.[106]In 2012, co-founder ofSun Microsystems,Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[107]In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognised influences among artists.[108]In 2019Springer Naturepublished the first research book created using machine learning.[109]In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19.[110]Machine learning was recently applied to predict the pro-environmental behaviour of travellers.[111]Recently, machine learning technology was also applied to optimise smartphone's performance and thermal behaviour based on the user's interaction with the phone.[112][113][114]When applied correctly, machine learning algorithms (MLAs) can utilise a wide range of company characteristics to predict stock returns withoutoverfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques likeOLS.[115]\nRecent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes.[116]\nMachine Learning is becoming a useful tool to investigate and predict evacuation decision making in large scale and small scale disasters. Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes.[117][118][119]Other applications have been focusing on pre evacuation decisions in building fires.[120][121]\nAlthough machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results.[122][123][124]Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.[125]\nThe \"black box theory\" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted out of the data.[126]The House of Lords Select Committee, which claimed that such an \"intelligence system\" that could have a \"substantial impact on an individual's life\" would not be considered acceptable unless it provided \"a full and satisfactory explanation for the decisions\" it makes.[126]\nIn 2018, a self-driving car fromUberfailed to detect a pedestrian, who was killed after a collision.[127]Attempts to use machine learning in healthcare with theIBM Watsonsystem failed to deliver even after years of time and billions of dollars invested.[128][129]Microsoft'sBing Chatchatbot has been reported to produce hostile and offensive response against its users.[130]\nMachine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.[131]\nExplainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI.[132]It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision.[133]By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation.\nSettling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalising the theory in accordance with how complex the theory is.[134]\nLearners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses.[135]A real-world example is that, unlike humans, current image classifiers often do not primarily make judgements from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in \"adversarial\" images that the system misclassifies.[136][137]\nAdversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel.[138]Machine learning models are often vulnerable to manipulation or evasion viaadversarial machine learning.[139]\nResearchers have demonstrated howbackdoorscan be placed undetectably into classifying (e.g., for categories \"spam\" and well-visible \"not spam\" of posts) machine learning models that are often developed or trained by third parties. Parties can change the classification of any input, including in cases for which a type ofdata/software transparencyis provided, possibly includingwhite-box access.[140][141][142]\nClassification of machine learning models can be validated by accuracy estimation techniques like theholdoutmethod, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validationmethod randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods,bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[143]\nIn addition to overall accuracy, investigators frequently reportsensitivity and specificitymeaning true positive rate (TPR) and true negative rate (TNR) respectively. Similarly, investigators sometimes report thefalse positive rate(FPR) as well as thefalse negative rate(FNR). However, these rates are ratios that fail to reveal their numerators and denominators.Receiver operating characteristic(ROC) along with the accompanying Area Under the ROC Curve (AUC) offer additional tools for classification model assessment. Higher AUC is associated with a better performing model.[144]\nTheethicsofartificial intelligencecovers a broad range of topics within AI that are considered to have particular ethical stakes.[145]This includesalgorithmic biases,fairness,[146]automated decision-making,[147]accountability,privacy, andregulation. It also covers various emerging or potential future challenges such asmachine ethics(how to make machines that behave ethically),lethal autonomous weapon systems,arms racedynamics,AI safetyandalignment,technological unemployment, AI-enabledmisinformation,[148]how to treat certain AI systems if they have amoral status(AI welfare and rights),artificial superintelligenceandexistential risks.[145]\nDifferent machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.[149]\nSystems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitising cultural prejudices.[150]For example, in 1988, the UK'sCommission for Racial Equalityfound thatSt. George's Medical Schoolhad been using a computer program trained from data of previous admissions staff and that this program had denied nearly 60 candidates who were found to either be women or have non-European sounding names.[149]Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants.[151][152]Another example includes predictive policing companyGeolitica's predictive algorithm that resulted in \"disproportionately high levels of over-policing in low-income and minority communities\" after being trained with historical crime data.[153]\nWhile responsiblecollection of dataand documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learning's vulnerability to biases.[154]In fact, according to research carried out by the Computing Research Association (CRA) in 2021, \"female faculty merely make up 16.1%\" of all faculty members who focus on AI among several universities around the world.[155]Furthermore, among the group of \"new U.S. resident AI PhD graduates,\" 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI.[155]\nLanguage models learned from data have been shown to contain human-like biases.[156][157]Because human languages contain biases, machines trained on languagecorporawill necessarily also learn these biases.[158][159]In 2016, Microsoft testedTay, achatbotthat learned from Twitter, and it quickly picked up racist and sexist language.[160]\nIn an experiment carried out byProPublica, aninvestigative journalismorganisation, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged \"black defendants high risk twice as often as white defendants\".[153]In 2015, Google Photos once tagged a couple of black people as gorillas, which caused controversy. The gorilla label was subsequently removed, and in 2023, it still cannot recognise gorillas.[161]Similar issues with recognising non-white people have been found in many other systems.[162]\nBecause of such challenges, the effective use of machine learning may take longer to be adopted in other domains.[163]Concern forfairnessin machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, includingFei-Fei Li, who said that \"[t]here's nothing artificial about AI. It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\"[164]\nThere are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.[165]\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for trainingdeep neural networks(a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units.[166]By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI.[167]OpenAIestimated the hardware compute used in the largest deep learning projects fromAlexNet(2012) toAlphaZero(2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.[168][169]\nTensor Processing Units (TPUs)are specialised hardware accelerators developed byGooglespecifically for machine learning workloads. Unlike general-purposeGPUsandFPGAs, TPUs are optimised for tensor computations, making them particularly efficient for deep learning tasks such as training and inference. They are widely used in Google Cloud AI services and large-scale machine learning models like Google's DeepMind AlphaFold and large language models. TPUs leverage matrix multiplication units and high-bandwidth memory to accelerate computations while maintaining energy efficiency.[170]Since their introduction in 2016, TPUs have become a key component of AI infrastructure, especially in cloud-based environments.\nNeuromorphic computingrefers to a class of computing systems designed to emulate the structure and functionality of biological neural networks. These systems may be implemented through software-based simulations on conventional hardware or through specialised hardware architectures.[171]\nAphysical neural networkis a specific type of neuromorphic hardware that relies on electrically adjustable materials, such as memristors, to emulate the function ofneural synapses. The term \"physical neural network\" highlights the use of physical hardware for computation, as opposed to software-based implementations. It broadly refers to artificial neural networks that use materials with adjustable resistance to replicate neural synapses.[172][173]\nEmbedded machine learning is a sub-field of machine learning where models are deployed onembedded systemswith limited computing resources, such aswearable computers,edge devicesandmicrocontrollers.[174][175][176][177]Running models directly on these devices eliminates the need to transfer and store data on cloud servers for further processing, thereby reducing the risk of data breaches, privacy leaks and theft of intellectual property, personal data and business secrets. Embedded machine learning can be achieved through various techniques, such ashardware acceleration,[178][179]approximate computing,[180]and model optimisation.[181][182]Common optimisation techniques includepruning,quantization,knowledge distillation, low-rank factorisation, network architecture search, and parameter sharing.\nSoftware suitescontaining a variety of machine learning algorithms include the following:"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence",
        "title": "Symbolic artificial intelligence - Wikipedia",
        "content": "Inartificial intelligence,symbolic artificial intelligence(also known asclassical artificial intelligenceorlogic-based artificial intelligence)[1][2]is the term for the collection of all methods in artificial intelligence research that are based on high-levelsymbolic(human-readable) representations of problems,logicandsearch.[3]Symbolic AI used tools such aslogic programming,production rules,semantic netsandframes, and it developed applications such asknowledge-based systems(in particular,expert systems),symbolic mathematics,automated theorem provers,ontologies, thesemantic web, andautomated planning and schedulingsystems. The Symbolic AI paradigm led to seminal ideas insearch,symbolic programminglanguages,agents,multi-agent systems, thesemantic web, and the strengths and limitations of formal knowledge andreasoning systems.\nSymbolic AI was the dominantparadigmof AI research from the mid-1950s until the mid-1990s.[4]Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine withartificial general intelligenceand considered this the ultimate goal of their field.[5]An early boom, with early successes such as theLogic TheoristandSamuel'sCheckers Playing Program, led to unrealistic expectations and promises and was followed by the firstAI Winteras funding dried up.[6][7]A second boom (1969–1986) occurred with the rise of expert systems, their promise of capturing corporate expertise, and an enthusiastic corporate embrace.[8][9]That boom, and some early successes, e.g., withXCONatDEC, was followed again by later disappointment.[9]Problems with difficulties in knowledge acquisition, maintaining large knowledge bases, and brittleness in handling out-of-domain problems arose. Another, second, AI Winter (1988–2011) followed.[10]Subsequently, AI researchers focused on addressing underlying problems in handling uncertainty and in knowledge acquisition.[11]Uncertainty was addressed with formal methods such ashidden Markov models,Bayesian reasoning, andstatistical relational learning.[12][13]Symbolic machine learning addressed the knowledge acquisition problem with contributions includingVersion Space,Valiant'sPAC learning,Quinlan'sID3decision-treelearning,case-based learning, andinductive logic programmingto learn relations.[14]\nNeural networks, a subsymbolic approach, had been pursued from early days and reemerged strongly in 2012.  Early examples areRosenblatt'sperceptronlearning work, thebackpropagationwork of Rumelhart, Hinton and Williams,[15]and work inconvolutional neural networksby LeCun et al. in 1989.[16]However, neural networks were not viewed as successful until about 2012: \"Until Big Data became commonplace, the general consensus in the Al community was that the so-called neural-network approach was hopeless. Systems just didn't work that well, compared to other methods. ... A revolution came in 2012, when a number of people, including a team of researchers working with Hinton, worked out a way to use the power ofGPUsto enormously increase the power of neural networks.\"[17]Over the next several years,deep learninghad spectacular success in handling vision,speech recognition, speech synthesis, image generation, and machine translation. However, since 2020, as inherent difficulties with bias, explanation, comprehensibility, and robustness became more apparent with deep learning approaches; an increasing number of AI researchers have called forcombiningthe best of both the symbolic and neural network approaches[18][19]and addressing areas that both approaches have difficulty with, such ascommon-sense reasoning.[17]\nA short history of symbolic AI to the present day follows below. Time periods and titles are drawn from Henry Kautz's 2020 AAAI Robert S. Engelmore Memorial Lecture[20]and the longer Wikipedia article on theHistory of AI, with dates and titles differing slightly for increased clarity.\nSuccess at early attempts in AI occurred in three main areas: artificial neural networks, knowledge representation, and heuristic search, contributing to high expectations. This section summarizes Kautz's reprise of early AI history.\nCybernetic approaches attempted to replicate the feedback loops between animals and their environments. A robotic turtle, with sensors, motors for driving and steering, and seven vacuum tubes for control, based on a preprogrammed neural net, was built as early as 1948. This work can be seen as an early precursor to later work in neural networks, reinforcement learning, and situated robotics.[21]\nAn important early symbolic AI program was theLogic theorist, written byAllen Newell,Herbert SimonandCliff Shawin 1955–56, as it was able to prove 38 elementary theorems from Whitehead and Russell'sPrincipia Mathematica. Newell, Simon, and Shaw later generalized this work to create a domain-independent problem solver,GPS(General Problem Solver). GPS solved problems represented with formal operators via state-space search usingmeans-ends analysis.[22]\nDuring the 1960s, symbolic approaches achieved great success at simulating intelligent behavior in structured environments such as game-playing, symbolic mathematics, and theorem-proving. AI research was concentrated in four institutions in the 1960s:Carnegie Mellon University,Stanford,MITand (later)University of Edinburgh. Each one developed its own style of research. Earlier approaches based oncyberneticsorartificial neural networkswere abandoned or pushed into the background.\nHerbert SimonandAllen Newellstudied human problem-solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well ascognitive science,operations researchandmanagement science. Their research team used the results ofpsychologicalexperiments to develop programs that simulated the techniques that people used to solve problems.[23][24]This tradition, centered at Carnegie Mellon University would eventually culminate in the development of theSoararchitecture in the middle 1980s.[25][26]\nIn addition to the highly specialized domain-specific kinds of knowledge that we will see later used in expert systems, early symbolic AI researchers discovered another more general application of knowledge. These were called heuristics, rules of thumb that guide a search in promising directions: \"How can non-enumerative search be practical when the underlying problem is exponentially hard? The approach advocated by Simon and Newell is to employheuristics: fast algorithms that may fail on some inputs or output suboptimal solutions.\"[27]Another important advance was to find a way to apply these heuristics that guarantees a solution will be found, if there is one, not withstanding the occasional fallibility of heuristics: \"TheA* algorithmprovided a general frame for complete and optimal heuristically guided search. A* is used as a subroutine within practically every AI algorithm today but is still no magic bullet; its guarantee of completeness is bought at the cost of worst-case exponential time.[27]\nEarly work covered both applications of formal reasoning emphasizingfirst-order logic, along with attempts to handlecommon-sense reasoningin a less formal manner.\nUnlike Simon and Newell,John McCarthyfelt that machines did not need to simulate the exact mechanisms of human thought, but could instead try to find the essence of abstract reasoning and problem-solving with logic,[28]regardless of whether people used the same algorithms.[a]His laboratory atStanford(SAIL) focused on using formallogicto solve a wide variety of problems, includingknowledge representation, planning andlearning.[32]Logic was also the focus of the work at theUniversity of Edinburghand elsewhere in Europe which led to the development of the programming languagePrologand the science of logic programming.[33][34]\nResearchers atMIT(such asMarvin MinskyandSeymour Papert)[35][36][7]found that solving difficult problems invisionandnatural language processingrequired ad hoc solutions—they argued that no simple and general principle (likelogic) would capture all the aspects of intelligent behavior.Roger Schankdescribed their \"anti-logic\" approaches as \"scruffy\" (as opposed to the \"neat\" paradigms atCMUand Stanford).[37][38]Commonsense knowledge bases(such asDoug Lenat'sCyc) are an example of \"scruffy\" AI, since they must be built by hand, one complicated concept at a time.[39][40][41]\nThe first AI winter was a shock:\nDuring the first AI summer, many people thought that machine intelligence could be achieved in just a few years. The Defense Advance Research Projects Agency (DARPA) launched programs to support AI research to use AI to solve problems of national security; in particular, to automate the translation of Russian to English for intelligence operations and to create autonomous tanks for the battlefield. Researchers had begun to realize that achieving AI was going to be much harder than was supposed a decade earlier, but a combination of hubris and disingenuousness led many university and think-tank researchers to accept funding with promises of deliverables that they should have known they could not fulfill. By the mid-1960s neither useful natural language translation systems nor autonomous tanks had been created, and a dramatic backlash set in. New DARPA leadership canceled existing AI funding programs.\n...\nOutside of the United States, the most fertile ground for AI research was the United Kingdom. The AI winter in the United Kingdom was spurred on not so much by disappointed military leaders as by rival academics who viewed AI researchers as charlatans and a drain on research funding. A professor of applied mathematics,Sir James Lighthill, was commissioned by Parliament to evaluate the state of AI research in the nation. The report stated that all of the problems being worked on in AI would be better handled by researchers from other disciplines—such as applied mathematics. The report also claimed that AI successes on toy problems could never scale to real-world applications due to combinatorial explosion.[42]\nAs limitations with weak, domain-independent methods became more and more apparent,[43]researchers from all three traditions began to buildknowledgeinto AI applications.[44][8]The knowledge revolution was driven by the realization that knowledge underlies high-performance, domain-specific AI applications.\nEdward Feigenbaumsaid:\nto describe that high performance in a specific domain requires both general and highly domain-specific knowledge. Ed Feigenbaum and Doug Lenat called this The Knowledge Principle:\n(1) The Knowledge Principle: if a program is to perform a complex task well, it must know a great deal about the world in which it operates.(2) A plausible extension of that principle, called the Breadth Hypothesis: there are two additional abilities necessary for intelligent behavior in unexpected situations: falling back on increasingly general knowledge, and analogizing to specific but far-flung knowledge.[46]\nThis \"knowledge revolution\" led to the development and deployment ofexpert systems(introduced byEdward Feigenbaum), the first commercially successful form of AI software.[47][48][49]\nKey expert systems were:\nDENDRALis considered the first expert system that relied on knowledge-intensive problem-solving. It is described below, byEd Feigenbaum, from aCommunications of the ACMinterview,Interview with Ed Feigenbaum:\nOne of the people at Stanford interested in computer-based models of mind wasJoshua Lederberg, the 1958 Nobel Prize winner in genetics. When I told him I wanted an induction \"sandbox\", he said, \"I have just the one for you.\" His lab was doing mass spectrometry of amino acids. The question was: how do you go from looking at the spectrum of an amino acid to the chemical structure of the amino acid? That's how we started the DENDRAL Project: I was good at heuristic search methods, and he had an algorithm that was good at generating the chemical problem space.\nWe did not have a grandiose vision. We worked bottom up. Our chemist wasCarl Djerassi, inventor of the chemical behind the birth control pill, and also one of the world's most respected mass spectrometrists. Carl and his postdocs were world-class experts in mass spectrometry. We began to add to their knowledge, inventing knowledge of engineering as we went along. These experiments amounted to titrating DENDRAL more and more knowledge. The more you did that, the smarter the program became. We had very good results.\nThe generalization was: in the knowledge lies the power. That was the big idea. In my career that is the huge, \"Ah ha!,\" and it wasn't the way AI was being done previously. Sounds simple, but it's probably AI's most powerful generalization.[52]\nThe other expert systems mentioned above came after DENDRAL. MYCIN exemplifies the classic expert system architecture of a knowledge-base of rules coupled to a symbolic reasoning mechanism, including the use of certainty factors to handle uncertainty. GUIDON shows how an explicit knowledge base can be repurposed for a second application, tutoring, and is an example of anintelligent tutoring system, a particular kind of knowledge-based application. Clancey showed that it was not sufficient simply to useMYCIN's rules for instruction, but that he also needed to add rules for dialogue management and student modeling.[51]XCON is significant because of the millions of dollars it savedDEC, which triggered the expert system boom where most all major corporations in the US had expert systems groups, to capture corporate expertise, preserve it, and automate it:\nBy 1988, DEC's AI group had 40 expert systems deployed, with more on the way. DuPont had 100 in use and 500 in development. Nearly every major U.S. corporation had its own Al group and was either using or investigating expert systems.[50]\nChess expert knowledge was encoded inDeep Blue. In 1996, this allowedIBM'sDeep Blue, with the help of symbolic AI, to win in a game of chess against the world champion at that time,Garry Kasparov.[53]\nA key component of the system architecture for all expert systems is the knowledge base, which stores facts and rules for problem-solving.[54]The simplest approach for an expert system knowledge base is simply a collection or network ofproduction rules. Production rules connect symbols in a relationship similar to an If-Then statement. The expert system processes the rules to make deductions and to determine what additional information it needs, i.e. what questions to ask, using human-readable symbols. For example,OPS5,CLIPSand their successorsJessandDroolsoperate in this fashion.\nExpert systems can operate in either aforward chaining– from evidence to conclusions – orbackward chaining– from goals to needed data and prerequisites – manner. More advanced knowledge-based systems, such asSoarcan also perform meta-level reasoning, that is reasoning about their own reasoning in terms of deciding how to solve problems and monitoring the success of problem-solving strategies.\nBlackboard systemsare a second kind ofknowledge-basedorexpert systemarchitecture. They model a community of experts incrementally contributing, where they can, to solve a problem. The problem is represented in multiple levels of abstraction or alternate views. The experts (knowledge sources) volunteer their services whenever they recognize they can contribute. Potential problem-solving actions are represented on an agenda that is updated as the problem situation changes. A controller decides how useful each contribution is, and who should make the next problem-solving action. One example, the BB1 blackboard architecture[55]was originally inspired by studies of how humans plan to perform multiple tasks in a trip.[56]An innovation of BB1 was to apply the same blackboard model to solving its control problem, i.e., its controller performed meta-level reasoning with knowledge sources that monitored how well a plan or the problem-solving was proceeding and could switch from one strategy to another as conditions – such as goals or times – changed. BB1 has been applied in multiple domains: construction site planning, intelligent tutoring systems, and real-time patient monitoring.\nAt the height of the AI boom, companies such asSymbolics,LMI, andTexas Instrumentswere sellingLISP machinesspecifically targeted to accelerate the development of AI applications and research. In addition, several artificial intelligence companies, such as Teknowledge andInference Corporation, were selling expert system shells, training, and consulting to corporations.\nUnfortunately, the AI boom did not last and Kautz best describes the second AI winter that followed:\nMany reasons can be offered for the arrival of the second AI winter. The hardware companies failed when much more cost-effective general Unix workstations fromSuntogether with good compilers for LISP and Prolog came onto the market. Many commercial deployments of expert systems were discontinued when they proved too costly to maintain. Medical expert systems never caught on for several reasons: the difficulty in keeping them up to date; the challenge for medical professionals to learn how to use a bewildering variety of different expert systems for different medical conditions; and perhaps most crucially, the reluctance of doctors to trust a computer-made diagnosis over their gut instinct, even for specific domains where the expert systems could outperform an average doctor. Venture capital money deserted AI practically overnight. The world AI conference IJCAI hosted an enormous and lavish trade show and thousands of nonacademic attendees in 1987 inVancouver; the main AI conference the following year, AAAI 1988 inSt. Paul, was a small and strictly academic affair.[10]\nBoth statistical approaches and extensions to logic were tried.\nOne statistical approach,hidden Markov models, had already been popularized in the 1980s for speech recognition work.[12]Subsequently, in 1988,Judea Pearlpopularized the use ofBayesian Networksas a sound but efficient way of handling uncertain reasoning with his publication of the book Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.[57]and Bayesian approaches were applied successfully in expert systems.[58]Even later, in the 1990s, statistical relational learning, an approach that combines probability with logical formulas, allowed probability to be combined with first-order logic, e.g., with eitherMarkov Logic NetworksorProbabilistic Soft Logic.\nOther, non-probabilistic extensions to first-order logic to support were also tried. For example,non-monotonic reasoningcould be used withtruth maintenance systems. Atruth maintenance systemtracked assumptions and justifications for all inferences. It allowed inferences to be withdrawn when assumptions were found out to be incorrect or a contradiction was derived. Explanations could be provided for an inference byexplaining which rules were appliedto create it and then continuing through underlying inferences and rules all the way back to root assumptions.[59]Lotfi Zadehhad introduced a different kind of extension to handle the representation of vagueness. For example, in deciding how \"heavy\" or \"tall\" a man is, there is frequently no clear \"yes\" or \"no\" answer, and a predicate for heavy or tall would instead return values between 0 and 1. Those values represented to what degree the predicates were true. Hisfuzzy logicfurther provided a means for propagating combinations of these values through logical formulas.[60]\nSymbolic machine learning approaches were investigated to address theknowledge acquisitionbottleneck. One of the earliest isMeta-DENDRAL. Meta-DENDRAL used a generate-and-test technique to generate plausible rule hypotheses to test against spectra. Domain and task knowledge reduced the number of candidates tested to a manageable size.Feigenbaumdescribed Meta-DENDRAL as\n...the culmination of my dream of the early to mid-1960s having to do with theory formation. The conception was that you had a problem solver like DENDRAL that took some inputs and produced an output. In doing so, it used layers of knowledge to steer and prune the search. That knowledge got in there because we interviewed people. But how did the people get the knowledge? By looking at thousands of spectra. So we wanted a program that would look at thousands of spectra and infer the knowledge of mass spectrometry that DENDRAL could use to solve individual hypothesis formation problems.\nWe did it. We were even able to publish new knowledge of mass spectrometry in theJournal of the American Chemical Society, giving credit only in a footnote that a program, Meta-DENDRAL, actually did it. We were able to do something that had been a dream: to have a computer program come up with a new and publishable piece of science.[52]\nIn contrast to the knowledge-intensive approach of Meta-DENDRAL,Ross Quinlaninvented a domain-independent approach to statistical classification,decision tree learning, starting first withID3[61]and then later extending its capabilities toC4.5.[62]The decision trees created areglass box, interpretable classifiers, with human-interpretable classification rules.\nAdvances were made in understanding machine learning theory, too.Tom Mitchellintroducedversion space learningwhich describes learning as a search through a space of hypotheses, with upper, more general, and lower, more specific, boundaries encompassing all viable hypotheses consistent with the examples seen so far.[63]More formally,ValiantintroducedProbably Approximately Correct Learning(PAC Learning), a framework for the mathematical analysis of machine learning.[64]\nSymbolic machine learning encompassed more than learning by example. E.g.,John Andersonprovided acognitive modelof human learning where skill practice results in a compilation of rules from a declarative format to a procedural format with hisACT-Rcognitive architecture. For example, a student might learn to apply \"Supplementary angles are two angles whose measures sum 180 degrees\" as several different procedural rules. E.g., one rule might say that if X and Y are supplementary and you know X, then Y will be 180 - X. He called his approach \"knowledge compilation\".ACT-Rhas been used successfully to model aspects of human cognition, such as learning and retention. ACT-R is also used inintelligent tutoring systems, calledcognitive tutors, to successfully teach geometry, computer programming, and algebra to school children.[65]\nInductive logic programming was another approach to learning that allowed logic programs to be synthesized from input-output examples. E.g.,Ehud Shapiro's MIS (Model Inference System) could synthesize Prolog programs from examples.[66]John R. Kozaappliedgenetic algorithmstoprogram synthesisto creategenetic programming, which he used to synthesize LISP programs. Finally,Zohar MannaandRichard Waldingerprovided a more general approach toprogram synthesisthat synthesizes afunctional programin the course of proving its specifications to be correct.[67]\nAs an alternative to logic,Roger Schankintroduced case-based reasoning (CBR). The CBR approach outlined in his book, Dynamic Memory,[68]focuses first on remembering key problem-solving cases for future use and generalizing them where appropriate. When faced with a new problem, CBR retrieves the most similar previous case and adapts it to the specifics of the current problem.[69]Another alternative to logic,genetic algorithmsandgenetic programmingare based on an evolutionary model of learning, where sets of rules are encoded into populations, the rules govern the behavior of individuals, and selection of the fittest prunes out sets of unsuitable rules over many generations.[70]\nSymbolic machine learning was applied to learning concepts, rules, heuristics, and problem-solving. Approaches, other than those above, include:\nWith the rise of deep learning, the symbolic AI approach has been compared to deep learning as complementary \"...with parallels having been drawn many times by AI researchers betweenKahneman'sresearch on human reasoning and decision making – reflected in his bookThinking, Fast and Slow– and the so-called \"AI systems 1 and 2\", which would in principle be modelled by deep learning and symbolic reasoning, respectively.\" In this view, symbolic reasoning is more apt for deliberative reasoning, planning, and explanation while deep learning is more apt for fast pattern recognition in perceptual applications with noisy data.[18][19]\nNeuro-symbolic AI attempts to integrate neural and symbolic architectures in a manner that addresses strengths and weaknesses of each, in a complementary fashion, in order to support robust AI capable of reasoning, learning, and cognitive modeling. As argued byValiant[78]and many others,[79]the effective construction of rich computationalcognitive modelsdemands the combination of sound symbolic reasoning and efficient (machine) learning models.Gary Marcus, similarly, argues that: \"We cannot construct rich cognitive models in an adequate, automated way without the triumvirate of hybrid architecture, rich prior knowledge, and sophisticated techniques for reasoning.\",[80]and in particular:\n\"To build a robust, knowledge-driven approach to AI we must have the machinery of symbol-manipulation in our toolkit. Too much of useful knowledge is abstract to make do without tools that represent and manipulate abstraction, and to date, the only machinery that we know of that can manipulate such abstract knowledge reliably is the apparatus of symbol manipulation.\"[81]\nHenry Kautz,[20]Francesca Rossi,[82]andBart Selman[83]have also argued for a synthesis. Their arguments are based on a need to address the two kinds of thinking discussed inDaniel Kahneman's book,Thinking, Fast and Slow. Kahneman describes human thinking as having two components,System 1 and System 2. System 1 is fast, automatic, intuitive and unconscious. System 2 is slower, step-by-step, and explicit. System 1 is the kind used for pattern recognition while System 2 is far better suited for planning, deduction, and deliberative thinking. In this view, deep learning best models the first kind of thinking while symbolic reasoning best models the second kind and both are needed.\nGarcezand Lamb describe research in this area as being ongoing for at least the past twenty years,[84]dating from their 2002 book on neurosymbolic learning systems.[85]A series of workshops on neuro-symbolic reasoning has been held every year since 2005.[86]\nIn their 2015 paper, Neural-Symbolic Learning and Reasoning: Contributions and Challenges, Garcez et al. argue that:\nThe integration of the symbolic and connectionist paradigms of AI has been pursued by a relatively small research community over the last two decades and has yielded several significant results. Over the last decade, neural symbolic systems have been shown capable of overcoming the so-called propositional fixation of neural networks, as McCarthy (1988) put it in response to Smolensky (1988); see also (Hinton, 1990). Neural networks were shown capable of representing modal and temporal logics (d'Avila Garcez and Lamb, 2006) and fragments of first-order logic (Bader, Hitzler, Hölldobler, 2008; d'Avila Garcez, Lamb, Gabbay, 2009). Further, neural-symbolic systems have been applied to a number of problems in the areas of bioinformatics, control engineering, software verification and adaptation, visual intelligence, ontology learning, and computer games.[79]\nApproaches for integration are varied.Henry Kautz's taxonomy of neuro-symbolic architectures, along with some examples, follows:\nMany key research questions remain, such as:\nThis section provides an overview of techniques and contributions in an overall context leading to many other, more detailed articles in Wikipedia. Sections onMachine LearningandUncertain Reasoningare covered earlier in thehistory section.\nThe key AI programming language in the US during the last symbolic AI boom period wasLISP.LISPis the second oldest programming language afterFORTRANand was created in 1958 byJohn McCarthy. LISP provided the firstread-eval-print loopto support rapid program development. Compiled functions could be freely mixed with interpreted functions. Program tracing, stepping, and breakpoints were also provided, along with the ability to change values or functions and continue from breakpoints or errors. It had the firstself-hosting compiler, meaning that the compiler itself was originally written in LISP and then ran interpretively to compile the compiler code.\nOther key innovations pioneered by LISP that have spread to other programming languages include:\nPrograms were themselves data structures that other programs could operate on, allowing the easy definition of higher-level languages.\nIn contrast to the US, in Europe the key AI programming language during that same period wasProlog. Prolog provided a built-in store of facts and clauses that could be queried by aread-eval-print loop. The store could act as a knowledge base and the clauses could act as rules or a restricted form of logic. As a subset of first-order logic Prolog was based onHorn clauseswith aclosed-world assumption—any facts not known were considered false—and aunique name assumptionfor primitive terms—e.g., the identifier barack_obama was considered to refer to exactly one object.Backtrackingandunificationare built-in to Prolog.\nAlain Colmerauerand Philippe Roussel are credited as the inventors of Prolog. Prolog is a form of logic programming, which was invented byRobert Kowalski. Its history was also influenced byCarl Hewitt'sPLANNER, an assertional database with pattern-directed invocation of methods. For more detail see thesection on the origins of Prolog in the PLANNER article.\nProlog is also a kind ofdeclarative programming. The logic clauses that describe programs are directly interpreted to run the programs specified. No explicit series of actions is required, as is the case withimperative programminglanguages.\nJapan championed Prolog for itsFifth Generation Project, intending to build special hardware for high performance. Similarly,LISP machineswere built to run LISP, but as the second AI boom turned to bust these companies could not compete with new workstations that could now run LISP or Prolog natively at comparable speeds. See thehistory sectionfor more detail.\nSmalltalkwas another influential AI programming language. For example, it introducedmetaclassesand, along withFlavorsandCommonLoops, influenced theCommon Lisp Object System, or (CLOS), that is now part ofCommon Lisp, the current standard Lisp dialect.CLOSis a Lisp-based object-oriented system that allowsmultiple inheritance, in addition to incremental extensions to both classes and metaclasses, thus providing a run-timemeta-object protocol.[90]\nFor other AI programming languages see thislist of programming languages for artificial intelligence. Currently,Python, amulti-paradigm programming language, is the most popular programming language, partly due to its extensive package library that supportsdata science, natural language processing, and deep learning.Pythonincludes a read-eval-print loop, functional elements such ashigher-order functions, andobject-oriented programmingthat includes metaclasses.\nSearch arises in many kinds of problem solving, includingplanning,constraint satisfaction, and playing games such ascheckers,chess, andgo. The best known AI-search tree search algorithms arebreadth-first search,depth-first search,A*, andMonte Carlo Search. Key search algorithms forBoolean satisfiabilityareWalkSAT,conflict-driven clause learning, and theDPLL algorithm. For adversarial search when playing games,alpha-beta pruning,branch and bound, andminimaxwere early contributions.\nMultiple different approaches to represent knowledge and then reason with those representations have been investigated. Below is a quick overview of approaches to knowledge representation and automated reasoning.\nSemantic networks,conceptual graphs,frames, andlogicare all approaches to modeling knowledge such as domain knowledge, problem-solving knowledge, and the semantic meaning of language.Ontologiesmodel key concepts and their relationships in a domain. Example ontologies areYAGO,WordNet, andDOLCE.DOLCEis an example of anupper ontologythat can be used for any domain while WordNet is a lexical resource that can also be viewed as anontology. YAGO incorporates WordNet as part of its ontology, to align facts extracted fromWikipediawith WordNetsynsets. TheDisease Ontologyis an example of a medical ontology currently being used.\nDescription logicis a logic for automated classification of ontologies and for detecting inconsistent classification data.OWLis a language used to represent ontologies withdescription logic.Protégéis an ontology editor that can read inOWLontologies and then check consistency withdeductive classifierssuch as such as HermiT.[91]\nFirst-order logic is more general than description logic. The automated theorem provers discussed below can prove theorems in first-order logic.Horn clauselogic is more restricted than first-order logic and is used in logic programming languages such as Prolog. Extensions to first-order logic includetemporal logic, to handle time;epistemic logic, to reason about agent knowledge;modal logic, to handle possibility and necessity; andprobabilistic logicsto handle logic and probability together.\nExamples of automated theorem provers for first-order logic are:\nProver9can be used in conjunction with theMace4model checker.ACL2is a theorem prover that can handle proofs by induction and is a descendant of the Boyer-Moore Theorem Prover, also known asNqthm.\nKnowledge-based systems have an explicitknowledge base, typically of rules, to enhance reusability across domains by separating procedural code and domain knowledge. A separateinference engineprocesses rules and adds, deletes, or modifies a knowledge store.\nForward chaininginference engines are the most common, and are seen inCLIPSandOPS5.Backward chainingoccurs in Prolog, where a more limited logical representation is used,Horn Clauses. Pattern-matching, specificallyunification, is used in Prolog.\nA more flexible kind of problem-solving occurs when reasoning about what to do next occurs, rather than simply choosing one of the available actions. This kind of meta-level reasoning is used in Soar and in the BB1 blackboard architecture.\nCognitive architectures such as ACT-R may have additional capabilities, such as the ability to compile frequently used knowledge into higher-levelchunks.\nMarvin Minskyfirst proposed frames as a way of interpreting common visual situations, such as an office, and Roger Schank extended this idea toscriptsfor common routines, such as dining out.Cychas attempted to capture useful common-sense knowledge and has \"micro-theories\" to handle particular kinds of domain-specific reasoning.\nQualitative simulation, such asBenjamin Kuipers's QSIM,[92]approximates human reasoning about naive physics, such as what happens when we heat a liquid in a pot on the stove. We expect it to heat and possibly boil over, even though we may not know its temperature, its boiling point, or other details, such as atmospheric pressure.\nSimilarly,Allen'stemporal interval algebrais a simplification of reasoning about time andRegion Connection Calculusis a simplification of reasoning about spatial relationships. Both can be solved withconstraint solvers.\nConstraint solvers perform a more limited kind of inference than first-order logic. They can simplify sets of spatiotemporal constraints, such as those forRCCorTemporal Algebra, along with solving other kinds of puzzle problems, such asWordle,Sudoku,cryptarithmetic problems, and so on.Constraint logic programmingcan be used to solve scheduling problems, for example withconstraint handling rules(CHR).\nTheGeneral Problem Solver(GPS) cast planning as problem-solving usedmeans-ends analysisto create plans.STRIPStook a different approach, viewing planning as theorem proving.Graphplantakes a least-commitment approach to planning, rather than sequentially choosing actions from an initial state, working forwards, or a goal state if working backwards.Satplanis an approach to planning where a planning problem is reduced to aBoolean satisfiability problem.\nNatural language processing focuses on treating language as data to perform tasks such as identifying topics without necessarily understanding the intended meaning. Natural language understanding, in contrast, constructs a meaning representation and uses that for further processing, such as answering questions.\nParsing,tokenizing,spelling correction,part-of-speech tagging,noun and verb phrase chunkingare all aspects of natural language processing long handled by symbolic AI, but since improved by deep learning approaches. In symbolic AI,discourse representation theoryand first-order logic have been used to represent sentence meanings.Latent semantic analysis(LSA) andexplicit semantic analysisalso provided vector representations of documents. In the latter case, vector components are interpretable as concepts named by Wikipedia articles.\nNew deep learning approaches based onTransformer modelshave now eclipsed these earlier symbolic AI approaches and attained state-of-the-art performance in natural languageprocessing. However, Transformer models are opaque and do not yet produce human-interpretable semantic representations for sentences and documents. Instead, they produce task-specific vectors where the meaning of the vector components is opaque.\nAgentsare autonomous systems embedded in an environment they perceive and act upon in some sense. Russell and Norvig's standard textbook on artificial intelligence is organized to reflect agent architectures of increasing sophistication.[93]The sophistication of agents varies from simple reactive agents, to those with a model of the world andautomated planningcapabilities, possibly aBDI agent, i.e., one with beliefs, desires, and intentions – or alternatively areinforcement learningmodel learned over time to choose actions – up to a combination of alternative architectures, such as a neuro-symbolic architecture[89]that includes deep learning for perception.[94]\nIn contrast, amulti-agent systemconsists of multiple agents that communicate amongst themselves with some inter-agent communication language such asKnowledge Query and Manipulation Language(KQML). The agents need not all have the same internal architecture. Advantages of multi-agent systems include the ability to divide work among the agents and to increase fault tolerance when agents are lost. Research problems includehow agents reach consensus,distributed problem solving,multi-agent learning,multi-agent planning, anddistributed constraint optimization.\nControversies arose from early on in symbolic AI, both within the field—e.g., between logicists (the pro-logic\"neats\") and non-logicists (the anti-logic\"scruffies\")—and between those who embraced AI but rejected symbolic approaches—primarilyconnectionists—and those outside the field. Critiques from outside of the field were primarily from philosophers, on intellectual grounds, but also from funding agencies, especially during the two AI winters.\nLimitations were discovered in using simple first-order logic to reason about dynamic domains. Problems were discovered both with regards to enumerating the preconditions for an action to succeed and in providing axioms for what did not change after an action was performed.\nMcCarthy and Hayes introduced theFrame Problemin 1969 in the paper, \"Some Philosophical Problems from the Standpoint of Artificial Intelligence.\"[95]A simple example occurs in \"proving that one person could get into conversation with another\", as an axiom asserting \"if a person has a telephone he still has it after looking up a number in the telephone book\" would be required for the deduction to succeed. Similar axioms would be required for other domain actions to specify whatdid notchange.\nA similar problem, called theQualification Problem, occurs in trying to enumerate thepreconditionsfor an action to succeed. An infinite number of pathological conditions can be imagined, e.g., a banana in a tailpipe could prevent a car from operating correctly.\nMcCarthy's approach to fix the frame problem wascircumscription, a kind ofnon-monotonic logicwhere deductions could be made from actions that need only specify what would change while not having to explicitly specify everything that would not change. Othernon-monotonic logicsprovidedtruth maintenance systemsthat revised beliefs leading to contradictions.\nOther ways of handling more open-ended domains includedprobabilistic reasoningsystems and machine learning to learn new concepts and rules.  McCarthy'sAdvice Takercan be viewed as an inspiration here, as it could incorporate new knowledge provided by a human in the form of assertions or rules. For example, experimental symbolic machine learning systems explored the ability to take high-level natural language advice and to interpret it into domain-specific actionable rules.\nSimilar to the problems in handling dynamic domains, common-sense reasoning is also difficult to capture in formal reasoning. Examples of common-sense reasoning include implicit reasoning about how people think or general knowledge of day-to-day events, objects, and living creatures.  This kind of knowledge is taken for granted and not viewed as noteworthy. Common-sense reasoning is an open area of research and challenging both for symbolic systems (e.g., Cyc has attempted to capture key parts of this knowledge over more than a decade) and neural systems (e.g.,self-driving carsthat do not know not to drive into cones or not to hit pedestrians walking a bicycle).\nMcCarthy viewed hisAdvice Takeras having common-sense, but his definition of common-sense was different than the one above.[96]He defined a program as having common sense \"if it automatically deduces for itself a sufficiently wide class of immediate consequences of anything it is told and what it already knows.\"\nConnectionist approaches include earlier work onneural networks,[97]such asperceptrons; work in the mid to late 80s, such asDanny Hillis'sConnection MachineandYann LeCun's advances inconvolutional neural networks; to today's more advanced approaches, such asTransformers,GANs, and other work in deep learning.\nThree philosophical positions[98]have been outlined among connectionists:\nOlazaran, in his sociological history of the controversies within the neural network community, described the moderate connectionism view as essentially compatible with current research in neuro-symbolic hybrids:\nThe third and last position I would like to examine here is what I call the moderate connectionist view, a more eclectic view of the current debate betweenconnectionismand symbolic AI. One of the researchers who has elaborated this position most explicitly isAndy Clark, a philosopher from the School of Cognitive and Computing Sciences of the University of Sussex (Brighton, England). Clark defended hybrid (partly symbolic, partly connectionist) systems. He claimed that (at least) two kinds of theories are needed in order to study and model cognition. On the one hand, for some information-processing tasks (such as pattern recognition) connectionism has advantages over symbolic models. But on the other hand, for other cognitive processes (such as serial, deductive reasoning, and generative symbol manipulation processes) the symbolic paradigm offers adequate models, and not only \"approximations\" (contrary to what radical connectionists would claim).[99]\nGary Marcushas claimed that the animus in the deep learning community against symbolic approaches now may be more sociological than philosophical:\nTo think that we can simply abandon symbol-manipulation is to suspend disbelief.\nAnd yet, for the most part, that's how most current AI proceeds.Hintonand many others have tried hard to banish symbols altogether. The deep learning hope—seemingly grounded not so much in science, but in a sort of historical grudge—is that intelligent behavior will emerge purely from the confluence of massive data and deep learning. Where classical computers and software solve tasks by defining sets of symbol-manipulating rules dedicated to particular jobs, such as editing a line in a word processor or performing a calculation in a spreadsheet,neural networkstypically try to solve tasks by statistical approximation and learning from examples.\nAccording to Marcus,Geoffrey Hintonand his colleagues have been vehemently \"anti-symbolic\":\nWhen deep learning reemerged in 2012, it was with a kind of take-no-prisoners attitude that has characterized most of the last decade. By 2015, his hostility toward all things symbols had fully crystallized. He gave a talk at an AI workshop at Stanford comparing symbols toaether, one of science's greatest mistakes.\n...\nSince then, his anti-symbolic campaign has only increased in intensity. In 2016,Yann LeCun,Bengio, and Hinton wrote a manifesto for deep learning in one of science's most important journals, Nature. It closed with a direct attack on symbol manipulation, calling not for reconciliation but for outright replacement. Later, Hinton told a gathering of European Union leaders that investing any further money in symbol-manipulating approaches was \"a huge mistake,\" likening it to investing in internal combustion engines in the era of electric cars.[100]\nPart of these disputes may be due to unclear terminology:\nTuring award winnerJudea Pearloffers a critique of machine learning which, unfortunately, conflates the terms machine learning and deep learning. Similarly, when Geoffrey Hinton refers to symbolic AI, the connotation of the term tends to be that of expert systems dispossessed of any ability to learn. The use of the terminology is in need of clarification. Machine learning is not confined toassociation rulemining, c.f. the body of work on symbolic ML and relational learning (the differences to deep learning being the choice of representation, localist logical rather than distributed, and the non-use ofgradient-based learning algorithms). Equally, symbolic AI is not just aboutproduction ruleswritten by hand. A proper definition of AI concernsknowledge representation and reasoning, autonomousmulti-agent systems, planning andargumentation, as well as learning.[101]\nIt is worth noting that, from a theoretical perspective, the boundary of advantages between connectionist AI and symbolic AI may not be as clear-cut as it appears. For instance, Heng Zhang and his colleagues have proved that mainstream knowledge representation formalisms are  recursively isomorphic, provided they are universal or have equivalent expressive power.[102]This finding implies that there is no fundamental distinction between using symbolic or connectionist knowledge representation formalisms for the realization ofartificial general intelligence(AGI). Moreover, the existence of recursive isomorphisms suggests that different technical approaches can draw insights from one another. From this perspective, it seems unnecessary to overemphasize the advantages of any single technical school; instead, mutual learning and integration may offer the most promising path toward the realization of AGI.\nAnother critique of symbolic AI is theembodied cognitionapproach:\nTheembodied cognitionapproach claims that it makes no sense to consider the brain separately: cognition takes place within a body, which is embedded in an environment. We need to study the system as a whole; the brain's functioning exploits regularities in its environment, including the rest of its body. Under the embodied cognition approach, robotics, vision, and other sensors become central, not peripheral.[103]\nRodney Brooksinventedbehavior-based robotics, one approach to embodied cognition.Nouvelle AI, another name for this approach, is viewed as an alternative tobothsymbolic AI and connectionist AI. His approach rejected representations, either symbolic or distributed, as not only unnecessary, but as detrimental. Instead, he created thesubsumption architecture, a layered architecture for embodied agents. Each layer achieves a different purpose and must function in the real world. For example, the first robot he describes inIntelligence Without Representation, has three layers. The bottom layer interprets sonar sensors to avoid objects. The middle layer causes the robot to wander around when there are no obstacles. The top layer causes the robot to go to more distant places for further exploration. Each layer can temporarily inhibit or suppress a lower-level layer. He criticized AI researchers for defining AI problems for their systems, when: \"There is no clean division between perception (abstraction) and reasoning in the real world.\"[104]He called his robots \"Creatures\" and each layer was \"composed of a fixed-topology network of simple finite state machines.\"[105]In the Nouvelle AI approach, \"First, it is vitally important to test the Creatures we build in the real world; i.e., in the same world that we humans inhabit. It is disastrous to fall into the temptation of testing them in a simplified world first, even with the best intentions of later transferring activity to an unsimplified world.\"[106]His emphasis on real-world testing was in contrast to \"Early work in AI concentrated on games, geometrical problems, symbolic algebra, theorem proving, and other formal systems\"[107]and the use of theblocks worldin symbolic AI systems such asSHRDLU.\nEach approach—symbolic, connectionist, and behavior-based—has advantages, but has been criticized by the other approaches. Symbolic AI has been criticized as disembodied, liable to the qualification problem, and poor in handling the perceptual problems where deep learning excels. In turn,connectionist AIhas been criticized as poorly suited for deliberative step-by-step problem solving, incorporating knowledge, and handling planning. Finally, Nouvelle AI excels in reactive and real-world robotics domains but has been criticized for difficulties in incorporating learning and knowledge.\nHybrid AIsincorporating one or more of these approaches are currently viewed as the path forward.[20][82][83]Russell and Norvig conclude that:\nOverall,Dreyfussaw areas where AI did not have complete answers and said that Al is therefore impossible; we now see many of these same areas undergoing continued research and development leading to increased capability, not impossibility.[103]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning - Wikipedia",
        "content": "Inmachine learning,deep learningfocuses on utilizing multilayeredneural networksto perform tasks such asclassification,regression, andrepresentation learning. The field takes inspiration frombiological neuroscienceand is centered around stackingartificial neuronsinto layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can besupervised,semi-supervisedorunsupervised.[2]\nSome common deep learning network architectures includefully connected networks,deep belief networks,recurrent neural networks,convolutional neural networks,generative adversarial networks,transformers, andneural radiance fields. These architectures have been applied to fields includingcomputer vision,speech recognition,natural language processing,machine translation,bioinformatics,drug design,medical image analysis,climate science, material inspection andboard gameprograms, where they have produced results comparable to and in some cases surpassing human expert performance.[3][4][5]\nEarly forms of neural networks were inspired by information processing and distributed communication nodes inbiological systems, particularly thehuman brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.[6]\nMost modern deep learning models are based on multi-layeredneural networkssuch asconvolutional neural networksandtransformers, although they can also includepropositional formulasor latent variables organized layer-wise in deepgenerative modelssuch as the nodes indeep belief networksand deepBoltzmann machines.[7]\nFundamentally, deep learning refers to a class ofmachine learningalgorithmsin which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in animage recognitionmodel, the raw input may be animage(represented as atensorofpixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face.\nImportantly, a deep learning process can learn which features to optimally place at which levelon its own. Prior to deep learning, machine learning techniques often involved hand-craftedfeature engineeringto transform the data into a more suitable representation for a classification algorithm to operate on. In the deep learning approach, features are not hand-crafted and the modeldiscoversuseful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.[8][2]\nThe word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantialcredit assignment path(CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For afeedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). Forrecurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.[9]No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than two. CAP of depth two has been shown to be a universal approximator in the sense that it can emulate any function.[10]Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > two) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.\nDeep learning architectures can be constructed with agreedylayer-by-layer method.[11]Deep learning helps to disentangle these abstractions and pick out which features improve performance.[8]\nDeep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data is more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner aredeep belief networks.[8][12]\nThe termdeep learningwas introduced to the machine learning community byRina Dechterin 1986,[13]and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context ofBooleanthreshold neurons.[14][15]Although the history of its appearance is apparently more complicated.[16]\nDeep neural networks are generally interpreted in terms of theuniversal approximation theorem[17][18][19][20][21]orprobabilistic inference.[22][23][8][9][24]\nThe classic universal approximation theorem concerns the capacity offeedforward neural networkswith a single hidden layer of finite size to approximatecontinuous functions.[17][18][19][20]In 1989, the first proof was published byGeorge Cybenkoforsigmoidactivation functions[17]and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik.[18]Recent work also showed that universal approximation also holds for non-bounded activation functions such asKunihiko Fukushima'srectified linear unit.[25][26]\nThe universal approximation theorem fordeep neural networksconcerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al.[21]proved that if the width of a deep neural network withReLUactivation is strictly larger than the input dimension, then the network can approximate anyLebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator.\nTheprobabilisticinterpretation[24]derives from the field ofmachine learning. It features inference,[23][7][8][9][12][24]as well as theoptimizationconcepts oftrainingandtesting, related to fitting andgeneralization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as acumulative distribution function.[24]The probabilistic interpretation led to the introduction ofdropoutasregularizerin neural networks. The probabilistic interpretation was introduced by researchers includingHopfield,WidrowandNarendraand popularized in surveys such as the one byBishop.[27]\nThere are twotypesof artificial neural network (ANN):feedforward neural network(FNN) ormultilayer perceptron(MLP) andrecurrent neural networks(RNN). RNNs have cycles in their connectivity structure, FNNs don't. In the 1920s,Wilhelm LenzandErnst Isingcreated theIsing model[28][29]which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972,Shun'ichi Amarimade this architecture adaptive.[30][31]His learning RNN was republished byJohn Hopfieldin 1982.[32]Other earlyrecurrent neural networkswere published by Kaoru Nakano in 1971.[33][34]Already in 1948,Alan Turingproduced work on \"Intelligent Machinery\"  that was not published in his lifetime,[35]containing \"ideas related to artificial evolution and learning RNNs\".[31]\nFrank Rosenblatt(1958)[36]proposed the perceptron, an MLP with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. He later published a 1962 book that also introduced variants and computer experiments, including a version with four-layer perceptrons \"with adaptive preterminal networks\" where the last two layers have learned weights (here he credits H. D. Block and B. W. Knight).[37]: section 16The book cites an earlier network by R. D. Joseph (1960)[38]\"functionally equivalent to a variation of\" this four-layer system (the book mentions Joseph over 30 times). Should Joseph therefore be considered the originator of proper adaptivemultilayer perceptronswith learning hidden units? Unfortunately, the learning algorithm was not a functional one, and fell into oblivion.\nThe first working deep learning algorithm was theGroup method of data handling, a method to train arbitrarily deep neural networks, published byAlexey Ivakhnenkoand Lapa in 1965. They regarded it as a form of polynomial regression,[39]or a generalization of Rosenblatt's perceptron to handle more complex, nonlinear, and hierarchical relationships.[40]A 1971 paper described a deep network with eight layers trained by this method,[41]which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates\".[31]\nThe first deep learningmultilayer perceptrontrained bystochastic gradient descent[42]was published in 1967 byShun'ichi Amari.[43]In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learnedinternal representationsto classify non-linearily separable pattern classes.[31]Subsequent developments in hardware and hyperparameter tunings have made end-to-endstochastic gradient descentthe currently dominant training technique.\nIn 1969,Kunihiko Fukushimaintroduced theReLU(rectified linear unit)activation function.[25][31]The rectifier has become the most popular activation function for deep learning.[44]\nDeep learning architectures forconvolutional neural networks(CNNs) with convolutional layers and downsampling layers began with theNeocognitronintroduced byKunihiko Fukushimain 1979, though not trained by backpropagation.[45][46]\nBackpropagationis an efficient application of thechain rulederived byGottfried Wilhelm Leibnizin 1673[47]to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt,[37]but he did not know how to implement this, althoughHenry J. Kelleyhad a continuous precursor of backpropagation in 1960 in the context ofcontrol theory.[48]The modern form of backpropagation was first published inSeppo Linnainmaa's master thesis (1970).[49][50][31]G.M. Ostrovski et al. republished it in 1971.[51][52]Paul Werbosapplied backpropagation to neural networks in 1982[53](his 1974 PhD thesis, reprinted in a 1994 book,[54]did not yet describe the algorithm[52]). In 1986,David E. Rumelhartet al. popularised backpropagation but did not cite the original work.[55][56]\nThetime delay neural network(TDNN) was introduced in 1987 byAlex Waibelto apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation.[57][58]In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.[59]In 1989,Yann LeCunet al. created a CNN calledLeNetforrecognizing handwritten ZIP codeson mail. Training required 3 days.[60]In 1990, Wei Zhang implemented a CNN onoptical computinghardware.[61]In 1991, a CNN was applied to medical image object segmentation[62]and breast cancer detection in mammograms.[63]LeNet-5 (1998), a 7-level CNN byYann LeCunet al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images.[64]\nRecurrent neural networks(RNN)[28][30]were further developed in the 1980s. Recurrence is used for sequence processing, and when a recurrent network is unrolled, it mathematically resembles a deep feedforward layer. Consequently, they have similar properties and issues, and their developments had mutual influences. In RNN, two early influential works were theJordan network(1986)[65]and theElman network(1990),[66]which applied RNN to study problems incognitive psychology.\nIn the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, in 1991,Jürgen Schmidhuberproposed a hierarchy of RNNs pre-trained one level at a time byself-supervised learningwhere each RNN tries to predict its own next input, which is the next unexpected input of the RNN below.[67][68]This \"neural history compressor\" usespredictive codingto learninternal representationsat multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can becollapsedinto a single RNN, bydistillinga higher levelchunkernetwork into a lower levelautomatizernetwork.[67][68][31]In 1993, a neural history compressor solved a \"Very Deep Learning\" task that required more than 1000 subsequentlayersin an RNN unfolded in time.[69]The \"P\" inChatGPTrefers to such pre-training.\nSepp Hochreiter's diploma thesis (1991)[70]implemented the neural history compressor,[67]and identified and analyzed thevanishing gradient problem.[70][71]Hochreiter proposed recurrentresidualconnections to solve the vanishing gradient problem. This led to thelong short-term memory(LSTM), published in 1995.[72]LSTM can learn \"very deep learning\" tasks[9]with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. That LSTM was not yet the modern architecture, which required a \"forget gate\", introduced in 1999,[73]which became the standard RNN architecture.\nIn 1991,Jürgen Schmidhuberalso published adversarial neural networks that contest with each other in the form of azero-sum game, where one network's gain is the other network's loss.[74][75]The first network is agenerative modelthat models aprobability distributionover output patterns. The second network learns bygradient descentto predict the reactions of the environment to these patterns. This was called \"artificial curiosity\". In 2014, this principle was used ingenerative adversarial networks(GANs).[76]\nDuring 1985–1995, inspired by statistical mechanics, several architectures and methods were developed byTerry Sejnowski,Peter Dayan,Geoffrey Hinton, etc., including theBoltzmann machine,[77]restricted Boltzmann machine,[78]Helmholtz machine,[79]and thewake-sleep algorithm.[80]These were designed for unsupervised learning of deep generative models. However, those were more computationally expensive compared to backpropagation. Boltzmann machine learning algorithm, published in 1985, was briefly popular before being eclipsed by the backpropagation algorithm in 1986. (p. 112[81]). A 1988 network became state of the art inprotein structure prediction, an early application of deep learning to bioinformatics.[82]\nBoth shallow and deep learning (e.g., recurrent nets) of ANNs forspeech recognitionhave been explored for many years.[83][84][85]These methods never outperformed non-uniform internal-handcrafting Gaussianmixture model/Hidden Markov model(GMM-HMM) technology based on generative models of speech trained discriminatively.[86]Key difficulties have been analyzed, including gradient diminishing[70]and weak temporal correlation structure in neural predictive models.[87][88]Additional difficulties were the lack of training data and limited computing power.\nMostspeech recognitionresearchers moved away from neural nets to pursue generative modeling. An exception was atSRI Internationalin the late 1990s. Funded by the US government'sNSAandDARPA, SRI researched in speech andspeaker recognition. The speaker recognition team led byLarry Heckreported significant success with deep neural networks in speech processing in the 1998NISTSpeaker Recognition benchmark.[89][90]It was deployed in the Nuance Verifier, representing the first major industrial application of deep learning.[91]\nThe principle of elevating \"raw\" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linearfilter-bankfeatures in the late 1990s,[90]showing its superiority over theMel-Cepstralfeatures that contain stages of fixed transformation from spectrograms. The raw features of speech,waveforms, later produced excellent larger-scale results.[92]\nNeural networks entered a lull, and simpler models that use task-specific handcrafted features such asGabor filtersandsupport vector machines(SVMs) became the preferred choices in the 1990s and 2000s, because of artificial neural networks' computational cost and a lack of understanding of how the brain wires its biological networks.[citation needed]\nIn 2003, LSTM became competitive with traditional speech recognizers on certain tasks.[93]In 2006,Alex Graves, Santiago Fernández, Faustino Gomez, and Schmidhuber combined it withconnectionist temporal classification(CTC)[94]in stacks of LSTMs.[95]In 2009, it became the first RNN to win apattern recognitioncontest, in connectedhandwriting recognition.[96][9]\nIn 2006, publications byGeoff Hinton,Ruslan Salakhutdinov, Osindero andTeh[97][98]deep belief networkswere developed for generative modeling. They are trained by training one restricted Boltzmann machine, then freezing it and training another one on top of the first one, and so on, then optionallyfine-tunedusing supervised backpropagation.[99]They could model high-dimensional probability distributions, such as the distribution ofMNIST images, but convergence was slow.[100][101][102]\nThe impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun.[103]Industrial applications of deep learning to large-scale speech recognition started around 2010.\nThe 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems.[104]The nature of the recognition errors produced by the two types of systems was characteristically different,[105]offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems.[23][106][107]Analysis around 2009–2010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition.[105]That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.[104][105][108]In 2010, researchers extended deep learning fromTIMITto large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed bydecision trees.[109][110][111][106]\nThe deep learning revolution started around CNN- and GPU-based computer vision.\nAlthough CNNs trained by backpropagation had been around for decades and GPU implementations of NNs for years,[112]including CNNs,[113]faster implementations of CNNs on GPUs were needed to progress on computer vision. Later, as deep learning becomes widespread, specialized hardware and algorithm optimizations were developed specifically for deep learning.[114]\nA key advance for the deep learning revolution was hardware advances, especially GPU. Some early work dated back to 2004.[112][113]In 2009, Raina, Madhavan, andAndrew Ngreported a 100M deep belief network trained on 30 NvidiaGeForce GTX 280GPUs, an early demonstration of GPU-based deep learning. They reported up to 70 times faster training.[115]\nIn 2011, a CNN namedDanNet[116][117]by Dan Ciresan, Ueli Meier, Jonathan Masci,Luca Maria Gambardella, andJürgen Schmidhuberachieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3.[9]It then won more contests.[118][119]They also showed howmax-poolingCNNs on GPU improved performance significantly.[3]\nIn 2012,Andrew NgandJeff Deancreated an FNN that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken fromYouTubevideos.[120]\nIn October 2012,AlexNetbyAlex Krizhevsky,Ilya Sutskever, andGeoffrey Hinton[4]won the large-scaleImageNet competitionby a significant margin over shallow machine learning methods. Further incremental improvements included theVGG-16network byKaren SimonyanandAndrew Zisserman[121]and Google'sInceptionv3.[122]\nThe success in image classification was then extended to the more challenging task ofgenerating descriptions(captions) for images, often as a combination of CNNs and LSTMs.[123][124][125]\nIn 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers.[126]Stacking too many layers led to a steep reduction intrainingaccuracy,[127]known as the \"degradation\" problem.[128]In 2015, two techniques were developed to train very deep networks: the Highway Network was published in May 2015, and theresidual neural network(ResNet)[129]in Dec 2015. ResNet behaves like an open-gated Highway Net.\nAround the same time, deep learning started impacting the field of art. Early examples includedGoogle DeepDream(2015), andneural style transfer(2015),[130]both of which were based on pretrained image classification neural networks, such asVGG-19.\nGenerative adversarial network(GAN) by (Ian Goodfellowet al., 2014)[131](based onJürgen Schmidhuber's principle of artificial curiosity[74][76])\nbecame state of the art in generative modeling during 2014-2018 period. Excellent image quality is achieved byNvidia'sStyleGAN(2018)[132]based on the Progressive GAN by Tero Karras et al.[133]Here the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerningdeepfakes.[134]Diffusion models(2015)[135]eclipsed GANs in generative modeling since then, with systems such asDALL·E 2(2022) andStable Diffusion(2022).\nIn 2015, Google's speech recognition improved by 49% by an LSTM-based model, which they made available throughGoogle Voice Searchonsmartphone.[136][137]\nDeep learning is part of state-of-the-art systems in various disciplines, particularly computer vision andautomatic speech recognition(ASR). Results on commonly used evaluation sets such asTIMIT(ASR) andMNIST(image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved.[104][138]Convolutional neural networks were superseded for ASR byLSTM.[137][139][140][141]but are more successful in computer vision.\nYoshua Bengio,Geoffrey HintonandYann LeCunwere awarded the 2018Turing Awardfor \"conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing\".[142]\nArtificial neural networks(ANNs) orconnectionistsystemsare computing systems inspired by thebiological neural networksthat constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manuallylabeledas \"cat\" or \"no cat\" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm usingrule-based programming.\nAn ANN is based on a collection of connected units calledartificial neurons, (analogous to biologicalneuronsin abiological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented byreal numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.\nTypically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.\nThe original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such asbackpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.\nNeural networks have been used on a variety of tasks, including computer vision,speech recognition,machine translation,social networkfiltering,playing board and video gamesand medical diagnosis.\nAs of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, or playing \"Go\"[144]).\nA deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers.[7][9]There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions.[145]These components as a whole function in a way that mimics functions of the human brain, and can be trained like any other ML algorithm.[citation needed]\nFor example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer,[146]and complex DNN have many layers, hence the name \"deep\" networks.\nDNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition ofprimitives.[147]The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.[7]For instance, it was proved that sparsemultivariate polynomialsare exponentially easier to approximate with DNNs than with shallow networks.[148]\nDeep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.[146]\nDNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or \"weights\", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights.[149]That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.\nRecurrent neural networks, in which data can flow in any direction, are used for applications such aslanguage modeling.[150][151][152][153][154]Long short-term memory is particularly effective for this use.[155][156]\nConvolutional neural networks(CNNs) are used in computer vision.[157]CNNs also have been applied toacoustic modelingfor automatic speech recognition (ASR).[158]\nAs with ANNs, many issues can arise with naively trained DNNs. Two common issues areoverfittingand computation time.\nDNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data.Regularizationmethods such as Ivakhnenko's unit pruning[41]orweight decay(ℓ2{\\displaystyle \\ell _{2}}-regularization) orsparsity(ℓ1{\\displaystyle \\ell _{1}}-regularization) can be applied during training to combat overfitting.[159]Alternativelydropoutregularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies.[160]Another interesting recent development is research into models of just enough complexity through an estimation of the intrinsic complexity of the task being modelled. This approach has been successfully applied for multivariate time series prediction tasks such as traffic prediction.[161]Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.[162]\nDNNs must consider many training parameters, such as the size (number of layers and number of units per layer), thelearning rate, and initial weights.Sweeping through the parameter spacefor optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such asbatching(computing the gradient on several training examples at once rather than individual examples)[163]speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.[164][165]\nAlternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.[166][167]\nSince the 2010s, advances in both machine learning algorithms andcomputer hardwarehave led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.[168]By 2019,graphics processing units(GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method for training large-scale commercial cloud AI .[169]OpenAIestimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.[170][171]\nSpecialelectronic circuitscalleddeep learning processorswere designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) inHuaweicellphones[172]andcloud computingservers such astensor processing units(TPU) in theGoogle Cloud Platform.[173]Cerebras Systemshas also built a dedicated system to handle large deep learning models, the CS-2, based on the largest processor in the industry, the second-generation Wafer Scale Engine (WSE-2).[174][175]\nAtomically thinsemiconductorsare considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage.\nIn 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based onfloating-gatefield-effect transistors(FGFETs).[176]\nIn 2021, J. Feldmann et al. proposed an integratedphotonichardware acceleratorfor parallel convolutional processing.[177]The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer throughwavelengthdivisionmultiplexingin conjunction withfrequency combs, and (2) extremely high data modulation speeds.[177]Their system can execute trillions of multiply-accumulate operations per second, indicating the potential ofintegratedphotonicsin data-heavy AI applications.[177]\nLarge-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn \"Very Deep Learning\" tasks[9]that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates[156]is competitive with traditional speech recognizers on certain tasks.[93]\nThe initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight majordialectsofAmerican English, where each speaker reads 10 sentences.[178]Its small size lets many configurations be tried. More importantly, the TIMIT task concernsphone-sequence recognition, which, unlike word-sequence recognition, allows weak phonebigramlanguage models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.\nThe debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007, accelerated progress in eight major areas:[23][108][106]\nMore recent speech recognition models useTransformersorTemporal Convolution Networkswith significant success and widespread applications.[183][184][185]All major commercial speech recognition systems (e.g., MicrosoftCortana,Xbox,Skype Translator,Amazon Alexa,Google Now,Apple Siri,BaiduandiFlyTekvoice search, and a range ofNuancespeech products, etc.) are based on deep learning.[23][186][187]\nA common evaluation set for image classification is theMNIST databasedata set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.[188]\nDeep learning-based image recognition has become \"superhuman\", producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs, and in 2014, with recognition of human faces.[189][190]\nDeep learning-trained vehicles now interpret 360° camera views.[191]Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.\nClosely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of\nNeural networks have been used for implementing language models since the early 2000s.[150]LSTM helped to improve machine translation and language modeling.[151][152][153]\nOther key techniques in this field are negative sampling[194]andword embedding. Word embedding, such asword2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in avector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of asprobabilistic context free grammar(PCFG) implemented by an RNN.[195]Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing.[195]Deep neural architectures provide the best results for constituency parsing,[196]sentiment analysis,[197]information retrieval,[198][199]spoken language understanding,[200]machine translation,[151][201]contextual entity linking,[201]writing style recognition,[202]named-entity recognition(token classification),[203]text classification, and others.[204]\nRecent developments generalizeword embeddingtosentence embedding.\nGoogle Translate(GT) uses a large end-to-endlong short-term memory(LSTM) network.[205][206][207][208]Google Neural Machine Translation (GNMT)uses anexample-based machine translationmethod in which the system \"learns from millions of examples\".[206]It translates \"whole sentences at a time, rather than pieces\". Google Translate supports over one hundred languages.[206]The network encodes the \"semantics of the sentence rather than simply memorizing phrase-to-phrase translations\".[206][209]GT uses English as an intermediate between most language pairs.[209]\nA large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipatedtoxic effects.[210][211]Research has explored use of deep learning to predict thebiomolecular targets,[212][213]off-targets, andtoxic effectsof environmental chemicals in nutrients, household products and drugs.[214][215][216]\nAtomNet is a deep learning system for structure-basedrational drug design.[217]AtomNet was used to predict novel candidate biomolecules for disease targets such as theEbola virus[218]andmultiple sclerosis.[219][218]\nIn 2017graph neural networkswere used for the first time to predict various properties of molecules in a large toxicology data set.[220]In 2019, generative neural networks were used to produce molecules that were validated experimentally all the way into mice.[221][222]\nRecommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations.[223][224]Multi-view deep learning has been applied for learning user preferences from multiple domains.[225]The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.\nAnautoencoderANN was used inbioinformatics, to predictgene ontologyannotations and gene-function relationships.[226]\nIn medical informatics, deep learning was used to predict sleep quality based on data from wearables[227]and predictions of health complications fromelectronic health recorddata.[228]\nDeep neural networks have shown unparalleled performance inpredicting protein structure, according to the sequence of the amino acids that make it up. In 2020,AlphaFold, a deep-learning based system, achieved a level of accuracy significantly higher than all previous computational methods.[229][230]\nDeep neural networks can be used to estimate the entropy of astochastic processand called Neural Joint Entropy Estimator (NJEE).[231]Such an estimation provides insights on the effects of inputrandom variableson an independentrandom variable. Practically, the DNN is trained as aclassifierthat maps an inputvectorormatrixX to an outputprobability distributionover the possible classes of random variable Y, given input X. For example, inimage classificationtasks, the NJEE maps a vector ofpixels' color values to probabilities over possible image classes. In practice, the probability distribution of Y is obtained by aSoftmaxlayer with number of nodes that is equal to thealphabetsize of Y. NJEE uses continuously differentiableactivation functions, such that the conditions for theuniversal approximation theoremholds. It is shown that this method provides a stronglyconsistent estimatorand outperforms other methods in case of large alphabet sizes.[231]\nDeep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement.[232][233]Modern deep learning tools demonstrate the high accuracy of detecting various diseases and the helpfulness of their use by specialists to improve the diagnosis efficiency.[234][235]\nFinding the appropriate mobile audience formobile advertisingis always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server.[236]Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.\nDeep learning has been successfully applied toinverse problemssuch asdenoising,super-resolution,inpainting, andfilm colorization.[237]These applications include learning methods such as \"Shrinkage Fields for Effective Image Restoration\"[238]which trains on an image dataset, andDeep Image Prior, which trains on the image that needs restoration.\nDeep learning is being successfully applied to financialfraud detection, tax evasion detection,[239]and anti-money laundering.[240]\nIn November 2023, researchers atGoogle DeepMindandLawrence Berkeley National Laboratoryannounced that they had developed an AI system known as GNoME. This system has contributed tomaterials scienceby discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganiccrystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through theMaterials Projectdatabase, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.[241][242][243]\nThe United States Department of Defense applied deep learning to train robots in new tasks through observation.[244]\nPhysics informed neural networks have been used to solvepartial differential equationsin both forward and inverse problems in a data driven manner.[245]One example is the reconstructing fluid flow governed by theNavier-Stokes equations. Using physics informed neural networks does not require the often expensive mesh generation that conventionalCFDmethods rely on.[246][247]\nDeep backward stochastic differential equation methodis a numerical method that combines deep learning withBackward stochastic differential equation(BSDE). This method is particularly useful for solving high-dimensional problems in financial mathematics. By leveraging the powerful function approximation capabilities ofdeep neural networks, deep BSDE addresses the computational challenges faced by traditional numerical methods in high-dimensional settings. Specifically, traditional methods like finite difference methods or Monte Carlo simulations often struggle with the curse of dimensionality, where computational cost increases exponentially with the number of dimensions. Deep BSDE methods, however, employ deep neural networks to approximate solutions of high-dimensional partial differential equations (PDEs), effectively reducing the computational burden.[248]\nIn addition, the integration ofPhysics-informed neural networks(PINNs) into the deep BSDE framework enhances its capability by embedding the underlying physical laws directly into the neural network architecture. This ensures that the solutions not only fit the data but also adhere to the governing stochastic differential equations. PINNs leverage the power of deep learning while respecting the constraints imposed by the physical models, resulting in more accurate and reliable solutions for financial mathematics problems.\nImage reconstruction is the reconstruction of the underlying images from the image-related measurements. Several works showed the better and superior performance of the deep learning methods compared to analytical methods for various applications, e.g., spectral imaging[249]and ultrasound imaging.[250]\nTraditional weather prediction systems solve a very complex system of partial differential equations. GraphCast is a deep learning based model, trained on a long history of weather data to predict how weather patterns change over time. It is able to  predict weather conditions for up to 10 days globally, at a very detailed level, and in under a minute, with precision similar to state of the art systems.[251][252]\nAn epigenetic clock is abiochemical testthat can be used to measure age. Galkin et al. used deep neural networks to train an epigenetic aging clock of unprecedented accuracy using >6,000 blood samples.[253]The clock uses information from 1000CpG sitesand predicts people with certain conditions older than healthy controls:IBD,frontotemporal dementia,ovarian cancer,obesity. The aging clock was planned to be released for public use in 2021 by anInsilico Medicinespinoff company Deep Longevity.\nDeep learning is closely related to a class of theories ofbrain development(specifically, neocortical development) proposed bycognitive neuroscientistsin the early 1990s.[254][255][256][257]These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave ofnerve growth factor) support theself-organizationsomewhat analogous to the neural networks utilized in deep learning models. Like theneocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack oftransducers, well-tuned to their operating environment. A 1995 description stated, \"...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature\".[258]\nA variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of thebackpropagationalgorithm have been proposed in order to increase its processing realism.[259][260]Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchicalgenerative modelsanddeep belief networks, may be closer to biological reality.[261][262]In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.[263]\nAlthough a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons[264]and neural populations.[265]Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system[266]both at the single-unit[267]and at the population[268]levels.\nFacebook's AI lab performs tasks such asautomatically tagging uploaded pictureswith the names of the people in them.[269]\nGoogle'sDeepMind Technologiesdeveloped a system capable of learning how to playAtarivideo games using only pixels as data input. In 2015 they demonstrated theirAlphaGosystem, which learned the game ofGowell enough to beat a professional Go player.[270][271][272]Google Translateuses a neural network to translate between more than 100 languages.\nIn 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.[273]\nAs of 2008,[274]researchers atThe University of Texas at Austin(UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor.[244]First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration betweenU.S. Army Research Laboratory(ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation.[244]Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as \"good job\" and \"bad job\".[275]\nDeep learning has attracted both criticism and comment, in some cases from outside the field of computer science.\nA main criticism concerns the lack of theory surrounding some methods.[276]Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear.[citation needed](e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as ablack box, with most confirmations done empirically, rather than theoretically.[277]\nIn further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained[278]demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article onThe Guardian's[279]website.\nSome deep learning architectures display problematic behaviors,[280]such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014)[281]and misclassifying minuscule perturbations of correctly classified images (2013).[282]Goertzelhypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-componentartificial general intelligence(AGI) architectures.[280]These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar[283]decompositions of observed entities and events.[280]Learning a grammar(visual or linguistic) from training data would be equivalent to restricting the system tocommonsense reasoningthat operates on concepts in terms of grammaticalproduction rulesand is a basic goal of both human language acquisition[284]andartificial intelligence(AI).[285]\nAs deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception.[286]By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an \"adversarial attack\".[287]\nIn 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points, and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system.[288]One defense is reverse image search, in which a possible fake image is submitted to a site such asTinEyethat can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.[289]\nAnother group showed that certainpsychedelicspectacles could fool afacial recognition systeminto thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers tostop signsand caused an ANN to misclassify them.[288]\nANNs can however be further trained to detect attempts atdeception, potentially leading attackers and defenders into an arms race similar to the kind that already defines themalwaredefense industry. ANNs have been trained to defeat ANN-based anti-malwaresoftware by repeatedly attacking a defense with malware that was continually altered by agenetic algorithmuntil it tricked the anti-malware while retaining its ability to damage the target.[288]\nIn 2016, another group demonstrated that certain sounds could make theGoogle Nowvoice command system open a particular web address, and hypothesized that this could \"serve as a stepping stone for further attacks (e.g., opening a web page hosting drive-by malware)\".[288]\nIn \"data poisoning\", false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery.[288]\nThe deep learning systems that are trained using supervised learning often rely on data that is created or annotated by humans, or both.[290]It has been argued that not only low-paidclickwork(such as onAmazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of humanmicroworkthat are often not recognized as such.[291]The philosopherRainer Mühlhoffdistinguishes five types of \"machinic capture\" of human microwork to generate training data: (1)gamification(the embedding of annotation or computation tasks in the flow of a game), (2) \"trapping and tracking\" (e.g.CAPTCHAsfor image recognition or click-tracking on Googlesearch results pages), (3) exploitation of social motivations (e.g.tagging facesonFacebookto obtain labeled facial images), (4)information mining(e.g. by leveragingquantified-selfdevices such asactivity trackers) and (5)clickwork.[291]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Bayesian_network",
        "title": "Bayesian network - Wikipedia",
        "content": "ABayesian network(also known as aBayes network,Bayes net,belief network, ordecision network) is aprobabilistic graphical modelthat represents a set of variables and theirconditional dependenciesvia adirected acyclic graph(DAG).[1]While it is one of several forms ofcausal notation, causal networks are special cases of Bayesian networks. Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.\nEfficient algorithms can performinferenceandlearningin Bayesian networks. Bayesian networks that model sequences of variables (e.g.speech signalsorprotein sequences) are calleddynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are calledinfluence diagrams.\nFormally, Bayesian networks aredirected acyclic graphs(DAGs) whose nodes represent variables in theBayesiansense: they may be observable quantities,latent variables, unknown parameters or hypotheses. Each edge represents a direct conditional dependency. Any pair of nodes that are not connected (i.e. no path connects one node to the other) represent variables that areconditionally independentof each other. Each node is associated with aprobability functionthat takes, as input, a particular set of values for the node'sparentvariables, and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. For example, ifm{\\displaystyle m}parent nodes representm{\\displaystyle m}Boolean variables, then the probability function could be represented by a table of2m{\\displaystyle 2^{m}}entries, one entry for each of the2m{\\displaystyle 2^{m}}possible parent combinations. Similar ideas may be applied to undirected, and possibly cyclic, graphs such asMarkov networks.\nSuppose we want to model the dependencies between three variables: the sprinkler (or more appropriately, its state - whether it is on or not), the presence or absence of rain and whether the grass is wet or not. Observe that two events can cause the grass to become wet: an active sprinkler or rain. Rain has a direct effect on the use of the sprinkler (namely that when it rains, the sprinkler usually is not active). This situation can be modeled with a Bayesian network (shown to the right). Each variable has two possible values, T (for true) and F (for false).\nThejoint probability functionis, by thechain rule of probability,\nwhereG= \"Grass wet (true/false)\",S= \"Sprinkler turned on (true/false)\", andR= \"Raining (true/false)\".\nThe model can answer questions about the presence of a cause given the presence of an effect (so-called inverse probability) like \"What is the probability that it is raining, given the grass is wet?\" by using theconditional probabilityformula and summing over allnuisance variables:\nUsing the expansion for the joint probability functionPr(G,S,R){\\displaystyle \\Pr(G,S,R)}and the conditional probabilities from theconditional probability tables (CPTs)stated in the diagram, one can evaluate each term in the sums in the numerator and denominator. For example,\nThen the numerical results (subscripted by the associated variable values) are\nTo answer an interventional question, such as \"What is the probability that it would rain, given that we wet the grass?\" the answer is governed by the post-intervention joint distribution function\nobtained by removing the factorPr(G∣S,R){\\displaystyle \\Pr(G\\mid S,R)}from the pre-intervention distribution. The do operator forces the value of G to be true. The probability of rain is unaffected by the action:\nTo predict the impact of turning the sprinkler on:\nwith the termPr(S=T∣R){\\displaystyle \\Pr(S=T\\mid R)}removed, showing that the action affects the grass but not the rain.\nThese predictions may not be feasible given unobserved variables, as in most policy evaluation problems. The effect of the actiondo(x){\\displaystyle {\\text{do}}(x)}can still be predicted, however, whenever the back-door criterion is satisfied.[2][3]It states that, if a setZof nodes can be observed thatd-separates[4](or blocks) all back-door paths fromXtoYthen\nA back-door path is one that ends with an arrow intoX. Sets that satisfy the back-door criterion are called \"sufficient\" or \"admissible.\" For example, the setZ=Ris admissible for predicting the effect ofS=TonG, becauseRd-separates the (only) back-door pathS←R→G. However, ifSis not observed, no other setd-separates this path and the effect of turning the sprinkler on (S=T) on the grass (G) cannot be predicted from passive observations. In that caseP(G| do(S=T)) is not \"identified\". This reflects the fact that, lacking interventional data, the observed dependence betweenSandGis due to a causal connection or is spurious\n(apparent dependence arising from a common cause,R). (seeSimpson's paradox)\nTo determine whether a causal relation is identified from an arbitrary Bayesian network with unobserved variables, one can use the three rules of \"do-calculus\"[2][5]and test whether alldoterms can be removed from the expression of that relation, thus confirming that the desired quantity is estimable from frequency data.[6]\nUsing a Bayesian network can save considerable amounts of memory over exhaustive probability tables, if the dependencies in the joint distribution are sparse. For example, a naive way of storing the conditional probabilities of 10 two-valued variables as a table requires storage space for210=1024{\\displaystyle 2^{10}=1024}values. If no variable's local distribution depends on more than three parent variables, the Bayesian network representation stores at most10⋅23=80{\\displaystyle 10\\cdot 2^{3}=80}values.\nOne advantage of Bayesian networks is that it is intuitively easier for a human to understand (a sparse set of) direct dependencies and local distributions than complete joint distributions.\nBayesian networks perform three main inference tasks:\nBecause a Bayesian network is a complete model for its variables and their relationships, it can be used to answer probabilistic queries about them. For example, the network can be used to update knowledge of the state of a subset of variables when other variables (theevidencevariables) are observed. This process of computing theposteriordistribution of variables given evidence is called probabilistic inference. The posterior gives a universalsufficient statisticfor detection applications, when choosing values for the variable subset that minimize some expected loss function, for instance the probability of decision error. A Bayesian network can thus be considered a mechanism for automatically applyingBayes' theoremto complex problems.\nThe most common exact inference methods are:variable elimination, which eliminates (by integration or summation) the non-observed non-query variables one by one by distributing the sum over the product;clique tree propagation, which caches the computation so that many variables can be queried at one time and new evidence can be propagated quickly; and recursive conditioning and AND/OR search, which allow for aspace–time tradeoffand match the efficiency of variable elimination when enough space is used. All of these methods have complexity that is exponential in the network'streewidth. The most commonapproximate inferencealgorithms areimportance sampling, stochasticMCMCsimulation, mini-bucket elimination,loopy belief propagation,generalized belief propagationandvariational methods.\nIn order to fully specify the Bayesian network and thus fully represent thejoint probability distribution, it is necessary to specify for each nodeXthe probability distribution forXconditional uponX'sparents. The distribution ofXconditional upon its parents may have any form. It is common to work with discrete orGaussian distributionssince that simplifies calculations. Sometimes only constraints on distribution are known; one can then use theprinciple of maximum entropyto determine a single distribution, the one with the greatestentropygiven the constraints. (Analogously, in the specific context of adynamic Bayesian network, the conditional distribution for the hidden state's temporal evolution is commonly specified to maximize theentropy rateof the implied stochastic process.)\nOften these conditional distributions include parameters that are unknown and must be estimated from data, e.g., via themaximum likelihoodapproach. Direct maximization of the likelihood (or of theposterior probability) is often complex given unobserved variables. A classical approach to this problem is theexpectation-maximization algorithm, which alternates computing expected values of the unobserved variables conditional on observed data, with maximizing the complete likelihood (or posterior) assuming that previously computed expected values are correct. Under mild regularity conditions, this process converges on maximum likelihood (or maximum posterior) values for parameters.\nA more fully Bayesian approach to parameters is to treat them as additional unobserved variables and to compute a full posterior distribution over all nodes conditional upon observed data, then to integrate out the parameters. This approach can be expensive and lead to large dimension models, making classical parameter-setting approaches more tractable.\nIn the simplest case, a Bayesian network is specified by an expert and is then used to perform inference. In other applications, the task of defining the network is too complex for humans. In this case, the network structure and the parameters of the local distributions must be learned from data.\nAutomatically learning the graph structure of a Bayesian network (BN) is a challenge pursued withinmachine learning. The basic idea goes back to a recovery algorithm developed by Rebane andPearl[7]and rests on the distinction between the three possible patterns allowed in a 3-node DAG:\nThe first 2 represent the same dependencies (X{\\displaystyle X}andZ{\\displaystyle Z}are independent givenY{\\displaystyle Y}) and are, therefore, indistinguishable. The collider, however, can be uniquely identified, sinceX{\\displaystyle X}andZ{\\displaystyle Z}are marginally independent and all other pairs are dependent. Thus, while theskeletons(the graphs stripped of arrows) of these three triplets are identical, the directionality of the arrows is partially identifiable. The same distinction applies whenX{\\displaystyle X}andZ{\\displaystyle Z}have common parents, except that one must first condition on those parents. Algorithms have been developed to systematically determine the skeleton of the underlying graph and, then, orient all arrows whose directionality is dictated by the conditional independences observed.[2][8][9][10]\nAn alternative method of structural learning uses optimization-based search. It requires ascoring functionand a search strategy. A common scoring function isposterior probabilityof the structure given the training data, like theBICor the BDeu. The time requirement of anexhaustive searchreturning a structure that maximizes the score issuperexponentialin the number of variables. A local search strategy makes incremental changes aimed at improving the score of the structure. A global search algorithm likeMarkov chain Monte Carlo(MCMC) can avoid getting trapped inlocal minima. Friedman et al.[11][12]discuss usingmutual informationbetween variables and finding a structure that maximizes this. They do this by restricting the parent candidate set toknodes and exhaustively searching therein.\nA particularly fast method for exact BN learning is to cast the problem as an optimization problem, and solve it usinginteger programming. Acyclicity constraints are added to the integer program (IP) during solving in the form ofcutting planes.[13]Such method can handle problems with up to 100 variables.\nIn order to deal with problems with thousands of variables, a different approach is necessary. One is to first sample one ordering, and then find the optimal BN structure with respect to that ordering. This implies working on the search space of the possible orderings, which is convenient as it is smaller than the space of network structures. Multiple orderings are then sampled and evaluated. This method has been proven to be the best available in literature when the number of variables is huge.[14]\nAnother method consists of focusing on the sub-class of decomposable models, for which theMLEhave a closed form. It is then possible to discover a consistent structure for hundreds of variables.[15]\nLearning Bayesian networks with bounded treewidth is necessary to allow exact, tractable inference, since the worst-case inference complexity is exponential in the treewidth k (under the exponential time hypothesis). Yet, as a global property of the graph, it considerably increases the difficulty of the learning process. In this context it is possible to useK-treefor effective learning.[16]\nGiven datax{\\displaystyle x\\,\\!}and parameterθ{\\displaystyle \\theta }, a simpleBayesian analysisstarts with aprior probability(prior)p(θ){\\displaystyle p(\\theta )}andlikelihoodp(x∣θ){\\displaystyle p(x\\mid \\theta )}to compute aposterior probabilityp(θ∣x)∝p(x∣θ)p(θ){\\displaystyle p(\\theta \\mid x)\\propto p(x\\mid \\theta )p(\\theta )}.\nOften the prior onθ{\\displaystyle \\theta }depends in turn on other parametersφ{\\displaystyle \\varphi }that are not mentioned in the likelihood. So, the priorp(θ){\\displaystyle p(\\theta )}must be replaced by a likelihoodp(θ∣φ){\\displaystyle p(\\theta \\mid \\varphi )}, and a priorp(φ){\\displaystyle p(\\varphi )}on the newly introduced parametersφ{\\displaystyle \\varphi }is required, resulting in a posterior probability\nThis is the simplest example of ahierarchical Bayes model.\nThe process may be repeated; for example, the parametersφ{\\displaystyle \\varphi }may depend in turn on additional parametersψ{\\displaystyle \\psi \\,\\!}, which require their own prior. Eventually the process must terminate, with priors that do not depend on unmentioned parameters.\nGiven the measured quantitiesx1,…,xn{\\displaystyle x_{1},\\dots ,x_{n}\\,\\!}each withnormally distributederrors of knownstandard deviationσ{\\displaystyle \\sigma \\,\\!},\nSuppose we are interested in estimating theθi{\\displaystyle \\theta _{i}}. An approach would be to estimate theθi{\\displaystyle \\theta _{i}}using amaximum likelihoodapproach; since the observations are independent, the likelihood factorizes and the maximum likelihood estimate is simply\nHowever, if the quantities are related, so that for example the individualθi{\\displaystyle \\theta _{i}}have themselves been drawn from an underlying distribution, then this relationship destroys the independence and suggests a more complex model, e.g.,\nwithimproper priorsφ∼flat{\\displaystyle \\varphi \\sim {\\text{flat}}},τ∼flat∈(0,∞){\\displaystyle \\tau \\sim {\\text{flat}}\\in (0,\\infty )}. Whenn≥3{\\displaystyle n\\geq 3}, this is anidentified model(i.e. there exists a unique solution for the model's parameters), and the posterior distributions of the individualθi{\\displaystyle \\theta _{i}}will tend to move, orshrinkaway from the maximum likelihood estimates towards their common mean. Thisshrinkageis a typical behavior in hierarchical Bayes models.\nSome care is needed when choosing priors in a hierarchical model, particularly on scale variables at higher levels of the hierarchy such as the variableτ{\\displaystyle \\tau \\,\\!}in the example. The usual priors such as theJeffreys prioroften do not work, because the posterior distribution will not be normalizable and estimates made by minimizing theexpected losswill beinadmissible.\nSeveral equivalent definitions of a Bayesian network have been offered. For the following, letG= (V,E) be adirected acyclic graph(DAG) and letX= (Xv),v∈Vbe a set ofrandom variablesindexed byV.\nXis a Bayesian network with respect toGif its jointprobability density function(with respect to aproduct measure) can be written as a product of the individual density functions, conditional on their parent variables:[17]\nwhere pa(v) is the set of parents ofv(i.e. those vertices pointing directly tovvia a single edge).\nFor any set of random variables, the probability of any member of ajoint distributioncan be calculated from conditional probabilities using thechain rule(given atopological orderingofX) as follows:[17]\nUsing the definition above, this can be written as:\nThe difference between the two expressions is theconditional independenceof the variables from any of their non-descendants, given the values of their parent variables.\nXis a Bayesian network with respect toGif it satisfies thelocal Markov property: each variable isconditionally independentof its non-descendants given its parent variables:[18]\nwhere de(v) is the set of descendants andV\\ de(v) is the set of non-descendants ofv.\nThis can be expressed in terms similar to the first definition, as\nThe set of parents is a subset of the set of non-descendants because the graph isacyclic.\nIn general, learning a Bayesian network from data is known to beNP-hard.[19]This is due in part to thecombinatorial explosionofenumerating DAGsas the number of variables increases. Nevertheless, insights about an underlying Bayesian network can be learned from data in polynomial time by focusing on its marginal independence structure:[20]while the conditional independence statements of a distribution modeled by a Bayesian network are encoded by a DAG (according to the factorization and Markov properties above), its marginal independence statements—the conditional independence statements in which the conditioning set is empty—are encoded by asimple undirected graphwith special properties such as equalintersectionandindependence numbers.\nDeveloping a Bayesian network often begins with creating a DAGGsuch thatXsatisfies the local Markov property with respect toG. Sometimes this is acausalDAG. The conditional probability distributions of each variable given its parents inGare assessed. In many cases, in particular in the case where the variables are discrete, if the joint distribution ofXis the product of these conditional distributions, thenXis a Bayesian network with respect toG.[21]\nTheMarkov blanketof a node is the set of nodes consisting of its parents, its children, and any other parents of its children. The Markov blanket renders the node independent of the rest of the network; the joint distribution of the variables in the Markov blanket of a node is sufficient knowledge for calculating the distribution of the node.Xis a Bayesian network with respect toGif every node is conditionally independent of all other nodes in the network, given itsMarkov blanket.[18]\nThis definition can be made more general by defining the \"d\"-separation of two nodes, where d stands for directional.[2]We first define the \"d\"-separation of a trail and then we will define the \"d\"-separation of two nodes in terms of that.\nLetPbe a trail from nodeutov. A trail is a loop-free, undirected (i.e. all edge directions are ignored) path between two nodes. ThenPis said to bed-separated by a set of nodesZif any of the following conditions holds:\nThe nodesuandvared-separated byZif all trails between them ared-separated. Ifuandvare not d-separated, they are d-connected.\nXis a Bayesian network with respect toGif, for any two nodesu,v:\nwhereZis a set whichd-separatesuandv. (TheMarkov blanketis the minimal set of nodes whichd-separates nodevfrom all other nodes.)\nAlthough Bayesian networks are often used to representcausalrelationships, this need not be the case: a directed edge fromutovdoes not require thatXvbe causally dependent onXu. This is demonstrated by the fact that Bayesian networks on the graphs:\nare equivalent: that is they impose exactly the same conditional independence requirements.\nA causal network is a Bayesian network with the requirement that the relationships be causal. The additional semantics of causal networks specify that if a nodeXis actively caused to be in a given statex(an action written as do(X=x)), then the probability density function changes to that of the network obtained by cutting the links from the parents ofXtoX, and settingXto the caused valuex.[2]Using these semantics, the impact of external interventions from data obtained prior to intervention can be predicted.\nIn 1990, while working at Stanford University on large bioinformatic applications, Cooper proved that exact inference in Bayesian networks isNP-hard.[22]This result prompted research on approximation algorithms with the aim of developing a tractable approximation to probabilistic inference. In 1993, Paul Dagum andMichael Lubyproved two surprising results on the complexity of approximation of probabilistic inference in Bayesian networks.[23]First, they proved that no tractabledeterministic algorithmcan approximate probabilistic inference to within anabsolute errorɛ< 1/2. Second, they proved that no tractablerandomized algorithmcan approximate probabilistic inference to within an absolute errorɛ< 1/2 with confidence probability greater than 1/2.\nAt about the same time,Rothproved that exact inference in Bayesian networks is in fact#P-complete(and thus as hard as counting the number of satisfying assignments of aconjunctive normal formformula (CNF)) and that approximate inference within a factor 2n1−ɛfor everyɛ> 0, even for Bayesian networks with restricted architecture, is NP-hard.[24][25]\nIn practical terms, these complexity results suggested that while Bayesian networks were rich representations for AI and machine learning applications, their use in large real-world applications would need to be tempered by either topological structural constraints, such as naïve Bayes networks, or by restrictions on the conditional probabilities. The bounded variance algorithm[26]developed by Dagum and Luby was the first provable fast approximation algorithm to efficiently approximate probabilistic inference in Bayesian networks with guarantees on the error approximation. This powerful algorithm required the minor restriction on the conditional probabilities of the Bayesian network to be bounded away from zero and one by1/p(n){\\displaystyle 1/p(n)}wherep(n){\\displaystyle p(n)}was any polynomial of the number of nodes in the network,n{\\displaystyle n}.\nNotable software for Bayesian networks include:\nThe term Bayesian network was coined byJudea Pearlin 1985 to emphasize:[28]\nIn the late 1980s Pearl'sProbabilistic Reasoning in Intelligent Systems[30]andNeapolitan'sProbabilistic Reasoning in Expert Systems[31]summarized their properties and established them as a field of study."
    },
    {
        "url": "https://en.wikipedia.org/wiki/Evolutionary_algorithm",
        "title": "Evolutionary algorithm - Wikipedia",
        "content": "Evolutionary algorithms(EA) reproduce essential elements of biologicalevolutionin acomputer algorithmin order to solve \"difficult\" problems, at leastapproximately, for which no exact or satisfactory solution methods are known. They aremetaheuristicsandpopulation-based bio-inspired algorithms[1]andevolutionary computation, which itself are part of the field ofcomputational intelligence.[2]The mechanisms of biological evolution that an EA mainly imitates arereproduction,mutation,recombinationandselection.Candidate solutionsto theoptimization problemplay the role of individuals in a population, and thefitness functiondetermines the quality of the solutions (see alsoloss function). Evolution of the population then takes place after the repeated application of the above operators.\nEvolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlyingfitness landscape. Techniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations ofmicroevolution(microevolutionary processes) and planning models based upon cellular processes. In most real applications of EAs, computational complexity is a prohibiting factor.[3]In fact, this computational complexity is due to fitness function evaluation.Fitness approximationis one of the solutions to overcome this difficulty. However, seemingly simple EA can solve often complex problems;[4][5][6]therefore, there may be no direct link between algorithm complexity and problem complexity.\nThe following is an example of a generic evolutionary algorithm:[7][8][9]\nSimilar techniques differ ingenetic representationand other implementation details, and the nature of the particular applied problem.\nThe following theoretical principles apply to all or almost all EAs.\nTheno free lunch theoremof optimization states that all optimization strategies are equally effective when the set of all optimization problems is considered. Under the same condition, no evolutionary algorithm is fundamentally better than another. This can only be the case if the set of all problems is restricted. This is exactly what is inevitably done in practice. Therefore, to improve an EA, it must exploit problem knowledge in some form (e.g. by choosing a certain mutation strength or aproblem-adapted coding). Thus, if two EAs are compared, this constraint is implied. In addition, an EA can use problem specific knowledge by, for example, not randomly generating the entire start population, but creating some individuals throughheuristicsor other procedures.[18][19]Another possibility to tailor an EA to a given problem domain is to involve suitable heuristics,local search proceduresor other problem-related procedures in the process of generating the offspring. This form of extension of an EA is also known as amemetic algorithm. Both extensions play a major role in practical applications, as they can speed up the search process and make it more robust.[18][20]\nFor EAs in which, in addition to the offspring, at least the best individual of the parent generation is used to form the subsequent generation (so-called elitist EAs), there is a general proof ofconvergenceunder the condition that anoptimumexists.Without loss of generality, a maximum search is assumed for the proof:\nFrom the property of elitist offspring acceptance and the existence of the optimum it follows that per generationk{\\displaystyle k}an improvement of the fitnessF{\\displaystyle F}of the respective best individualx′{\\displaystyle x'}will occur with a probabilityP>0{\\displaystyle P>0}. Thus:\nI.e., the fitness values represent amonotonicallynon-decreasingsequence, which isboundeddue to the existence of the optimum. From this follows the convergence of the sequence against the optimum.\nSince the proof makes no statement about the speed of convergence, it is of little help in practical applications of EAs. But it does justify the recommendation to use elitist EAs. However, when using the usualpanmicticpopulation model, elitist EAs tend toconverge prematurelymore than non-elitist ones.[21]In a panmictic population model, mate selection (see step 4 of thegeneric definition) is such that every individual in the entire population is eligible as a mate. Innon-panmictic populations, selection is suitably restricted, so that the dispersal speed of better individuals is reduced compared to panmictic ones. Thus, the general risk of premature convergence of elitist EAs can be significantly reduced by suitable population models that restrict mate selection.[22][23]\nWith the theory of virtual alphabets,David E. Goldbergshowed in 1990 that by using a representation with real numbers, an EA that uses classicalrecombination operators(e.g. uniform or n-point crossover) cannot reach certain areas of the search space, in contrast to a coding with binary numbers.[24]This results in the recommendation for EAs with real representation to use arithmetic operators for recombination (e.g. arithmetic mean or intermediate recombination). With suitable operators, real-valued representations are more effective than binary ones, contrary to earlier opinion.[25][26]\nA possible limitation[according to whom?]of many evolutionary algorithms is their lack of a cleargenotype–phenotype distinction. In nature, the fertilized egg cell undergoes a complex process known asembryogenesisto become a maturephenotype. This indirectencodingis believed to make the genetic search more robust (i.e. reduce the probability of fatal mutations), and also may improve theevolvabilityof the organism.[27][28]Such indirect (also known as generative or developmental) encodings also enable evolution to exploit the regularity in the environment.[29]Recent work in the field ofartificial embryogeny, or artificial developmental systems, seeks to address these concerns. Andgene expression programmingsuccessfully explores a genotype–phenotype system, where the genotype consists of linear multigenic chromosomes of fixed length and the phenotype consists of multiple expression trees or computer programs of different sizes and shapes.[30][improper synthesis?]\nBoth method classes have in common that their individual search steps are determined by chance. The main difference, however, is that EAs, like many other metaheuristics, learn from past search steps and incorporate this experience into the execution of the next search steps in a method-specific form. With EAs, this is done firstly through the fitness-based selection operators for partner choice and the formation of the next generation. And secondly, in the type of search steps: In EA, they start from a current solution and change it or they mix the information of two solutions. In contrast, when dicing out new solutions inMonte-Carlo methods, there is usually no connection to existing solutions.[31][32]\nIf, on the other hand, the search space of a task is such that there is nothing to learn, Monte-Carlo methods are an appropriate tool, as they do not contain any algorithmic overhead that attempts to draw suitable conclusions from the previous search. An example of such tasks is the proverbialsearch for a needle in a haystack, e.g. in the form of a flat (hyper)plane with a single narrow peak.\nThe areas in which evolutionary algorithms are practically used are almost unlimited[6]and range from industry,[33][34]engineering,[3][4][35]complex scheduling,[5][36][37]agriculture,[38]robot movement planning[39]and finance[40][41]to research[42][43]andart. The application of an evolutionary algorithm requires some rethinking from the inexperienced user, as the approach to a task using an EA is different from conventional exact methods and this is usually not part of the curriculum of engineers or other disciplines. For example, the fitness calculation must not only formulate the goal but also support the evolutionary search process towards it, e.g. by rewarding improvements that do not yet lead to a better evaluation of the original quality criteria. For example, if peak utilisation of resources such as personnel deployment or energy consumption is to be avoided in a scheduling task, it is not sufficient to assess the maximum utilisation. Rather, the number and duration of exceedances of a still acceptable level should also be recorded in order to reward reductions below the actual maximum peak value.[44]There are therefore some publications that are aimed at the beginner and want to help avoiding beginner's mistakes as well as leading an application project to success.[44][45][46]This includes clarifying the fundamental question of when an EA should be used to solve a problem and when it is better not to.\nThere are some other proven and widely used methods of nature inspired global search techniques such as\nIn addition, many new nature-inspired or metaphor-guided algorithms have been proposed since the beginning of this century[when?]. For criticism of most publications on these, see the remarks at the end of the introduction to the article onmetaheuristics.\nIn 2020,Googlestated that their AutoML-Zero can successfully rediscover classic algorithms such as the concept of neural networks.[50]\nThe computer simulationsTierraandAvidaattempt to modelmacroevolutionarydynamics.\n[51][52]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Hybrid_intelligent_system",
        "title": "Hybrid intelligent system - Wikipedia",
        "content": "Hybrid intelligent systemdenotes a software system which employs, in parallel, a combination of methods and techniques from artificial intelligence subfields, such as:\nFrom thecognitive scienceperspective, every natural intelligent system is hybrid because it performs mental operations on both the symbolic and subsymbolic levels. For the past few years, there has been an increasing discussion of the importance ofA.I. Systems Integration. Based on notions that there have already been created simple and specificAIsystems (such as systems forcomputer vision,speech synthesis, etc., or software that employs some of the models mentioned above) and now is the time for integration to create broadAIsystems. Proponents of this approach are researchers such asMarvin Minsky,Ron Sun,Aaron Sloman,Angelo DalliandMichael A. Arbib.\nAn example hybrid is ahierarchical control systemin which the lowest, reactive layers are sub-symbolic. The higher layers, having relaxed time constraints, are capable of reasoning from an abstract world model and performingplanning.\nIntelligent systems usually rely on hybrid reasoning processes, which includeinduction,deduction,abductionand reasoning byanalogy."
    },
    {
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_systems_integration",
        "title": "Artificial intelligence systems integration - Wikipedia",
        "content": "The core idea ofartificial intelligence systems integrationis making individualsoftware components, such asspeech synthesizers, interoperable with other components, such ascommon sense knowledgebases, in order to create larger, broader and more capable A.I. systems. The main methods that have been proposed for integration are message routing, or communication protocols that the software components use to communicate with each other, often through a middlewareblackboard system.\nMostartificial intelligencesystems involve some sort of integrated technologies, for example, the integration of speech synthesis technologies with that ofspeech recognition. However, in recent years, there has been an increasing discussion on the importance ofsystems integrationas a field in its own right. Proponents of this approach are researchers such asMarvin Minsky,Aaron Sloman,Deb Roy,Kristinn R. ThórissonandMichael A. Arbib. A reason for the recent attention A.I. integration is attracting is that there have already been created a number of (relatively) simple A.I. systems for specific problem domains (such ascomputer vision,speech synthesis, etc.), and that integrating what's already available is a more logical approach to broader A.I. than building monolithic systems from scratch.\nThe focus on systems' integration, especially with regard to modular approaches, derive from the fact that most intelligences of significant scales are composed of a multitude of processes and/or utilizemulti-modalinput and output. For example, a humanoid-type of intelligence would preferably have to be able to talk using speech synthesis, hear using speech recognition, understand using a logical (or some other undefined) mechanism, and so forth. In order to produce artificially intelligent software of broader intelligence, integration of these modalities is necessary.\nCollaboration is an integral part ofsoftware developmentas evidenced by the size of software companies and the size of their software departments. Among the tools to ease software collaboration are various procedures and standards that developers can follow to ensure quality, reliability and that their software is compatible with software created by others (such asW3Cstandards for webpage development). However, collaboration in fields of A.I. has been lacking, for the most part not seen outside the respected schools, departments or research institutes (and sometimes not within them either). This presents practitioners of A.I. systems integration with a substantial problem and often causes A.I. researchers to have to 're-invent the wheel' each time they want a specific functionality to work with their software. Even more damaging is the \"not invented here\" syndrome, which manifests itself in a strong reluctance of A.I. researchers to build on the work of others.\nThe outcome of this in A.I. is a large set of \"solution islands\": A.I. research has produced numerous isolated software components and mechanisms that deal with various parts of intelligence separately. To take some examples:\nWith the increased popularity of thefree software movement, a lot of the software being created, including A.I. systems, is available for public exploit. The next natural step is to merge these individual software components into coherent, intelligent systems of a broader nature. As a multitude of components (that often serve the same purpose) have already been created by the community, the most accessible way of integration is giving each of these components an easy way to communicate with each other. By doing so, each component by itself becomes a module, which can then be tried in various settings and configurations of larger architectures. Some challenging and limitations of using A.I. software is the uncontrolled fatal errors. For example, serious and fatal errors have been discovered in very precise fields such as human oncology, as in an article published in the journal Oral Oncology Reports entitled \"When AI goes wrong: Fatal errors in oncological research reviewing assistance\".[1]The article pointed out a grave error in artificial intelligence based on GBT in the field of biophysics.\nMany online communities for A.I. developers exist where tutorials, examples, and forums aim at helping both beginners and experts build intelligent systems. However, few communities have succeeded in making a certain standard, or a code of conduct popular to allow the large collection of miscellaneous systems to be integrated with ease.\nTheconstructionist design methodology(CDM, or 'Constructionist A.I.') is a formal methodology proposed in 2004, for use in the development of cognitive robotics, communicative humanoids and broad AI systems. The creation of such systems requires the integration of a large number of functionalities that must be carefully coordinated to achieve coherent system behavior. CDM is based on iterative design steps that lead to the creation of a network of named interacting modules, communicating via explicitly typed streams and discrete messages. The OpenAIR message protocol (see below) was inspired by the CDM and has frequently been used to aid in the development of intelligent systems using CDM."
    },
    {
        "url": "https://en.wikipedia.org/wiki/Open-source_artificial_intelligence",
        "title": "Open-source artificial intelligence - Wikipedia",
        "content": "Open-source artificial intelligenceis an AI system that is freely available to use, study, modify, and share.[1]These attributes extend to each of the system's components, including datasets, code, and model parameters, promoting a collaborative and transparent approach to AI development.[1]Free and open-source software(FOSS) licenses, such as theApache License,MIT License, andGNU General Public License, outline the terms under which open-source artificial intelligence can be accessed, modified, and redistributed.[2]\nThe open-source model provides wider access to AI technology, allowing more individuals and organizations to participate in AI research and development.[3][4]In contrast, closed-source artificial intelligence is proprietary, restricting access to the source code and internal components.[3]Companies often develop closed products in an attempt to keep a competitive advantage in the marketplace.[5]However, some experts suggest that open-source AI tools may have a development advantage over closed-source products and have the potential to overtake them in the marketplace.[5][4]\nPopular open-source artificial intelligence project categories includelarge language models,machine translationtools, andchatbots.[6]Forsoftware developersto produce open-source artificial intelligence (AI) resources, they must trust the various other open-source software components they use in its development.[7][8]Open-source AI software has been speculated to have potentially increased risk compared to closed-source AI as bad actors may remove safety protocols of public models as they wish.[4]Similarly, closed-source AI has also been speculated to have an increased risk compared to open-source AI due to issues of dependence, privacy, opaque algorithms, corporate control and limited availability while potentially slowing beneficial innovation.[9][10][11]\nThere also is a debate about the openness of AI systems as openness is differentiated[12]– an article inNaturesuggests that some systems presented as open, such as Meta'sLlama 3, \"offer little more than an API or the ability to download a model subject to distinctly non-open use restrictions\". Such software has been criticized as \"openwashing\"[13]systems that are better understood as closed.[10]There are some works and frameworks that assess the openness of AI systems[14][12]as well as a new definition by theOpen Source Initiativeabout what constitutes open source AI.[15][16][17]Some large language models are released as open-weight, which means that their trained parameters are publicly available, even if the training code and data aren't.[18][19]\nThe history of open-source artificial intelligence is intertwined with both the development of AI technologies and the growth of the open-source software movement.[20]Open-source AI has evolved significantly over the past few decades, with contributions from various academic institutions, research labs, tech companies, and independent developers.[21]This section explores the major milestones in the development of open-source AI, from its early days to its current state.\nThe concept of AI dates back to the mid-20th century, when computer scientists likeAlan TuringandJohn McCarthylaid the groundwork for modern AI theories and algorithms.[22]An early form of AI, thenatural language processing\"doctor\"ELIZA, was re-implemented and shared in 1977 by Jeff Shrager as a BASIC program, and soon translated to many other languages. Early AI research focused on developingsymbolic reasoning systemsandrule-based expert systems.[23]\nDuring this period, the idea of open-source software was beginning to take shape, with pioneers likeRichard Stallmanadvocating for free software as a means to promote collaboration and innovation in programming.[24]TheFree Software Foundation, founded in 1985 by Stallman, was one of the first major organizations to promote the idea of software that could be freely used, modified, and distributed. The ideas from this movement eventually influenced the development of open-source AI, as more developers began to see the potential benefits of open collaboration in software creation, including AI models and algorithms.[25][26]\nIn the 1990s, open-source software began to gain more traction,[27]the rise of machine learning and statistical methods also led to the development of more practical AI tools. In 1993, the CMU Artificial Intelligence Repository was initiated, with a variety of openly shared software.[28]\nIn the early 2000s open-source AI began to take off, with the release of more user-friendly foundational libraries and frameworks that were available for anyone to use and contribute to.[29]\nOpenCVwas released in 2000[30]with a variety of traditional AI algorithms likedecision trees,k-Nearest Neighbors(kNN),Naive BayesandSupport Vector Machines(SVM).[31]\nIn 2007,Scikit-learnwas released.[32]It became one of the most widely used libraries for general-purpose machine learning due to its ease of use and robust functionality, providing implementations of common algorithms like regression, classification, and clustering.[33][34]Theanowas also released in the same year.[35]\nOpen-source deep learning framework asTorchwas released in 2002 and made open-source with Torch7 in 2011, and was later augmented byPyTorch, andTensorFlow.[36][37]These frameworks allowed researchers and developers to build and train neural networks for tasks like image recognition, natural language processing (NLP), and autonomous driving.[38][39]\nAlexNetwas released in 2012,[40]andWord2vecfornatural language processingby Google in 2013.[41][42]\nIn 2014,GloVe, a competitor to Word2vec, was released source code under an Apache 2.0 license, documented the datasets they trained on, and released the model weights under aPublic DomainDedication and License.[43]\nWith the announcement of GPT-2, OpenAI originally planned to keep the source code of their models private citing concerns about malicious applications.[44]After OpenAI faced public backlash, however, it released the source code for GPT-2 to GitHub three months after its release.[44]OpenAI has not publicly released the source code or pretrained weights for the GPT-3 or GPT-4 models, though their functionalities can be integrated by developers through the OpenAI API.[45][46]\nThe rise oflarge language models(LLMs) andgenerative AI, such as OpenAI's GPT-3 (2020), further propelled the demand for open-source AI frameworks.[47][48]These models have been used in a variety of applications, including chatbots, content creation, and code generation, demonstrating the broad capabilities of AI systems.[49]At the time of GPT-3's release GPT-2 was still the most powerful open source language model in the world, spurringEleutherAIto train and release GPT-Neo[50]and GPT-J[50][51]in 2021.\nIn February 2022 EleutherAI released GPT-NeoX-20B, taking back the title of most powerful open source language model in the world from Meta whose FairSeq Dense 13B model had surpassed GPT-J at the end of 2021.[52]2022 also saw the rise of larger and more powerful models under various non-open source licenses including Meta's OPT[53]and Galactica,[54][55]the BigScience Research Workshop's BLOOM,[56][57]and Tsinghua University's GLM.\nDuring early negotiations in 2021 and 2022 around AI legislation in Europe, proposals were made to avoid over-regulating open-source AI.[58]Noting that some organizations were mis-applying the \"open-source\" label to their work, in 2022, theOpen Source Initiative, which originally came up with the widely accepted standard for open-source software in 1998, started working with experts on a definition of \"open-source\" that would fit the needs of AI software and models. The most controversial aspect relates to data access, since some models are trained on sensitive data which can't be released. In 2024, they finalized the Open Source AI Definition 1.0 (OSAID 1.0), with endorsements from over 20 organizations.[59][60]It requires full release of the software for processing the data, training the model and making inferences from the model. For the data, it only requires \"sufficiently detailed information about the data used to train the system so that a skilled person can build a substantially equivalent system\".[60]\nIn 2023Llama1 and 2, MosaicML's MPT,[61][62]andMistral AI's Mistral and Mixtral models were released.\nIn 2024, Meta released a collection of large AI models, includingLlama3.1 405B, comparable to the most advanced closed-source models.[63]The company claimed its approach to AI would be open-source, differing from other major tech companies.[63]TheOpen Source Initiativeand others stated that Llama is not open-source despite Meta describing it as open-source, due to Llama's software license prohibiting it from being used for some purposes.[64][65][66]\nDeepSeekreleased their V3 LLM in December 2024, and their R1reasoning modelon January 20, 2025, both as open-weights models under the MIT license.[67][68]\nSince the release of OpenAI's proprietary ChatGPT model in late 2022, there have been only a few fully open (weights, data, code, etc.) large language models released. Among these are theOLMo seriesof models[69][70]released by theAllen Institute for AI.\nIn parallel with the development of AI models, there has been growing interest in ensuring ethical standards in AI development.[71][72]This includes addressing concerns such as bias, privacy, and the potential for misuse of AI systems.[71][72]As a result, frameworks for responsible AI development and the creation of guidelines for documenting ethical considerations, such as the Model Card concept introduced by Google,[73]have gained popularity, though studies show the continued need for their adoption to avoid unintended negative outcomes.[74][75]\nThe LF AI & Data Foundation, a project under theLinux Foundation, has significantly influenced the open-source AI landscape by fostering collaboration and innovation, and supporting open-source projects.[76]By providing a neutral platform, LF AI & Data unites developers, researchers, and organizations to build cutting-edge AI and data solutions, addressing critical technical challenges and promoting ethical AI development.[77]\nAs of October 2024, the foundation comprised 77 member companies from North America, Europe, and Asia, and hosted 67 open-source software (OSS) projects contributed by a diverse array of organizations, including silicon valley giants such asNvidia,Amazon,Intel, andMicrosoft.[78]Other large conglomerates likeAlibaba,TikTok,AT&T, andIBMhave also contributed.[78]Research organizations such as NYU, University of Michigan AI labs, Columbia University, Penn State are also associate members of the LF AI & Data Foundation.[78]\nIn 2024, while the OSAID was in development, theLinux Foundationwas also developing a rubric of components of an AI system, and published a draft Model Openness Framework (MOF).[79]The MOF is a system for evaluating and classifying the completeness and openness of machine learning models. It included three classes of openness, from more open to less open: Class I: Open Science Model; Class II: Open Tooling Model, and Class III: Open Model.[79][80]The Linux Foundation participated in the OSAID development, and OSAID adopted the same rubric of components.[81]\nIn September 2022, the PyTorch Foundation was established to oversee the widely usedPyTorchdeep learning framework, which was donated by Meta.[82]The foundation's mission is to drive the adoption of AI tools by fostering and sustaining an ecosystem of open-source, vendor-neutral projects integrated with PyTorch, and to democratize access to state-of-the-art tools, libraries, and other components, making these innovations accessible to everyone.[83]\nThe PyTorch Foundation also separates business and technical governance, with the PyTorch project maintaining its technical governance structure, while the foundation handles funding, hosting expenses, events, and management of assets such as the project's website, GitHub repository, and social media accounts, ensuring open community governance.[83]Upon its inception, the foundation formed a governing board comprising representatives from its initial members:AMD,Amazon Web Services,Google Cloud,Hugging Face, IBM, Intel, Meta, Microsoft, and NVIDIA.[83]\nOpen-source AI has assisted in developing and adopting of Large Language Models (LLMs). While proprietary models like OpenAI's GPT series have redefined what is possible in applications such as interactive dialogue systems and automated content creation, fully open-source models have also made significant strides. Google's BERT is an open-source model widely used for tasks like entity recognition and language translation, establishing itself as a versatile tool in NLP.[84]These open-source LLMs have democratized access to advanced language technologies and reduce reliance on proprietary systems.[85]\nHugging Face's MarianMT is an example of Machine Translation, providing support for a wide range of language pairs.[86]Another notable is model OpenNMT.[87]Alongside these open-source models, open-source datasets such as the WMT (Workshop on Machine Translation) datasets including,Europarl Corpus, and OPUS contribute to the sector[88][89]and assist developers to train and fine-tune models for specific languages.[88]\nLibraries includingOpenCVsupport real-time computer vision applications, such as image recognition, motion tracking, and facial detection.[90][91]Originally developed byIntel, OpenCV has become one of the most popular libraries for computer vision.[91][90]Other open-source computer vision models includingYOLO(You Only Look Once) and Detectron2 also offer similar features.[92][93]\nUnlike the previous generations of Computer Vision models, which process image data through convolutional layers, newer generations of computer vision models useVision Transformers[94]which break down an image into smaller patches to identify which areas of the image are most relevant[94]which generally produces more accurate results.[95]\nOpen-source artificial intelligence has made a notable impact in robotics by providing a flexible, scalable development environment for both academia and industry.[96]TheRobot Operating System(ROS)[97]is an example of a framework used by developers to work across different hardware platforms and robotic architectures.[96]Gazebois another an open-sourcerobotic simulation softwareused to test robotic systems in a virtual environment before real-world deployment.[98]\nIn thehealthcare industry, open-source AI has been used indiagnostics,patient care, andpersonalized treatmentoptions.[99]Open-source libraries have been used for medical imaging for tasks such astumor detection, improving the speed and accuracy of diagnostic processes.[100][99]Additionally, OpenChem, an open-source library specifically geared toward chemistry and biology applications, enables the development of predictive models fordrug discovery, helping researchers identify potential compounds for treatment.[101]\nMeta's Llama models, which have been described as open-source by Meta, were adopted by U.S. defense contractors likeLockheed MartinandOracleafter unauthorized adaptations by Chinese researchers affiliated with thePeople's Liberation Army(PLA) came to light.[102][103]TheOpen Source Initiativeand others have contested Meta's use of the termopen-sourceto describe Llama, due to Llama's license containing anacceptable use policythat prohibits use cases including non-U.S. military use.[66]Chinese researchers used an earlier version of Llama to develop tools like ChatBIT, optimized for military intelligence and decision-making, prompting Meta to expand its partnerships with U.S. contractors to ensure the technology could be used strategically for national security.[103]These applications now include logistics, maintenance, and cybersecurity enhancements.[103]\nOpen-source AI democratizes access to cutting-edge tools, lowering entry barriers for individuals and smaller organizations that may lack resources.[104]By making these technologies freely available, open-source AI allows developers to innovate and create AI solutions that might have been otherwise inaccessible due to financial constraints, enabling independent developers and researchers, smaller organizations, and startups to utilize advanced AI models without the financial burden of proprietary software licenses.[104]This affordability encourages innovation in niche or specialized applications, as developers can modify existing models to meet unique needs.[104][105]\nBy sharing code, data, and research findings, open-source AI enables collective problem-solving and innovation.[105]Large-scale collaborations, such as those seen in the development of frameworks like TensorFlow and PyTorch, have accelerated advancements in machine learning (ML) and deep learning.[106]\nThe open-source nature of these platforms also facilitates rapid iteration and improvement, as contributors from across the globe can propose modifications and enhancements to existing tools.[106][25]Beyond enhancements directly within ML and deep learning, this collaboration can lead to faster advancements in the products of AI, as shared knowledge and expertise are pooled together.[25][105]\nThe openness of the development process encourages diverse contributions, making it possible for underrepresented groups to shape the future of AI. This inclusivity not only fosters a more equitable development environment but also helps to address biases that might otherwise be overlooked by larger, profit-driven corporations.[107]With contributions from a broad spectrum of perspectives, open-source AI has the potential to create more fair, accountable, and impactful technologies that better serve global communities.[107]\nOne key benefit of open-source AI is the increased transparency it offers compared to closed-source alternatives.[108]With open-source models, the underlying algorithms and code are accessible for inspection, which promotes accountability and helps developers understand how a model reaches its conclusions.[14]Additionally, open-weight models, such as Llama andStable Diffusion, allow developers to directly access model parameters, potentially facilitating the reduced bias and increased fairness in their applications.[14]This transparency can help create systems with human-readable outputs, or \"explainable AI\", which is a growingly key concern, especially in high-stakes applications such as healthcare, criminal justice, and finance, where the consequences of decisions made by AI systems can be significant (though may also pose certain risks, as mentioned in theConcernssection).[109]\nANatureeditorial suggests medical care could become dependent on AI models that could be taken down at any time, are difficult to evaluate, and may threaten patient privacy.[9]Its authors propose that health-care institutions, academic researchers, clinicians, patients and technology companies worldwide should collaborate to build open-source models for health care of which the underlying code and base models are easily accessible and can be fine-tuned freely with own data sets.[9]\nCurrent open-source models underperform closed-source models on most tasks, but open-source models are improving faster to close the gap.[110]\nOpen-source development of models has been deemed to have theoretical risks. Once a model is public, it cannot be rolled back or updated if serious security issues are detected.[4]For example, Open-source AI may allowbioterrorismgroups likeAum Shinrikyoto removefine-tuningand other safeguards of AI models to get AI to help develop more devastating terrorist schemes.[111]The main barrier to developing real-world terrorist schemes lies in stringent restrictions on necessary materials and equipment.[4]Furthermore, the rapid pace of AI advancement makes it less appealing to use older models, which are more vulnerable to attacks but also less capable.[4]\nIn July 2024, theUnited Statesreleased a presidential report saying it did not find sufficient evidence to restrict revealing model weights.[112]\nThere have been numerous cases of artificial intelligence leading to unintentionally biased products. Some notable examples include AI software predicting higher risk of future crime and recidivism for African-Americans when compared to white individuals, voice recognition models performing worse for non-native speakers, and facial-recognition models performing worse for women and darker-skinned individuals.[113][107][114]\nResearchers have also criticized open-source artificial intelligence for existing security and ethical concerns. An analysis of over 100,000 open-source models onHugging FaceandGitHubusingcode vulnerability scannerslike Bandit, FlawFinder, andSemgrepfound that over 30% of models have high-severity vulnerabilities.[115]Furthermore, closed models typically have fewer safety risks than open-sourced models.[4]The freedom to augment open-source models has led to developers releasing models without ethical guidelines, such asGPT4-Chan.[4]\nThere are numerous systemic problems that may contribute to inequitable and biased AI outcomes, stemming from causes such as biased data, flaws in model creation, and failing to recognize or plan for the possibility of these outcomes.[75]As highlighted in research, poor data quality—such as the underrepresentation of specific demographic groups in datasets—and biases introduced during data curation lead to skewed model outputs.[114]\nA study of open-source AI projects revealed a failure to scrutinize for data quality, with less than 28% of projects including data quality concerns in their documentation.[75]This study also showed a broader concern that developers do not place enough emphasis on the ethical implications of their models, and even when developers do take ethical implications into consideration, these considerations overemphasize certain metrics (behavior of models) and overlook others (data quality and risk-mitigation steps).[75]\nAnother key concern with many AI systems with respect to issues such as safety and bias is their lack of transparency.[114][116]Many open-source AI models operate as \"black boxes\", where their decision-making process is not easily understood, even by their creators.[114][117]This lack of interpretability can hinder accountability, making it difficult to identify why a model made a particular decision or to ensure it operates fairly across diverse groups.[114]\nFurthermore, when AI models are closed-source (proprietary), this can facilitate biased systems slipping through the cracks, as was the case for numerous widely adopted facial recognition systems.[114]These hidden biases can persist when those proprietary systems fail to publicize anything about the decision process which could help reveal those biases, such as confidence intervals for decisions made by AI.[114]Especially for systems like those used in healthcare, being able to see and understand systems' reasoning or getting \"an [accurate] explanation\" of how an answer was obtained is \"crucial for ensuring trust and transparency\".[118]\nEfforts to counteract these challenges have resulted in the creation of structured documentation frameworks that guide the ethical development and deployment of AI:\nAs AI use grows, increasing AI transparency and reducing model biases has become increasingly emphasized as a concern.[109]These frameworks can help empower developers and stakeholders to identify and mitigate bias, fostering fairness and inclusivity in AI systems.[113][109]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Applications_of_artificial_intelligence",
        "title": "Applications of artificial intelligence - Wikipedia",
        "content": "Artificial intelligenceis the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making.Artificial intelligence(AI) has been used in applications throughout industry and academia. Within the field of Artificial Intelligence, there are multiple subfields. The subfield ofMachine learninghas been used for various scientific and commercial purposes[1]includinglanguage translation,image recognition,decision-making,[2][3]credit scoring, ande-commerce. In recent years, there have been massive advancements in the field ofGenerative Artificial Intelligence, which uses generative models to produce text, images, videos or other forms of data[4]. This article describes applications of AI in different sectors.\nIn agriculture, AI has been proposed as a way for farmers to identify areas that need irrigation, fertilization, or pesticide treatments to increase yields, thereby improving efficiency.[5]AI has been used to attempt toclassify livestock pig call emotions,[6]automategreenhouses,[7]detect diseases and pests,[8]and optimize irrigation.[9]\nArtificial intelligence in architectureis the use ofartificial intelligencein automation, design, and planning in the architectural process or in assisting human skills in the field of architecture.[10]Artificial intelligence is thought to potentially lead to and ensue major changes in architecture.[11][12][13]\nAI in architecture has created a way for architects to create things beyond human understanding. AI implementation of machine learning text-to-render technologies, like DALL-E and stable Diffusion, gives power to visualization complex.[14]\nAI allows designers to demonstrate their creativity and even invent new ideas while designing. In future, AI will not replace architects; instead, it will improve the speed of translating ideas sketching.[14]\nAnoptical character readeris used in the extraction of data in business documents like invoices and receipts. It can also be used in business contract documents e.g.employment agreementsto extract critical data like employment terms, delivery terms, termination clauses, etc.[15]\nAI can be used for real-time code completion, chat, and automated test generation. These tools are typically integrated with editors andIDEsasplugins. They differ in functionality, quality, speed, and approach to privacy. Code suggestions could be incorrect, and should be carefully reviewed by software developers before accepted.[citation needed]GitHub Copilotis one example. It was developed byGitHubandOpenAIand is able to autocomplete code in multiple programming languages.[16]\nAI can be used to create other AIs. For example, around November 2017, Google's AutoML project to evolve new neural net topologies createdNASNet, a system optimized forImageNetand POCO F1. NASNet's performance exceeded all previously published performance on ImageNet.[17]\nMachine learning has been used for noise-cancelling inquantum technology,[18]includingquantum sensors.[19]Moreover, there is substantial research and development of using quantum computers with machine learning algorithms. For example, there is a prototype, photonic,quantummemristive deviceforneuromorphic (quantum-)computers(NC)/artificial neural networksand NC-using quantum materials with some variety of potential neuromorphic computing-related applications,[20][21]andquantum machine learningis a field with some variety of applications under development. AI could be used forquantum simulatorswhich may have the application of solving physics andchemistry[22][23]problems as well as forquantum annealersfor training of neural networks for AI applications.[24]There may also be some usefulness in chemistry, e.g. for drug discovery, and in materials science, e.g. for materials optimization/discovery (with possible relevance to quantum materials manufacturing[25][26]).[27][28][29][better source needed]\nAI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered AI. All of the following were originally developed in AI laboratories:[30]\nAnother application of AI is in human resources. AI can screen resumes and rank candidates based on their qualifications, predict candidate success in given roles, and automate repetitive communication tasks via chatbots.[citation needed]\nAI has simplified the recruiting/job search process for both recruiters and job seekers. According toRaj MukherjeefromIndeed, 65% of job searchers search again within 91 days after hire. An AI-powered engine streamlines the complexity of job hunting by assessing information on job skills, salaries, and user tendencies, matching job seekers to the most relevant positions. Machine intelligence calculates appropriate wages and highlights resume information for recruiters using NLP, which extracts relevant words and phrases from text. Another application is an AI resume builder that compiles a CV in 5 minutes.[citation needed]Chatbotsassist website visitors and refine workflows.\nAI underliesavatars(automated online assistants) on web pages.[31]It can reduce operation and training costs.[31]Pypestreamautomated customer service for its mobile application to streamline communication with customers.[32]\nA Google app analyzes language and converts speech into text.[33]The platform can identify angry customers through their language and respond appropriately.[34]Amazon uses a chatbot for customer service that can perform tasks like checking the status of an order, cancelling orders, offering refunds and connecting the customer with a human representative.[35]Generative AI (GenAI), such as ChatGPT, is increasingly used in business to automate tasks and enhance decision-making.[36]\nIn the hospitality industry, AI is used to reduce repetitive tasks, analyze trends, interact with guests, and predict customer needs.[37]AI hotel services come in the form of a chatbot,[38]application, virtual voice assistant and service robots.\nAI elevates teaching, focusing on significant issues like the knowledge nexus and educational equality.[39]The evolution of AI in education and technology should be used to improve human capabilities in relationships where they do not replace humans.UNESCOrecognizes the future of AI in education as an instrument to reach Sustainable Development Goal 4, called \"Inclusive and Equitable Quality Education.\"[40]\nTheWorld Economic Forumalso stresses AI's contribution to students' overall improvement and transforming teaching into a more enjoyable process.[40]\nAI driven tutoring systems (such as Khan Academy, Duolingo and Carnegie Learning) are the forefoot of delivering personalized education.[41]\nThese platforms leverage AI algorithms to analyze individual learning patterns, strengths, and weaknesses, enabling the customization of content and Algorithm to suit each student's pace and style of learning.[41]\nIn educational institutions, AI is increasingly used to automate routine tasks like attendance tracking, grading and marking, which allows educators to devote more time to interactive teaching and direct student engagement.[42]\nFurthermore, AI tools are employed to monitor student progress, analyze learning behaviors, and predict academic challenges, facilitating timely and proactive interventions for students who may be at risk of falling behind.[42]\nDespite the benefits, the integration of AI in education raises significant ethical and privacy concerns, particularly regarding the handling of sensitive student data.[41]\nIt is imperative that AI systems in education are designed and operated with a strong emphasis on transparency, security, and respect for privacy to maintain trust and uphold the integrity of educational practices.[41]\nMuch of the regulation will be influenced by the AI Act, the world's first comprehensive AI law.[43]\nPower electronicsconverters are used inrenewable energy,energy storage,electric vehiclesandhigh-voltage direct currenttransmission.[44]These converters are failure-prone, which can interrupt service and require costly maintenance or catastrophic consequences in mission critical applications.[citation needed]AI can guide the design process for reliable power electronics converters, by calculating exact design parameters that ensure the required lifetime.[45]\nThe U.S. Department of Energy wrote in an April 2024 report that AI may have applications in modeling power grids, reviewing federal permits withlarge language models, predicting levels of renewable energy production, and improving the planning process forelectrical vehiclecharging networks.[46]Other studies have suggested that machine learning can be used for energy consumption prediction and scheduling, e.g. to help withrenewable energy intermittency management(see also:smart gridandclimate change mitigation in the power grid).[47][48][49][50][51]\nAutonomous ships that monitor the ocean, AI-driven satellite data analysis,passive acoustics[52]orremote sensingand other applications ofenvironmental monitoringmake use of machine learning.[53][54][55][56]\nFor example, \"Global Plastic Watch\" is an AI-basedsatellite monitoring-platform for analysis/tracking ofplastic wastesites to helppreventionofplastic pollution– primarilyocean pollution– by helping identify who and where mismanages plastic waste, dumping it into oceans.[57][58]\nMachine learning can be used tospot early-warning signsof disasters and environmental issues, possibly including naturalpandemics,[59][60]earthquakes,[61][62][63]landslides,[64]heavy rainfall,[65]long-term water supply vulnerability,[66]tipping-points ofecosystem collapse,[67]cyanobacterial bloomoutbreaks,[68]and droughts.[69][70][71]\nAI for Goodis a platform launched in 2017 by theInternational Telecommunication Union(ITU) agency of the United Nations (UN). The goal of the platform is to use AI to help achieve the UN'sSustainable Development Goals.[citation needed]\nTheUniversity of Southern Californialaunched the Center for Artificial Intelligence in Society, with the goal of using AI to address problems such as homelessness.Stanfordresearchers useAIto analyze satellite images to identify high poverty areas.[72]\nAI applications analyze media content such as movies, TV programs, advertisement videos oruser-generated content. The solutions often involvecomputer vision.\nTypical scenarios include the analysis of images usingobject recognitionor face recognition techniques, or theanalysis of videofor scene recognizing scenes, objects or faces. AI-based media analysis can facilitate media search, the creation of descriptive keywords for content, content policy monitoring (such as verifying the suitability of content for a particular TV viewing time),speech to textfor archival or other purposes, and the detection of logos, products or celebrity faces for ad placement.\nDeep-fakescan be used for comedic purposes but are better known forfake newsand hoaxes.\nDeepfakes can portray individuals in harmful or compromising situations, causing significant reputational damage and emotional distress, especially when the content is defamatory or violates personal ethics. While defamation and false light laws offer some recourse, their focus on false statements rather than fabricated images or videos often leaves victims with limited legal protection and a challenging burden of proof.[85]\nIn January 2016,[86]theHorizon 2020program financed the InVID Project[87][88]to help journalists and researchers detect fake documents, made available as browser plugins.[89][90]\nIn June 2016, the visual computing group of theTechnical University of Munichand fromStanford Universitydeveloped Face2Face,[91]a program that animates photographs of faces, mimicking the facial expressions of another person. The technology has been demonstrated animating the faces of people includingBarack ObamaandVladimir Putin. Other methods have been demonstrated based ondeep neural networks, from which the namedeep fakewas taken.\nIn September 2018, U.S. SenatorMark Warnerproposed to penalizesocial mediacompanies that allow sharing of deep-fake documents on their platforms.[92]\nIn 2018, Darius Afchar and Vincent Nozick found a way to detect faked content by analyzing the mesoscopic properties of video frames.[93]DARPAgave 68 million dollars to work on deep-fake detection.[93]\nAudio deepfakes[94][95]and AI software capable of detecting deep-fakes and cloning human voices have been developed.[96][97]\nRespeecheris a program that enables one person to speak with the voice of another.\nAI algorithms have been used to detect deepfake videos.[98][99]\nArtificial intelligenceis also starting to be used in video production, with tools and software being developed that utilize generative AI in order to create new video, or alter existing video. Some of the major tools that are being used in these processes currently are DALL-E, Mid-journey, and Runway.[100]Way mark Studios utilized the tools offered by bothDALL-EandMid-journeyto create a fully AI generated film calledThe Frostin the summer of 2023.[100]Way mark Studios is experimenting with using these AI tools to generate advertisements and commercials for companies in mere seconds.[100]Yves Bergquist, a director of the AI &Neurosciencein Media Project at USC's Entertainment Technology Center, says post production crews in Hollywood are already using generative AI, and predicts that in the future more companies will embrace this new technology.[101]\nAI has been used to compose music of various genres.\nDavid Copecreated an AI calledEmily Howellthat managed to become well known in the field of algorithmic computer music.[102]The algorithm behind Emily Howell is registered as a US patent.[103]\nIn 2012, AIIamuscreated the first complete classical album.[104]\nAIVA(Artificial Intelligence Virtual Artist), composes symphonic music, mainlyclassical musicforfilm scores.[105]It achieved a world first by becoming the first virtual composer to be recognized by a musicalprofessional association.[106]\nMelomicscreates computer-generated music for stress and pain relief.[107]\nAt Sony CSL Research Laboratory, the Flow Machines software creates pop songs by learning music styles from a huge database of songs. It can compose in multiple styles.\nThe Watson Beat usesreinforcement learninganddeep belief networksto compose music on a simple seed input melody and a select style. The software was open sourced[108]and musicians such asTaryn Southern[109]collaborated with the project to create music.\nSouth Korean singer, Hayeon's, debut song, \"Eyes on You\" was composed using AI which was supervised by real composers, including NUVO.[110]\nNarrative Sciencesellscomputer-generated newsand reports. It summarizes sporting events based on statistical data from the game. It also creates financial reports and real estate analyses.[111]Automated Insightsgenerates personalized recaps and previews forYahoo SportsFantasy Football.[112]\nYseop, uses AI to turn structured data into natural language comments and recommendations.Yseopwrites financial reports, executive summaries, personalized sales or marketing documents and more in multiple languages, including English, Spanish, French, and German.[113]\nTALESPIN made up stories similar to thefables of Aesop. The program started with a set of characters who wanted to achieve certain goals. The story narrated their attempts to satisfy these goals.[citation needed]Mark Riedl and Vadim Bulitko asserted that the essence of storytelling was experience management, or \"how to balance the need for a coherent story progression with user agency, which is often at odds\".[114]\nWhile AI storytelling focuses on story generation (character and plot), story communication also received attention. In 2002, researchers developed an architectural framework for narrative prose generation. They faithfully reproduced text variety and complexity on stories such asLittle Red Riding Hood.[115]In 2016, a Japanese AI co-wrote a short story and almost won a literary prize.[116]\nSouth Korean company Hanteo Global uses a journalism bot to write articles.[117]\nLiterary authors are also exploring uses of AI. An example isDavid Jhave Johnston's workReRites(2017–2019), where the poet created a daily rite of editing the poetic output of a neural network to create a series of performances and publications.\nIn 2010, artificial intelligence usedbaseballstatistics to automatically generate news articles. This was launched byThe Big Ten Networkusing software fromNarrative Science.[118]\nAfter being unable to cover everyMinor League Baseballgame with a large team,Associated Presscollaborated withAutomated Insightsin 2016 to create game recaps that were automated by artificial intelligence.[119]\nUOL in Brazil expanded the use of AI in its writing. Rather than just generating news stories, they programmed the AI to include commonly searched words onGoogle.[119]\nEl Pais, a Spanish news site that covers many things including sports, allows users to make comments on each news article. They use thePerspective APIto moderate these comments and if the software deems a comment to contain toxic language, the commenter must modify it in order to publish it.[119]\nA local Dutch media group used AI to create automatic coverage of amateur soccer, set to cover 60,000 games in just a single season. NDC partnered with United Robots to create this algorithm and cover what would have never been possible before without an extremely large team.[119]\nLede AI has been used in 2023 to take scores fromhigh school footballgames to generate stories automatically for the local newspaper. This was met with significant criticism from readers for the very robotic diction that was published. With some descriptions of games being a \"close encounter of the athletic kind,\" readers were not pleased and let the publishing company,Gannett, know on social media. Gannett has since halted their used of Lede AI until they come up with a solution for what they call an experiment.[120]\nArtificial intelligenceis used inWikimedia projectsfor the purpose of developing those projects.[121][122]Human andbotinteraction in Wikimedia projects is routine and iterative.[123]\nVarious articles onWikipediahave been created entirely with or with the help ofartificial intelligence. AI-generated content can be detrimental to Wikipedia when unreliable or containing fake citations.\nMillions of its articles have been edited by bots[124]which however are usually not artificial intelligence software. Many AI platforms use Wikipedia data,[125]mainly for training machine learning applications. There is research and development of various artificial intelligence applications for Wikipedia such as for identifying outdated sentences,[126]detecting covert vandalism[127]or recommending articles and tasks to new editors.\nMachine translation(seeabove)has also be used for translating Wikipedia articles and could play a larger role in creating, updating, expanding, and generally improving articles in the future. A content translation tool allows editors of some Wikipedias to more easily translate articles across several select languages.[128][129]\nIn video games, AI is routinely used to generate behavior innon-player characters(NPCs). In addition, AI is used forpathfinding. Some researchers consider NPC AI in games to be a \"solved problem\" for most production tasks.[who?]Games with less typical AI include the AI director ofLeft 4 Dead(2008) and the neuroevolutionary training of platoons inSupreme Commander 2(2010).[130][131]AI is also used inAlien Isolation(2014) as a way to control the actions the Alien will perform next.[132]\nGames have been a major application[relevant?]of AI's capabilities since the 1950s. In the 21st century, AIs have beaten human players in many games, includingchess(Deep Blue),Jeopardy!(Watson),[133]Go(AlphaGo),[134][135][136][137][138][139][140]poker(Pluribus[141]andCepheus),[142]E-sports(StarCraft),[143][144]andgeneral game playing(AlphaZero[145][146][147]andMuZero).[148][149][150][151]\nKuki AI is a set ofchatbotsand other apps which were designed for entertainment and as a marketing tool.[152][153]Character.aiis another example of a chatbot being used for recreation.[citation needed]\nKinect, which provides a 3D body–motion interface for theXbox 360and theXbox One, uses algorithms that emerged from AI research.[154][which?]\nAI has been used to produce visual art. The first AI art program, calledAARON, was developed byHarold Cohenin 1968[155]with the goal of being able to code the act of drawing. It started by creating simple black and white drawings, and later to painting using special brushes and dyes that were chosen by the program itself without mediation from Cohen.[156]\nAI platforms such asDALL-E,[157]Stable Diffusion,[157]Imagen,[158]andMidjourney[159]have been used for generating visual images from inputs such as text or other images.[160]Some AI tools allow users to input images and output changed versions of that image, such as to display an object or product in different environments. AI image models can also attempt to replicate the specific styles of artists, and can add visual complexity to rough sketches.\nSince their design in 2014,generative adversarial networks(GANs) have been used by AI artists. GAN computer programming, generates technical images through machine learning frameworks that surpass the need for human operators.[155]Examples of GAN programs that generate art includeArtbreederandDeepDream.\nIn addition to the creation of original art, research methods that utilize AI have been generated to quantitatively analyze digital art collections. Although the main goal of the large-scale digitization of artwork in the past few decades was to allow for accessibility and exploration of these collections, the use of AI in analyzing them has brought about new research perspectives.[161]Two computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art.[162]While distant viewing includes the analysis of large collections, close reading involves one piece of artwork.\nAI has been in use since the early 2000s, most notably by a system designed by Pixar called \"Genesis\".[163]It was designed to learn algorithms and create 3D models for its characters and props. Notable movies that used this technology included Up and The Good Dinosaur.[164]AI has been used less ceremoniously in recent years. In 2023, it was revealed Netflix of Japan was using AI to generate background images for their upcoming show to be met with backlash online.[165]In recent years, motion capture became an easily accessible form of AI animation. For example, Move AI is a program built to capture any human movement and reanimate it in its animation program using learning AI.[166]\nFinancial institutionshave long usedartificial neural networksystems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI inbankingbegan in 1987 whenSecurity Pacific National Banklaunched a fraud prevention task-force to counter the unauthorized use of debit cards.[167]\nBanks use AI to organize operations for bookkeeping, investing in stocks, and managing properties. AI can adapt to changes during non-business hours.[168]AI is used to combat fraudand financial crimes by monitoring behavioral patterns for anyabnormal changes or anomalies.[169][170][171]\nThe use of AI in applications such as online trading and decision-making has changed major economic theories.[172]For example, AI-based buying and selling platforms estimate personalized demand and supply curves, thus enabling individualizedpricing. AI systems reduceinformation asymmetryin the market and thusmake markets more efficient.[173]The application of artificial intelligence in the financial industry can alleviate the financing constraints of non-state-owned enterprises, especially for smaller and more innovative enterprises.[174]\nAlgorithmic tradinginvolves using AI systems to make trading decisions at speeds of magnitude greater than any human is capable of, making millions of trades in a day without human intervention. Suchhigh-frequency tradingrepresents a fast-growing sector. Many banks, funds, and proprietary trading firms now have AI-managed portfolios.Automated trading systemsare typically used by large institutional investors but include smaller firms trading with their own AI systems.[175]\nLarge financial institutions use AI to assist with their investment practices.[176]BlackRock's AI engine,Aladdin, is used both within the company and by clients to help with investment decisions. Its functions include the use ofnatural language processingto analyze text such as news, broker reports, and social media feeds. It then gauges the sentiment on the companies mentioned and assigns a score. Banks such asUBSandDeutsche Bankuse SQREEM (Sequential Quantum Reduction and Extraction Model) to mine data to develop consumer profiles and match them withwealth managementproducts.[177]\nOnline lenderUpstartuses machine learning forunderwriting.[178]\nZestFinance's Zest Automated Machine Learning (ZAML) platform is used for credit underwriting.[179]This platform uses machine learning to analyze data, including purchase transactions and how a customer fills out a form, to score borrowers. The platform is handy for assigning credit scores to those with limited credit histories.[180]\nAI makes continuous auditing possible. Potential benefits include reducing audit risk, increasing the level of assurance, and reducing audit duration.[181][quantify]\nContinuous auditing with AI allows real-time monitoring and reporting of financial activities and provides businesses with timely insights that can lead to quick decision-making.[182]\nAI software, such as LaundroGraph which uses contemporary suboptimal datasets, could be used foranti–money laundering(AML).[183][184]Anti–money laundering\nIn recent years, thedebt collectionindustry has begun to adopt AI-driven \"agents\" to automate routine outreach and negotiation tasks. Platforms use natural-language processing and machine learning to interact with consumers.\nProponents claim these systems can handle high volumes of standard enquiries, freeing human collectors to focus on more complex cases, while delivering more consistent, 24/7 service. However, critics warn of potential compliance pitfalls, such as the risk of unintended bias in algorithmic decision-making.[185]\nIn the 1980s, AI started to become prominent in finance asexpert systemswere commercialized. For example, Dupont created 100 expert systems, which helped them to save almost $10 million per year.[186]One of the first systems was the Pro-trader expert system that predicted the 87-point drop in theDow Jones Industrial Averagein 1986. \"The major junctions of the system were to monitor premiums in the market, determine the optimum investment strategy, execute transactions when appropriate and modify the knowledge base through a learning mechanism.\"[187]\nOne of the first expert systems to help with financial plans was PlanPowerm and Client Profiling System, created by Applied Expert Systems (APEX). It was launched in 1986. It helped create personal financial plans for people.[188]\nIn the 1990s, AI was applied tofraud detection. In 1993, FinCEN Artificial Intelligence System (FAIS) was launched. It was able to review over 200,000 transactions per week, and over two years, it helped identify 400 potential cases ofmoney launderingequal to $1 billion.[189]These expert systems were later replaced by machine learning systems.[190]\nOutside finance, the late 1980s and early 1990s also saw expert systems used in technical and environmental domains. For example, researchers built a fishway design advisor to recommend fish passage structures under varying hydraulic and biological conditions using the VP-Expert shell.[191]Transportation researchers applied the same shell to balance airport capacity with noise-mitigation plans.[192]In agriculture, a potato insect expert system (PIES) supported pest management decisions for Colorado potato beetle.[193]The U.S. Environmental Protection Agency’s CORMIX system for modeling pollutant discharges combined rules with Fortran hydrodynamic models.[194]\nAI can enhance entrepreneurial activity, and AI is one of the most dynamic areas for start-ups, with significant venture capital flowing into AI.[195]\nIn theEuropean Union, theArtificial Intelligence Act(Regulation (EU) 2024/1689) classifies several finance‑sector uses of AI as \"high‑risk\", including systems used to evaluate the creditworthiness of natural persons or to establish a credit score and AI used for risk assessment and pricing in life or health insurance.[196][197][198]These systems must meet requirements on risk management, data governance, technical documentation and logging, transparency, and human oversight.[197]The Act's obligations are phased in: prohibitions and AI‑literacy rules apply from 2 February 2025, governance and most GPAI duties from 2 August 2025, the bulk of obligations from 2 August 2026, and certain safety‑component high‑risk obligations from 2 August 2027.[198]\nAI in healthcare is often used for classification, to evaluate aCT scanorelectrocardiogramor to identify high-risk patients for population health. AI is helping with the high-cost problem of dosing. One study suggested that AI could save $16 billion. In 2016, a study reported that an AI-derived formula derived the proper dose of immunosuppressant drugs to give to transplant patients.[199]Current research has indicated that non-cardiac vascular illnesses are also being treated with artificial intelligence (AI). For certain disorders, AI algorithms can aid in diagnosis, recommended treatments, outcome prediction, and patient progress tracking. As AI technology advances, it is anticipated that it will become more significant in the healthcare industry.[200]\nThe early detection of diseases like cancer is made possible by AI algorithms, which diagnose diseases by analyzing complex sets of medical data. For example, the IBM Watson system might be used to comb through massive data such as medical records and clinical trials to help diagnose a problem.[201]Microsoft's AI project Hanover helps doctors choosecancer treatmentsfrom among the more than 800 medicines and vaccines.[202][203]Its goal is to memorize all the relevant papers to predict which (combinations of) drugs will be most effective for each patient.Myeloid leukemiais one target. Another study reported on an AI that was as good as doctors in identifying skin cancers.[204]Another project monitors multiple high-risk patients by asking each patient questions based on data acquired from doctor/patient interactions.[205]In one study done withtransfer learning, an AI diagnosed eye conditions similar to anophthalmologistand recommended treatment referrals.[206]\nAnother study demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel judged better than a surgeon.[207]\nArtificial neural networksare used asclinical decision support systemsfor medical diagnosis,[208]such as inconcept processingtechnology inEMRsoftware.\nOther healthcare tasks thought suitable for an AI that are in development include:\nAI-enabledchatbotsdecrease the need for humans to perform basic call center tasks.[221]\nMachine learning insentiment analysiscan spot fatigue in order to preventoverwork.[221]Similarly,decision support systemscan preventindustrial disastersand makedisaster responsemore efficient.[222]For manual workers inmaterial handling,predictive analyticsmay be used to reducemusculoskeletal injury.[223]Data collected fromwearable sensorscan improveworkplace health surveillance,risk assessment, and research.[222][how?]\nAI can auto-codeworkers' compensationclaims.[224][225]AI-enabledvirtual realitysystems can enhance safety training for hazard recognition.[222]AI can more efficiently detect accidentnear misses, which are important in reducing accident rates, but are often underreported.[226]\nMachine learning has been used fordrug design,[51]drug discoveryand development,drug repurposing, improving pharmaceutical productivity, and clinical trials.[227]\nComputer-planned syntheses via computational reaction networks, described as a platform that combines \"computational synthesis with AI algorithms to predict molecular properties\",[228]has been used in drug-syntheses, and developing routes forrecycling200 industrialwaste chemicalsinto important drugs and agrochemicals (chemical synthesis design).[229]It has also been used to explore theorigins of life on Earth.[230]\nDeep learning has been used with databases for the development of a 46-day process to design, synthesize and test a drug which inhibits enzymes of a particular gene,DDR1. DDR1 is involved in cancers and fibrosis which is one reason for the high-quality datasets that enabled these results.[231]\nThe AI programAlphaFold 2can determine the 3D structure of a (folded) protein in hours rather than the months required by earlier automated approaches and was used to provide the likely structures of all proteins in the human body and essentially all proteins known to science (more than 200 million).[232][233][234][235]\nSpeech translation technology attempts to convert one language's spoken words into another language. This potentially reduces language barriers in global commerce and cross-cultural exchange, enabling speakers of various languages to communicate with one another.[236]\nAI has been used to automatically translate spoken language and textual content in products such asMicrosoft Translator,Google Translate, andDeepL Translator.[237]Additionally, research and development are in progress to decode and conduct animal communication.[6][238]\nMeaning is conveyed not only by text, but also through usage and context (seesemanticsandpragmatics). As a result, the two primary categorization approaches for machine translations arestatistical machine translation(SMT) andneural machine translations(NMTs). The old method of performing translation was to use statistical methodology to forecast the best probable output with specific algorithms. However, with NMT, the approach employs dynamic algorithms to achieve better translations based on context.[239]\nAIfacial recognition systemsare used formass surveillance, notably in China.[240][241]In 2019,Bengaluru, Indiadeployed AI-managed traffic signals. This system uses cameras to monitor traffic density and adjust signal timing based on the interval needed to clear traffic.[242]\nAI is a mainstay of law-related professions. Algorithms and machine learning do some tasks previously done by entry-level lawyers.[243]While its use is common, it is not expected to replace most work done by lawyers in the near future.[244]\nTheelectronic discoveryindustry uses machine learning to reduce manual searching.[245]\nLaw enforcement has begun usingfacial recognition systems(FRS) to identify suspects from visual data. FRS results have proven to be more accurate when compared to eyewitness results. Furthermore, FRS has shown to have much a better ability to identify individuals when video clarity and visibility are low in comparison to human participants.[246]\nCOMPASis a commercial system used byU.S. courtsto assess the likelihood ofrecidivism.[247]\nOne concern relates toalgorithmic bias, AI programs may become biased after processing data that exhibits bias.[248]ProPublicaclaims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than that of white defendants.[247]\nIn 2019, the city ofHangzhou, China established a pilot program artificial intelligence-based Internet Court to adjudicate disputes related to ecommerce and internet-relatedintellectual propertyclaims.[249]: 124Parties appear before the court via videoconference and AI evaluates the evidence presented and applies relevant legal standards.[249]: 124\nArtificial intelligence has been combined with digitalspectrometryby IdeaCuria Inc.,[250][251]enable applications such as at-home water quality monitoring.\nIn the 1990s, early artificial intelligence tools controlledTamagotchisandGiga Pets, theInternet, and the first widely released robot,Furby.Aibowas adomestic robotin the form of a robotic dog with intelligent features andautonomy.\nMattel created an assortment of AI-enabled toys that \"understand\" conversations, give intelligent responses, and learn.[252]\nOil and gascompanies have used artificial intelligence tools to automate functions, foresee equipment issues, and increase oil and gas output.[253][254]\nVarious countries are deploying AI military applications.[255]The main applications enhancecommand and control, communications, sensors, integration and interoperability.[citation needed]Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous andautonomous vehicles.[255]AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions,target acquisition, coordination and deconfliction of distributedJoint Firesbetween networked combat vehicles involving manned and unmanned teams.[citation needed]\nAI has been used in military operations in Iraq, Syria, Israel and Ukraine.[255][256][257][258]\nMachine learning has been used forrecommendation systemsin determining which posts should show up insocial media feeds.[259][260]Various types ofsocial media analysisalso make use of machine learning[261][262]and there is research into its use for (semi-)automated tagging/enhancement/correction ofonline misinformationand relatedfilter bubbles.[263][264][265]\nAI has been used to customize shopping options and personalize offers.[266]Online gamblingcompanies have used AI for targeting gamblers.[267]\nIntelligent personal assistantsuse AI to understand many natural language requests in other ways than rudimentary commands. Common examples are Apple's Siri, Amazon's Alexa, and a more recent AI, ChatGPT by OpenAI.[268]\nBing Chathas used artificial intelligence as part of itssearch engine.[269]\nMachine learning can be used to combat spam, scams, andphishing. It can scrutinize the contents of spam and phishing attacks to attempt to identify malicious elements.[270]Some models built via machine learning algorithms have over 90% accuracy in distinguishing between spam and legitimate emails.[271]These models can be refined using new data and evolving spam tactics. Machine learning also analyzes traits such as sender behavior, email header information, and attachment types, potentially enhancing spam detection.[272]\nAI has been used infacial recognition systems. Some examples are Apple'sFace IDand Android'sFace Unlock, which are used to secure mobile devices.[273]\nImage labeling has been used byGoogle Image Labelerto detect products in photos and to allow people to search based on a photo. Image labeling has also been demonstrated to generate speech to describe images to blind people.[237]Facebook'sDeepFaceidentifies human faces in digital images.[citation needed]\nIn April 2024, theScientific Advice Mechanismto theEuropean Commissionpublished advice[274]including a comprehensive evidence review of the opportunities and challenges posed by artificial intelligence in scientific research.\nAs benefits, the evidence review[275]highlighted:\nAs challenges:\nMachine learning can help to restore and attribute ancient texts.[276]It can help to index texts for example to enable better and easier searching and classification of fragments.[277]\nArtificial intelligence can also be used to investigate genomes to uncovergenetic history, such asinterbreeding between archaic and modern humansby which for example the past existence of aghost population, notNeanderthalorDenisovan, was inferred.[278]\nIt can also be used for \"non-invasive and non-destructive access to internal structures of archaeological remains\".[279]\nAdeep learningsystem was reported to learn intuitive physics from visual data (of virtual 3D environments) based on anunpublishedapproach inspired by studies of visual cognition in infants.[280][281]Other researchers have developed a machine learning algorithm that could discover sets of basic variables of various physical systems and predict the systems' future dynamics from video recordings of their behavior.[282][283]In the future, it may be possible that such can be used to automate the discovery of physical laws of complex systems.[282]\nAI could be used for materials optimization and discovery such as the discovery of stable materials and the prediction of their crystal structure.[284][25][26]\nIn November 2023, researchers atGoogle DeepMindandLawrence Berkeley National Laboratoryannounced that they had developed an AI system known as GNoME. This system has contributed tomaterials scienceby discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganiccrystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through theMaterials Projectdatabase, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.[285][286][287]\nMachine learning is used in diverse types ofreverse engineering. For example, machine learning has been used to reverse engineer a composite material part, enabling unauthorized production of high quality parts,[288]and for quickly understanding the behavior ofmalware.[289][290][291]It can be used to reverse engineer artificial intelligence models.[292]It can also design components by engaging in a type of reverse engineering of not-yet existent virtual components such as inverse molecular design for particular desired functionality[293]orprotein designfor pre-specifiedfunctional sites.[294][295]Biological network reverse engineering could model interactions in a human understandable way, e.g. bas on time series data of gene expression levels.[296]\nArtificial intelligence is used inastronomyto analyze increasing amounts of available data[297][298]and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights\" for example for discoveringexoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects ingravitational wave astronomy.[299]It could also be used for activities in space such asspace exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance,[300]and more autonomous operation.[301][302][56][298]\nIn thesearch for extraterrestrial intelligence(SETI), machine learning has been used in attempts to identify artificially generatedelectromagnetic wavesin available data[303][304]– such as real-time observations[305]– and othertechnosignatures, e.g. viaanomaly detection.[306]Inufology, the SkyCAM-5 project headed by Prof. Hakan Kayal[307]and theGalileo Projectheaded byAvi Loebuse machine learning to attempt to detect and classify types of UFOs.[308][309][310][311][312]The Galileo Project also seeks to detect two further types of potential extraterrestrial technological signatures with the use of AI:'Oumuamua-likeinterstellar objects, and non-manmade artificial satellites.[313][314]\nMachine learning can also be used to produce datasets of spectral signatures of molecules that may be involved in the atmospheric production or consumption of particular chemicals – such asphosphine possibly detected on Venus– which could prevent miss assignments and, if accuracy is improved, be used in future detections and identifications of molecules on other planets.[315]\nThere is research about which types of computer-aided chemistry would benefit from machine learning.[316]A deep learning AI-based process has been developed that usesgenome databasestodesign novel proteinsbased onevolutionary algorithms.[317][318]Machine learning has also been used for protein design with pre-specifiedfunctional sites,[294][295]predicting molecular properties, and exploring large chemical/reaction spaces.[319]\nUsingdrug discoveryAI algorithms, researchers generated 40,000 potential chemical weapon candidates, helping in theregulation of such chemicalsto prevent synthesizing them for real harm.[320][321][322]\nThere are various types of applications for machine learning in decoding human biology, such as helping to mapgene expressionpatterns to functional activation patterns[323]or identifying functionalDNA motifs.[324]It is widely used in genetic research.[325]There also is some use of machine learning insynthetic biology,[326][327]disease biology,[327]nanotechnology (e.g. nanostructured materials andbionanotechnology),[328][329]andmaterials science.[330][331][332]\nEmbodied AIsees the body as a contributor to states of the mind, than a mere follower of (algorithmic) instructions. Insights from embodied cognition have allowed researchers to build more dynamic robots with more fluid and more expressive motions facilitating better performance in complex scenarios.[333][334][335]Embodied AI gave birth to situated robotic perspectives, which unlike traditional robots, perform better in complex and dynamic environments that create unpredictable situations for robots most of the time.[336][337]\nThere are alsoprototype robot scientists, including robot-embodied ones like the twoRobot Scientists, Adam and Eve, which show a form of \"machine learning\" not commonly associated with the term.[338][339]\nWhole brain emulation ormind uploadingis a speculative process in which abrain scanis used to completely emulate the mental state of the individual in a digital computer.[340][341]Research is being conducted inneuroscienceonbrain–computer interfaces, and information extraction from dynamically functioning brains.[342]Many of the tools and ideas needed to achieve mind uploading already exist or are under active development, however others are speculative.\nAn alternative or additive approach to scanning, are types of reverse engineering of the brain.[343][344]Another approach isSeed AI, which would not be based on existing brains.[345]\nSimilarly, there is research and development of biological \"wetware computers\" that can learn (e.g. for use asbiosensors) and/or implantation into an organism's body (e.g. for use to control prosthetics).[346][347][348]Polymer-based artificial neurons operate directly in biological environments and define biohybrid neurons made of artificial and living components.[349][350]\nAdditionally,biological computers, even if both artificial and highly intelligent, are typically distinguishable from synthetic, predominantly silicon-based, computers.  The two technologies could, however, be combined and used for the design of either. Moreover, many tasks may be poorly carried out by AI even if it uses algorithms that are transparent, understood, bias-free, apparently effective and goal-aligned in addition to having trained data sets that are sufficiently large andcleansed.  This may occur, for instance, when the underlying data, available metrics,valuesor training methods are incorrect, flawed or used inappropriately.Computer-aidedis a phrase used to describe human activities that make use of computing as tool in more comprehensive activities and systems such as AI for narrow tasks or making use of such without substantially relying on its results (see also:human-in-the-loop).[citation needed]One study described the biological component as a limitation of AI stating that \"as long as the biological system cannot be understood, formalized, and imitated, we will not be able to develop technologies that can mimic it\" and that, even if it were understood, this does not necessarily mean there will be \"a technological solution to imitate natural intelligence\".[351]Technologies that integrate biology and AI includebiorobotics.\nCyber securitycompanies are adoptingneural networks,machine learning, andnatural language processingto improve their systems.[352]\nApplications of AI in cyber security include:\nAI in transport is expected to provide safe, efficient, and reliable transportation while minimizing the impact on the environment and communities. The major development challenge is the complexity of transportation systems that involves independent components and parties, with potentially conflicting objectives.[358]Transportation's complexity means that in most cases, training an AI in a real-world driving environment is impractical, and is achieved through simulator-based testing.[359]\nAI underpins self-driving vehicles. Companies involved with AI includeTesla,Waymo, andGeneral Motors. AI-based systems control functions such as braking, lane changing, collision prevention, navigation and mapping.[360]AI-basedfuzzy logiccontrollers operategearboxes. AI-baseddriver-assist systemsinclude features such asself-parkingandadaptive cruise control.[citation needed]Some autonomous vehicles do not allow human drivers (they have no steering wheels or pedals).[361][362]\nThere are prototypes of autonomous automotive public transport vehicles such asautonomous rail transportinoperation,[363][364][365]electric mini-buses,[366][367][368]and autonomous delivery vehicles,[369][370][362]sometimes includingdelivery robots.[371][372]\nAutonomous trucks are in the testing phase. The UK government passed legislation to begin testing of autonomous truck platoons in 2018.[373]A group of autonomous trucks follow closely behind each other. German corporationDaimleris testing itsFreightliner Inspiration.[374]\nAutonomous vehicles require accurate maps to be able to navigate between destinations.[375]\nAI has been used to optimize traffic management, which reduces wait times, energy use, and emissions by as much as 25 percent.[376]A technology forsmart traffic lightswas developed in 2009 byCarnegie Mellon Universityresearch professor Stephen Smith, and deployed inPittsburgh, Pennsylvania. His company then installed theSurtractechnology in 22 cities. It cost about $20,000 per intersection to install, drive time reduced by 25%, and traffic jam waiting time reduced by 40% at the intersections it was installed in.[377]\nTheRoyal Australian AirForce (RAAF)Air Operations Division(AOD) uses AI forexpert systems. AIs operate as surrogate operators for combat and training simulators, mission management aids, support systems for tactical decision making, and post processing of the simulator data into symbolic summaries.[378]\nAircraft simulators use AI for training aviators. Flight conditions can be simulated that allow pilots to make mistakes without risking themselves or expensive aircraft. Air combat can also be simulated.\nAI can also be used to operate planes analogously to their control of ground vehicles. Autonomous drones can fly independently or inswarms.[379]\nAOD uses the Interactive Fault Diagnosis and Isolation System, or IFDIS, which is a rule-based expert system using information fromTF-30documents and expert advice from mechanics that work on the TF-30. This system was designed to be used for the development of the TF-30 for theF-111C. The system replaced specialized workers. The system allowed regular workers to communicate with the system and avoid mistakes, miscalculations, or having to speak to one of the specialized workers.\nSpeech recognitionallows traffic controllers to give verbal directions to drones.\nArtificial intelligence supported design of aircraft,[380]or AIDA, is used to help designers in the process of creating conceptual designs of aircraft. This program allows the designers to focus more on the design itself and less on the design process. The software also allows the user to focus less on the software tools. The AIDA uses rule-based systems to compute its data. This is a diagram of the arrangement of the AIDA modules. Although simple, the program is proving effective.\nIn 2003 aDryden Flight Research Centerproject created software that could enable a damaged aircraft to continue flight until a safe landing can be achieved.[381]The software compensated for damaged components by relying on the remaining undamaged components.[382]\nThe 2016 Intelligent Autopilot System combinedapprenticeship learningand behavioral cloning whereby the autopilot observed low-level actions required to maneuver the airplane and high-level strategy used to apply those actions.[383]\nNeural networksare used bysituational awarenesssystems in ships and boats.[384]There also areautonomous boats.\nMany telecommunications companies make use ofheuristic searchto manage their workforces. For example,BT Groupdeployed heuristic search[385]in an application that schedules 20,000 engineers. Machine learning is also used forspeech recognition(SR), including of voice-controlled devices, and SR-related transcription, including of videos.[386][387]\nThe following are applications of artificial intelligence (AI) organized by category:"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Machine_learning_in_bioinformatics",
        "title": "Machine learning in bioinformatics - Wikipedia",
        "content": "Machine learning in bioinformaticsis the application ofmachine learningalgorithms tobioinformatics,[1]includinggenomics,proteomics,microarrays,systems biology,evolution, andtext mining.[2][3]\nPrior to the emergence of machine learning, bioinformatics algorithms had to be programmed by hand; for problems such asprotein structure prediction, this proved difficult.[4]Machine learning techniques such asdeep learningcanlearn featuresof data sets rather than requiring the programmer to define them individually. The algorithm can further learn how to combine low-levelfeaturesinto more abstract features, and so on. This multi-layered approach allows such systems to make sophisticated predictions when appropriately trained. These methods contrast with othercomputational biologyapproaches which, while exploiting existing datasets, do not allow the data to be interpreted and analyzed in unanticipated ways.\nMachine learning algorithms in bioinformatics can be used for prediction, classification, and feature selection. Methods to achieve this task are varied and span many disciplines; most well known among them are machine learning and statistics. Classification and prediction tasks aim at building models that describe and distinguish classes or concepts for future prediction. The differences between them are the following:\nDue to the exponential growth of information technologies and applicable models, including artificial intelligence and data mining, in addition to the access ever-more comprehensive data sets, new and better information analysis techniques have been created, based on their ability to learn. Such models allow reach beyond description and provide insights in the form of testable models.\nArtificial neural networksin bioinformatics have been used for:[5]\nThe way that features, often vectors in a many-dimensional space, are extracted from the domain data is an important component of learning systems.[6]In genomics, a typical representation of a sequence is a vector ofk-mersfrequencies, which is a vector of dimension4k{\\displaystyle 4^{k}}whose entries count the appearance of each subsequence of lengthk{\\displaystyle k}in a given sequence. Since for a value as small ask=12{\\displaystyle k=12}the dimensionality of these vectors is huge (e.g. in this case the dimension is412≈16×106{\\displaystyle 4^{12}\\approx 16\\times 10^{6}}), techniques such asprincipal component analysisare used to project the data to a lower dimensional space, thus selecting a smaller set of features from the sequences.[6][7]\nIn this type of machine learning task, the output is a discrete variable. One example of this type of task in bioinformatics is labeling new genomic data (such as genomes of unculturable bacteria) based on a model of already labeled data.[6]\nHidden Markov models(HMMs) are a class ofstatistical modelsfor sequential data (often related to systems evolving over time). An HMM is composed of two mathematical objects: an observed state‐dependent processX1,X2,…,XM{\\displaystyle X_{1},X_{2},\\ldots ,X_{M}}, and an unobserved (hidden) state processS1,S2,…,ST{\\displaystyle S_{1},S_{2},\\ldots ,S_{T}}. In an HMM, the state process is not directly observed – it is a 'hidden' (or 'latent') variable – but observations are made of a state‐dependent process (or observation process) that is driven by the underlying state process (and which can thus be regarded as a noisy measurement of the system states of interest).[8]HMMs can be formulated in continuous time.[9][10]\nHMMs can be used to profile and convert a multiple sequence alignment into a position-specific scoring system suitable for searching databases for homologous sequences remotely.[11]Additionally, ecological phenomena can be described by HMMs.[12]\nConvolutional neural networks(CNN) are a class ofdeep neural networkwhose architecture is based on shared weights of convolution kernels or filters that slide along input features, providing translation-equivariant responses known as feature maps.[13][14]CNNs take advantage of the hierarchical pattern in data and assemble patterns of increasing complexity using smaller and simpler patterns discovered via their filters.[15]\nConvolutional networks wereinspiredbybiologicalprocesses[16][17][18][19]in that the connectivity pattern betweenneuronsresembles the organization of the animalvisual cortex. Individualcortical neuronsrespond to stimuli only in a restricted region of thevisual fieldknown as thereceptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\nCNN uses relatively little pre-processing compared to otherimage classification algorithms. This means that the network learns to optimize thefilters(or kernels) through automated learning, whereas in traditional algorithms these filters arehand-engineered. This reduced reliance on prior knowledge of the analyst and on human intervention in manual feature extraction makes CNNs a desirable model.[15]\nA phylogenetic convolutional neural network (Ph-CNN) is aconvolutional neural networkarchitecture proposed by Fioranti et al. in 2018 to classifymetagenomicsdata.[20]In this approach, phylogenetic data is endowed with patristic distance (the sum of the lengths of all branches connecting twooperational taxonomic units[OTU]) to select k-neighborhoods for each OTU, and each OTU and its neighbors are processed with convolutional filters.\nUnlike supervised methods,self-supervised learningmethods learn representations without relying on annotated data. That is well-suited for genomics, wherehigh throughput sequencingtechniques can create potentially large amounts of unlabeled data. Some examples of self-supervised learning methods applied on genomics include DNABERT and Self-GenomeNet.[21][22]\nRandom forests(RF) classify by constructing an ensemble ofdecision trees, and outputting the average prediction of the individual trees.[23]This is a modification ofbootstrap aggregating(which aggregates a large collection of decision trees) and can be used forclassificationorregression.[24][25]\nAs random forests give an internal estimate of generalization error, cross-validation is unnecessary. In addition, they produce proximities, which can be used to impute missing values, and which enable novel data visualizations.[26]\nComputationally, random forests are appealing because they naturally handle both regression and (multiclass) classification, are relatively fast to train and to predict, depend only on one or two tuning parameters, have a built-in estimate of the generalization error, can be used directly for high-dimensional problems, and can easily be implemented in parallel. Statistically, random forests are appealing for additional features, such as measures of variable importance, differential class weighting, missing value imputation, visualization, outlier detection, and unsupervised learning.[26]\nClustering- the partitioning of a data set into disjoint subsets, so that the data in each subset are as close as possible to each other and as distant as possible from data in any other subset, according to some defineddistanceorsimilarityfunction - is a common technique for statistical data analysis.\nClustering is central to much data-driven bioinformatics research and serves as a powerful computational method whereby means of hierarchical, centroid-based, distribution-based, density-based, and self-organizing maps classification, has long been studied and used in classical machine learning settings. Particularly, clustering helps to analyze unstructured and high-dimensional data in the form of sequences, expressions, texts, images, and so on. Clustering is also used to gain insights into biological processes at thegenomiclevel, e.g. gene functions, cellular processes, subtypes of cells,gene regulation, and metabolic processes.[27]\nData clustering algorithms can be hierarchical or partitional. Hierarchical algorithms find successive clusters using previously established clusters, whereas partitional algorithms determine all clusters at once. Hierarchical algorithms can be agglomerative (bottom-up) or divisive (top-down).\nAgglomerative algorithms begin with each element as a separate cluster and merge them in successively larger clusters. Divisive algorithms begin with the whole set and proceed to divide it into successively smaller clusters.Hierarchical clusteringis calculated using metrics onEuclidean spaces, the most commonly used is theEuclidean distancecomputed by finding the square of the difference between each variable, adding all the squares, and finding the square root of the said sum. An example of ahierarchical clusteringalgorithm isBIRCH, which is particularly good on bioinformatics for its nearly lineartime complexitygiven generally large datasets.[28]Partitioning algorithms are based on specifying an initial number of groups, and iteratively reallocating objects among groups to convergence. This algorithm typically determines all clusters at once. Most applications adopt one of two popular heuristic methods:k-meansalgorithm ork-medoids. Other algorithms do not require an initial number of groups, such asaffinity propagation. In a genomic setting this algorithm has been used both to cluster biosynthetic gene clusters in gene cluster families(GCF) and to cluster said GCFs.[29]\nTypically, a workflow for applying machine learning to biological data goes through four steps:[2]\nIn general, a machine learning system can usually be trained to recognize elements of a certain class given sufficient samples.[31]For example, machine learning methods can be trained to identify specific visual features such as splice sites.[32]\nSupport vector machineshave been extensively used in cancer genomic studies.[33]In addition,deep learninghas been incorporated into bioinformatic algorithms. Deep learning applications have been used for regulatory genomics and cellular imaging.[34]Other applications include medical image classification, genomic sequence analysis, as well as protein structure classification and prediction.[35]Deep learning has been applied to regulatory genomics, variant calling and pathogenicity scores.[36]Natural language processingandtext mininghave helped to understand phenomena including protein-protein interaction, gene-disease relation as well as predicting biomolecule structures and functions.[37]\nNatural language processingalgorithmspersonalized medicinefor patients who suffer genetic diseases, by combining the extraction of clinical information and genomic data available from the patients. Institutes such as Health-funded Pharmacogenomics Research Network focus on finding breast cancer treatments.[38]\nPrecision medicineconsiders individual genomic variability, enabled by large-scale biological databases. Machine learning can be applied to perform the matching function between (groups of patients) and specific treatment modalities.[39]\nComputational techniques are used to solve other problems, such as efficient primer design forPCR, biological-image analysis and back translation of proteins (which is, given the degeneration of the genetic code, a complex combinatorial problem).[2]\nWhile genomic sequence data has historically been sparse due to the technical difficulty of sequencing a piece of DNA, the number of available sequences is growing. On average, the number ofbasesavailable in theGenBankpublic repository has doubled every 18 months since 1982.[40]However, whileraw datawas becoming increasingly available and accessible, As of 2002[update], biological interpretation of this data was occurring at a much slower pace.[41]This made for an increasing need for developingcomputational genomicstools, including machine learning systems, that can automatically determine the location of protein-encoding genes within a given DNA sequence (i.e.gene prediction).[41]\nGene prediction is commonly performed through bothextrinsic searchesandintrinsic searches.[41]For the extrinsic search, the input DNA sequence is run through a large database of sequences whose genes have been previously discovered and their locations annotated and identifying the target sequence's genes by determining which strings of bases within the sequence arehomologousto known gene sequences. However, not all the genes in a given input sequence can be identified through homology alone, due to limits in the size of the database of known and annotated gene sequences. Therefore, an intrinsic search is needed where a gene prediction program attempts to identify the remaining genes from the DNA sequence alone.[41]\nMachine learning has also been used for the problem ofmultiple sequence alignmentwhich involves aligning many DNA or amino acid sequences in order to determine regions of similarity that could indicate a shared evolutionary history.[2]It can also be used to detect and visualize genome rearrangements.[42]\nProteins, strings ofamino acids, gain much of their function fromprotein folding, where they conform into a three-dimensional structure, including theprimary structure, thesecondary structure(alpha helicesandbeta sheets), thetertiary structure, and thequaternary structure.\nProtein secondary structure prediction is a main focus of this subfield as tertiary and quaternary structures are determined based on the secondary structure.[4]Solving the true structure of a protein is expensive and time-intensive, furthering the need for systems that can accurately predict the structure of a protein by analyzing the amino acid sequence directly.[4][2]Prior to machine learning, researchers needed to conduct this prediction manually. This trend began in 1951 when Pauling and Corey released their work on predicting the hydrogen bond configurations of a protein from a polypeptide chain.[43]Automatic feature learning reaches an accuracy of 82-84%.[4][44]Recent approaches have utilized deep learning techniques for state-of-the-art secondary structure predictions. For example, DeepCNF (deep convolutional neural fields) achieved an accuracy of approximately 84% when tasked to classify the amino acids of a protein sequence into one of three structural classes (helix, sheet, or coil).[44]The theoretical limit for three-state protein secondary structure is 88–90%.[4]In 2018,AlphaFold, anartificial intelligence(AI) program developed byDeepMind, placed first in the overall rankings of the 13thCritical Assessment of Structure Prediction(CASP). It was particularly successful at predicting the most accurate structures for targets rated as most difficult by the competition organizers, where no existingtemplate structureswere available fromproteinswith partially similar sequences. AlphaFold 2 (2020) repeated this placement in the CASP14 competition and achieved a level of accuracy much higher than any other entry.[45][46][47]\nMachine learning has also been applied to proteomics problems such asprotein side-chainprediction,protein loopmodeling, andprotein contact mapprediction.[2]\nMetagenomicsis the study of microbial communities from environmental DNA samples.[48]Currently, limitations and challenges predominate in the implementation of machine learning tools due to the amount of data in environmental samples.[49]Supercomputers and web servers have made access to these tools easier.[50]The high dimensionality of microbiome datasets is a major challenge in studying the microbiome; this significantly limits the power of current approaches for identifying true differences and increases the chance of false discoveries.[51][better source needed]\nDespite their importance, machine learning tools related to metagenomics have focused on the study of gut microbiota and the relationship with digestive diseases, such asinflammatory bowel disease(IBD),Clostridioides difficileinfection (CDI),colorectal canceranddiabetes, seeking better diagnosis and treatments.[50]Many algorithms were developed to classify microbial communities according to the health condition of the host, regardless of the type of sequence data, e.g.16S rRNAorwhole-genome sequencing(WGS), using methods such as least absolute shrinkage and selection operator classifier,random forest, supervised classification model, and gradient boosted tree model.Neural networks, such asrecurrent neural networks(RNN),convolutional neural networks(CNN), andHopfield neural networkshave been added.[50]For example, in 2018, Fioravanti et al. developed an algorithm called Ph-CNN to classify data samples from healthy patients and patients with IBD symptoms (to distinguish healthy and sick patients) by using phylogenetic trees and convolutional neural networks.[52]\nIn addition,random forest(RF) methods and implemented importance measures help in the identification of microbiome species that can be used to distinguish diseased and non-diseased samples. However, the performance of a decision tree and the diversity of decision trees in the ensemble significantly influence the performance of RF algorithms. The generalization error for RF measures how accurate the individual classifiers are and their interdependence. Therefore, the high dimensionality problems of microbiome datasets pose challenges. Effective approaches require many possible variable combinations, which exponentially increases the computational burden as the number of features increases.[51]\nFor microbiome analysis in 2020 Dang & Kishino[51]developed a novel analysis pipeline. The core of the pipeline is an RF classifier coupled with forwardingvariable selection(RF-FVS), which selects a minimum-size core set of microbial species or functional signatures that maximize the predictive classifier performance. The framework combines:\nThey demonstrated performance by analyzing two published datasets from large-scale case-control studies:\nThe proposed approach improved the accuracy from 81% to 99.01% for CDI and from 75.14% to 90.17% for CRC.\nThe use of machine learning in environmental samples has been less explored, maybe because of data complexity, especially from WGS. Some works show that it is possible to apply these tools in environmental samples. In 2021 Dhungel et al.,[53]designed an R package called MegaR. This package allows working with 16S rRNA and whole metagenomic sequences to make taxonomic profiles and classification models by machine learning models. MegaR includes a comfortable visualization environment to improve the user experience. Machine learning in environmental metagenomics can help to answer questions related to the interactions between microbial communities and ecosystems, e.g. the work of Xun et al., in 2021[54]where the use of different machine learning methods offered insights on the relationship among the soil, microbiome biodiversity, and ecosystem stability.\nMicroarrays, a type oflab-on-a-chip, are used for automatically collecting data about large amounts of biological material. Machine learning can aid in analysis, and has been applied to expression pattern identification, classification, and genetic network induction.[2]\nThis technology is especially useful for monitoring gene expression, aiding in diagnosing cancer by examining which genes are expressed.[55]One of the main tasks is identifying which genes are expressed based on the collected data.[2]In addition, due to the huge number of genes on which data is collected by the microarray, winnowing the large amount of irrelevant data to the task of expressed gene identification is challenging. Machine learning presents a potential solution as various classification methods can be used to perform this identification. The most commonly used methods areradial basis function networks,deep learning,Bayesian classification,decision trees, andrandom forest.[55]\nSystems biology focuses on the study of emergent behaviors from complex interactions of simple biological components in a system. Such components can include DNA, RNA, proteins, and metabolites.[56]\nMachine learning has been used to aid in modeling these interactions in domains such as genetic networks, signal transduction networks, and metabolic pathways.[2]Probabilistic graphical models, a machine learning technique for determining the relationship between different variables, are one of the most commonly used methods for modeling genetic networks.[2]In addition, machine learning has been applied to systems biology problems such as identifyingtranscription factor binding sitesusingMarkov chain optimization.[2]Genetic algorithms, machine learning techniques which are based on the natural process of evolution, have been used to model genetic networks and regulatory structures.[2]\nOther systems biology applications of machine learning include the task of enzyme function prediction, high throughput microarray data analysis, analysis of genome-wide association studies to better understand markers of disease, protein function prediction.[57]\nThis domain, particularlyphylogenetic treereconstruction, uses the features of machine learning techniques. Phylogenetic trees are schematic representations of the evolution of organisms. Initially, they were constructed using features such as morphological and metabolic features. Later, due to the availability of genome sequences, the construction of the phylogenetic tree algorithm used the concept based on genome comparison. With the help of optimization techniques, a comparison was done by means of multiple sequence alignment.[58]\nMachine learning methods for the analysis ofneuroimagingdata are used to help diagnosestroke. Historically multiple approaches to this problem involved neural networks.[59][60]\nMultiple approaches to detect strokes used machine learning. As proposed by Mirtskhulava,[61]feed-forward networks were tested to detect strokes using neural imaging. As proposed by Titano[62]3D-CNN techniques were tested in supervised classification to screen head CT images for acute neurologic events. Three-dimensionalCNNandSVMmethods are often used.[60]\nThe increase in biological publications increased the difficulty in searching and compiling relevant available information on a given topic. This task is known asknowledge extraction. It is necessary for biological data collection which can then in turn be fed into machine learning algorithms to generate new biological knowledge.[2][63]Machine learning can be used for this knowledge extraction task using techniques such asnatural language processingto extract the useful information from human-generated reports in a database.Text Nailing, an alternative approach to machine learning, capable of extracting features from clinical narrative notes was introduced in 2017.\nThis technique has been applied to the search for novel drug targets, as this task requires the examination of information stored in biological databases and journals.[63]Annotations of proteins in protein databases often do not reflect the complete known set of knowledge of each protein, so additional information must be extracted from biomedical literature. Machine learning has been applied to the automatic annotation of gene and protein function, determination of theprotein subcellular localization,DNA-expression arrayanalysis, large-scaleprotein interactionanalysis, and molecule interaction analysis.[63]\nAnother application of text mining is the detection and visualization of distinct DNA regions given sufficient reference data.[64]\nMicrobial communities are complex assembles of diverse microorganisms,[65]where symbiont partners constantly produce diverse metabolites derived from the primary and secondary (specialized) metabolism, from which metabolism plays an important role in microbial interaction.[66]Metagenomic and metatranscriptomic data are an important source for deciphering communications signals.\nMolecular mechanisms produce specialized metabolites in various ways.Biosynthetic Gene Clusters(BGCs) attract attention, since several metabolites are clinically valuable, anti-microbial, anti-fungal, anti-parasitic, anti-tumor and immunosuppressive agents produced by the modular action of multi-enzymatic, multi-domains gene clusters, such asNonribosomal peptidesynthetases (NRPSs) andpolyketide synthases(PKSs).[67]Diverse studies[68][69][70][71][72][73][74][75]show that grouping BGCs that share homologous core genes into gene cluster families (GCFs) can yield useful insights into the chemical diversity of the analyzed strains, and can support linking BGCs to their secondary metabolites.[69][71]GCFs have been used as functional markers in human health studies[76][77]and to study the ability of soil to suppress fungal pathogens.[78]Given their direct relationship to catalytic enzymes, and compounds produced from their encoded pathways, BGCs/GCFs can serve as a proxy to explore the chemical space of microbial secondary metabolism. Cataloging GCFs in sequenced microbial genomes yields an overview of the existing chemical diversity and offers insights into future priorities.[68][70]Tools such as BiG-SLiCE and BIG-MAP[79]have emerged with the sole purpose of unveiling the importance of BGCs in natural environments.\nThe increase of experimentally characterizedribosomally synthesized and post-translationally modified peptides(RiPPs), together with the availability of information on their sequence and chemical structure, selected from databases such as BAGEL, BACTIBASE, MIBIG, and THIOBASE, provide the opportunity to develop machine learning tools to decode the chemical structure and classify them.\nIn 2017, researchers at the National Institute of Immunology of New Delhi, India, developed RiPPMiner[80]software, a bioinformatics resource for decoding RiPP chemical structures by genome mining. The RiPPMiner web server consists of a query interface and the RiPPDB database. RiPPMiner defines 12 subclasses of RiPPs, predicting the cleavage site of the leader peptide and the final cross-link of the RiPP chemical structure.\nMany tandem mass spectrometry(MS/MS)based metabolomics studies, such as library matching and molecular networking, use spectral similarity as a proxy for structural similarity. Spec2vec[81]algorithm provides a new way of spectral similarity score, based onWord2Vec. Spec2Vec learns fragmental relationships within a large set of spectral data, in order to assess spectral similarities between molecules and to classify unknown molecules through these comparisons.\nFor systemic annotation, some metabolomics studies rely on fitting measured fragmentation mass spectra to library spectra or contrasting spectra via network analysis. Scoring functions are used to determine the similarity between pairs of fragment spectra as part of these processes. So far, no research has suggested scores that are significantly different from the commonly utilizedcosine-based similarity.[82]\nAn important part of bioinformatics is the management of big datasets, known as databases of reference. Databases exist for each type of biological data, for example for biosynthetic gene clusters and metagenomes.\nThe National Center for Biotechnology Information (NCBI)[83]provides a large suite of online resources for biological information and data, including theGenBanknucleic acid sequence database and thePubMeddatabase of citations and abstracts for published life science journals. Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. Resources include PubMed Data Management, RefSeq Functional Elements, genome data download, variation services API, Magic-BLAST, QuickBLASTp, and Identical Protein Groups. All of these resources can be accessed through NCBI.[84]\nantiSMASH allows the rapid genome-wide identification, annotation and analysis of secondary metabolite biosynthesis gene clusters in bacterial and fungal genomes. It integrates and cross-links with a large number of in silicosecondary metaboliteanalysis tools.[85]\ngutSMASH is a tool that systematically evaluates bacterial metabolic potential by predicting both known and novelanaerobicmetabolic gene clusters (MGCs) from the gutmicrobiome.\nMIBiG,[86]the minimum information about a biosynthetic gene cluster specification, provides a standard for annotations andmetadataon biosynthetic gene clusters and their molecular products. MIBiG is a Genomic Standards Consortium project that builds on the minimum information about any sequence (MIxS) framework.[87]\nMIBiG facilitates the standardized deposition and retrieval of biosynthetic gene cluster data as well as the development of comprehensive comparative analysis tools. It empowers next-generation research on the biosynthesis, chemistry and ecology of broad classes of societally relevant bioactivesecondary metabolites, guided by robust experimental evidence and rich metadata components.[88]\nSILVA[89]is an interdisciplinary project among biologists and computers scientists assembling a complete database of RNA ribosomal (rRNA) sequences of genes, both small (16S,18S, SSU) and large (23S,28S, LSU) subunits, which belong to the bacteria, archaea and eukarya domains. These data are freely available for academic and commercial use.[90]\nGreengenes[91]is a full-length16S rRNAgene database that provides chimera screening, standard alignment and a curated taxonomy based on de novo tree inference.[92][93]Overview:\nOpen Tree of Life Taxonomy (OTT)[94]aims to build a complete, dynamic, and digitally available Tree of Life by synthesizing published phylogenetic trees along with taxonomic data. Phylogenetic trees have been classified, aligned, and merged. Taxonomies have been used to fill in sparse regions and gaps left by phylogenies. OTT is a base that has been little used for sequencing analyzes of the 16S region, however, it has a greater number of sequences classified taxonomically down to the genus level compared to SILVA and Greengenes. However, in terms of classification at the edge level, it contains a lesser amount of information[95]\nRibosomal Database Project (RDP)[96]is a database that provides RNA ribosomal (rRNA) sequences of small subunits of domain bacterial and archaeal (16S); and fungal rRNA sequences of large subunits (28S).[97]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Deepfake",
        "title": "Deepfake - Wikipedia",
        "content": "Deepfakes(aportmanteauof'deep learning'and'fake'[1]) are images, videos, or audio that have been edited or generatedusing artificial intelligence, AI-based tools or audio-video editing software. They may depict real or fictional people and are considered a form ofsynthetic media, that is media that is usually created by artificial intelligence systems by combining various media elements into a new media artifact.[2][3]\nWhile the act of creating fake content is not new, deepfakes uniquely leveragemachine learningandartificial intelligencetechniques,[4][5][6]includingfacial recognitionalgorithms and artificialneural networkssuch asvariational autoencoders(VAEs) andgenerative adversarial networks(GANs).[5][7]In turn, the field of image forensics has worked to develop techniques todetect manipulated images.[8]Deepfakes have garnered widespread attention for their potential use in creatingchild sexual abusematerial,celebrity pornographic videos,revenge porn,fake news,hoaxes,bullying, andfinancial fraud.[9][10][11][12]\nAcademics have raised concerns about the potential for deepfakes to promote disinformation and hate speech,[13]as well as interfere with elections.[14]In response, theinformation technologyindustry and governments have proposed recommendations and methods to detect and mitigate their use. Academic research has also delved deeper into the factors driving deepfake engagement online as well as potential countermeasures to malicious application of deepfakes.\nFrom traditionalentertainmenttogaming, deepfake technology has evolved to be increasingly convincing[15]and available to the public, allowing for the disruption of the entertainment andmediaindustries.[16]\nPhoto manipulationwas developed in the 19th century and soon applied to motion pictures. Technology steadily improved during the 20th century, and more quickly with the advent ofdigital video.\nDeepfake technology has been developed by researchers at academic institutions beginning in the 1990s, and later by amateurs in online communities.[17][18]More recently, the methods have been adopted by industry.[19]\nAcademic research related to deepfakes is split between the field ofcomputer vision, a sub-field of computer science,[17]which develops techniques for creating and identifying deepfakes, and humanities and social science approaches that study the social, ethical, aesthetic implications as well as journalistic and informational implications of deepfakes.[20]As deepfakes have risen in prominence in popularity with innovations provided by AI tools, significant research has gone into detection methods and defining the factors driving engagement with deepfakes on the internet.[21][22]Deepfakes have been shown to appear on social media platforms and other parts of the internet for purposes ranging from entertainment and education related to deepfakes to misinformation to elicit strong reactions.[23]There are gaps in research related to the propagation of deepfakes on social media. Negativity and emotional response are the primary driving factors for users sharing deepfakes.[24]\nAge and lack of literacy related to deepfakes are another factor that drives engagement. Older users who may be technologically-illiterate might not recognize deepfakes as falsified content and share this content because they believe it to be true. Alternatively, younger users accustomed to the entertainment value of deepfakes are more likely to share them with an awareness of their falsified content.[25]Despite cognitive ability being a factor in successfully detecting deepfakes, individuals who are aware of a deepfake may be just as likely to share it on social media as one who does not know it is a deepfake.[26]Within scholarship focused on detecting deepfakes, deep-learning methods using techniques to identify software-induced artifacts have been found to be the most effective in separating a deepfake from an authentic product.[21]Due to the capabilities of deepfakes, concerns have developed related to regulations and literacy toward the technology.[27]The potential malicious applications of deepfakes and their capability to impact public figures, reputations, or promote misleading narratives are the primary drivers of these concerns.[27]Amongst some experts, potential malicious applications of deepfakes have encouraged them into labeling deepfakes as a potential danger to democratic societies that would benefit from a regulatory framework to mitigate potential risks.[27]\nIn cinema studies, deepfakes illustrate how how \"the human face is emerging as a central object of ambivalence in the digital age\".[28]Video artists have used deepfakes to \"playfully rewrite film history by retrofitting canonical cinema with new star performers\".[29]Film scholar Christopher Holliday analyses how altering the gender and race of performers in familiar movie scenes destabilizes gender classifications and categories.[29]The concept of \"queering\" deepfakes is also discussed in Oliver M. Gingrich's discussion of media artworks that use deepfakes to reframe gender,[30]including British artistJake Elwes'Zizi: Queering the Dataset, an artwork that uses deepfakes of drag queens to intentionally play with gender. The aesthetic potentials of deepfakes are also beginning to be explored. Theatre historian John Fletcher notes that early demonstrations of deepfakes are presented as performances, and situates these in the context of theater, discussing \"some of the more troubling paradigm shifts\" that deepfakes represent as a performance genre.[31]\nPhilosophers and media scholars have discussed the ethical implications of deepfakes in the dissemination of disinformation. Amina Vatreš from the Department of Communication Studies at the University of Sarajevo identifies three factors contributing to the widespread acceptance of deepfakes, and where its greatest danger lies: 1) convincing visualization and auditory support, 2) widespread accessibility, and 3) the inability to draw a clear line between truth and falsehood.[20]Another area of discussion on deepfakes is in relation to pornography made with deepfakes.[32]\nBeyond pornography, deepfakes have been framed by philosophers as an \"epistemic threat\" to knowledge and thus to society.[33]There are several other suggestions for how to deal with the risks deepfakes give rise beyond pornography, but also to corporations, politicians and others, of \"exploitation, intimidation, and personal sabotage\",[34]and there are several scholarly discussions of potential legal and regulatory responses both in legal studies and media studies.[35]In psychology and media studies, scholars discuss the effects ofdisinformationthat uses deepfakes,[36][37]and the social impact of deepfakes.[38]\nWhile most English-language academic studies of deepfakes focus on the Western anxieties about disinformation and pornography, digital anthropologist Gabriele de Seta has analyzed the Chinese reception of deepfakes, which are known ashuanlian, which translates to \"changing faces\". The Chinese term does not contain the \"fake\" of the English deepfake, and de Seta argues that this cultural context may explain why the Chinese response has centered on practical regulatory measures to \"fraud risks, image rights, economic profit, and ethical imbalances\".[39]\nA landmark early project was the \"Video Rewrite\" program, published in 1997. The program modified existing video footage of a person speaking to depict that person mouthing the words from a different audio track.[40]It was the first system to fully automate this kind of facial reanimation, and it did so using machine learning techniques to make connections between the sounds produced by a video's subject and the shape of the subject's face.[40]\nContemporary academic projects have focused on creating more realistic videos and improving deepfake techniques.[41][42]The \"Synthesizing Obama\" program, published in 2017, modifies video footage of former presidentBarack Obamato depict him mouthing the words contained in a separate audio track.[41]The project lists as a main research contribution to itsphotorealistictechnique for synthesizing mouth shapes from audio.[41]The \"Face2Face\" program, published in 2016, modifies video footage of a person's face to depict them mimicking another person's facial expressions.[42]The project highlights its primary research contribution as the development of the first method for re-enacting facial expressions in real time using a camera that does not capture depth, enabling the technique to work with common consumer cameras.\nIn August 2018, researchers at theUniversity of California, Berkeleypublished a paper introducing a deepfake dancing app that can create the impression of masterful dancing ability using AI.[43]This project expands the application of deepfakes to the entire body; previous works focused on the head or parts of the face.[44]\nResearchers have also shown that deepfakes are expanding into other domains such as medical imagery.[45]In this work, it was shown how an attacker can automatically inject or remove lung cancer in a patient's3D CT scan. The result was so convincing that it fooled three radiologists and a state-of-the-art lung cancer detection AI. To demonstrate the threat, the authors successfully performed the attack on a hospital in aWhite hat penetration test.[46]\nA survey of deepfakes, published in May 2020, provides a timeline of how the creation and detection deepfakes have advanced over the last few years.[47]The survey identifies that researchers have been focusing on resolving the following challenges of deepfake creation:\nOverall, deepfakes are expected to have several implications in media and society, media production, media representations, media audiences, gender, law, and regulation, and politics.[48]\nThe termdeepfakeoriginated in late 2017 from aReddituser named \"deepfakes\".[49]He, along with other members of Reddit's \"r/deepfakes\", shared deepfakes they created; many videos involved celebrities' faces swapped onto the bodies of actors in pornographic videos,[49]while non-pornographic content included many videos with actorNicolas Cage's face swapped into various movies.[50]\nOther online communities remain, including Reddit communities that do not share pornography, such as \"r/SFWdeepfakes\" (short for \"safe for work deepfakes\"), in which community members share deepfakes depicting celebrities, politicians, and others in non-pornographic scenarios.[51]Other online communities continue to share pornography on platforms that have not banned deepfake pornography.[52]\nIn January 2018, a proprietary desktop application called \"FakeApp\" was launched.[53]This app allows users to easily create and share videos with their faces swapped with each other.[54]As of 2019, \"FakeApp\" had been largely replaced by open-source alternatives such as \"Faceswap\", command line-based \"DeepFaceLab\", and web-based apps such as DeepfakesWeb.[55][56][57]\nLarger companies started to use deepfakes.[19]Corporate training videos can be created using deepfaked avatars and their voices, for exampleSynthesia, which uses deepfake technology with avatars to create personalized videos.[58]The mobile appMomocreated the application Zao which allows users to superimpose their face on television and movie clips with a single picture.[19]As of 2019 the Japanese AI company DataGrid made a full body deepfake that could create a person from scratch.[59]\nAs of 2020audio deepfakes, and AI software capable of detecting deepfakes andcloning human voicesafter 5 seconds of listening time also exist.[60][61][62][63][64][65][excessive citations]A mobile deepfake app, Impressions, was launched in March 2020. It was the first app for the creation of celebrity deepfake videos from mobile phones.[66][67]\nDeepfake technology's ability to  fabricate messages and actions of others can include deceased individuals. In October 2020,Kim Kardashianposted a video featuring ahologramof her late fatherRobert Kardashiancreated by the company Kaleida, which used a combination of performance, motion tracking, SFX, VFX andDeepFaketechnologies to create the illusion.[68][69]\nIn 2020, a deepfake video of Joaquin Oliver, a victim of theParkland shootingwas created as part of a gun safety campaign. Oliver's parents partnered with nonprofit Change the Ref and McCann Health to produce a video in which Oliver to encourage people to support gun safety legislation and politicians who back do so as well.[70]\nIn 2022, a deepfake video ofElvis Presleywas used on the programAmerica's Got Talent 17.[71]\nA TV commercial used a deepfake video ofBeatlesmemberJohn Lennon, who was murdered in 1980.[72]\nDeepfakes rely on a type ofneural networkcalled anautoencoder.[73]These consist of an encoder, which reduces an image to a lower dimensionallatent space, and a decoder, which reconstructs the image from the latent representation.[74]Deepfakes utilize this architecture by having a universal encoder which encodes a person in to the latent space.[citation needed]The latent representation contains key features about their facial features and body posture. This can then be decoded with a model trained specifically for the target. This means the target's detailed information will be superimposed on the underlying facial and body features of the original video, represented in the latent space.[citation needed]\nA popular upgrade to this architecture attaches agenerative adversarial networkto the decoder. AGANtrains a generator, in this case the decoder, and a discriminator in an adversarial relationship. The generator creates new images from the latent representation of the source material, while the discriminator attempts to determine whether or not the image is generated.[citation needed]This causes the generator to create images that mimic reality extremely well as any defects would be caught by the discriminator.[75]Both algorithms improve constantly in azero sum game. This makes deepfakes difficult to combat as they are constantly evolving; any time a defect is determined, it can be corrected.[75]\nDigital clonesof professional actors have appeared infilmsbefore, and progress in deepfake technology is expected to further the accessibility and effectiveness of such clones.[76]The use of AI technology was a major issue in the2023 SAG-AFTRA strike, as new techniques enabled the capability of generating and storing a digital likeness to use in place of actors.[77]\nDisneyhas improved their visual effects using high-resolution deepfake face swapping technology.[78]Disney improved their technology through progressive training programmed to identify facial expressions, implementing a face-swapping feature, and iterating in order to stabilize and refine the output.[78]This high-resolution deepfake technology saves significant operational and production costs.[79]Disney's deepfake generation model can produce AI-generated media at a 1024 x 1024 resolution, as opposed to common models that produce media at a 256 x 256 resolution.[79]The technology allows Disney tode-agecharacters or revive deceased actors.[80]Similar technology was initially used by fans to unofficially insert faces into existing media, such as  overlayingHarrison Ford's young face onto Han Solo's face inSolo: A Star Wars Story.[81]Disney used deepfakes for the characters of Princess Leia inRogue Oneand Luke Skywalker in bothThe MandalorianandThe Book of Boba Fett.[82][83]\nThe 2020 documentaryWelcome to Chechnyaused deepfake technology to obscure the identity of the people interviewed, so as to protect them from retaliation.[84]\nCreative Artists Agencyhas developed a facility to capture the likeness of an actor \"in a single day\", to develop a digital clone of the actor, which would be controlled by the actor or their estate alongside otherpersonality rights.[85]\nCompanies which have used digital clones of professional actors in advertisements includePuma,NikeandProcter & Gamble.[86]\nDeepfakes allowed for the use of David Beckham in a campaign using nearly nine languages to raise awareness the fight against Malaria.[87]\nIn the 2024 IndianTamilscience fictionaction thrillerThe Greatest of All Time, the teenage version ofVijay's character Jeevan is portrayed by Ayaz Khan. Vijay's teenage face was then attained byAIdeepfake.[88]\nDeepfakes are also being used in education and media to create realistic videos and interactive content, which offer new ways to engage audiences.\nIn March 2018 the multidisciplinary artist Joseph Ayerle published thevideo artworkUn'emozione per sempre 2.0(English title:The Italian Game). The artist worked with Deepfake technology to create anAI actor,a synthetic version of 80s movie starOrnella Muti, traveling in time from 1978 to 2018. TheMassachusetts Institute of Technologyreferred this artwork in the study \"Collective Wisdom\".[89]The artist used Ornella Muti'stime travelto explore generational reflections, while also investigating questions about the role of provocation in the world of art.[90]For the technical realization Ayerle used scenes of photo modelKendall Jenner. The program replaced Jenner's face by an AI calculated face of Ornella Muti. As a result, the AI actor has the face of the Italian actor Ornella Muti and the body of Kendall Jenner.\nDeepfakes have been widely used insatireor to parody celebrities and politicians. The 2020 webseriesSassy Justice, created byTrey ParkerandMatt Stone, heavily features the use of deepfaked public figures to satirize current events and raise awareness of deepfake technology.[91]\nDeepfakes can be used to generate blackmail materials that falsely incriminate a victim. A report by the AmericanCongressional Research Servicewarned that deepfakes could be used to blackmail elected officials or those with access toclassified informationforespionageorinfluencepurposes.[92]\nWhen or if fakes cannot reliably be distinguished from genuine evidence, victims who are blackmailed over digital evidence might claim that true artifacts are fakes, thereby seeking plausible deniability by relying on an argument of indistinguishability between fake and genuine evidence. The hoped-for effect is to void credibility of certain existing blackmail materials, which, if they were the sole evidence retained by a blackmailer and could not be distinguished by a jury from fake evidence under this argument, could in theory erode loyalty to blackmailers and limit their control over the blackmailed. This phenomenon has been termed \"blackmail inflation\", since in theory it \"devalues\" authentic blackmail material.[93]It is possible to utilize commodity GPU hardware with a small software program to generate fake content intended to blackmail anyone for whom an adversary has ample training data.[94]However, even carefully manipulated fakes may still be detected.\nThe inflation argument could only work in theory if blackmailers have no other incriminating evidence that is not easily faked, and if the jury were persuaded that the evidences of guilt were not sufficient to convict beyond reasonable doubt. In reality this theory risks double hazards, namely, that those who are guilty might deploy arguments of plausible deniability, arguing that the footage has been faked, possibly resulting in acquittal of a guilty person on the basis of such doubt, and second, that fake evidence might be used to prosecute those who are not aware of the deepfake argument, to secure a conviction in cases where a jury is not adequately aware of the risk of false positives due to fake evidence, or to extort a plea deal where the prosecution claims to have damning evidence. The effect of this double hazard will depend on the level of discernment of the parties in the criminal justice system and their empowerment to act on that discernment. The inflation argument could be abused in either direction as illustrated, and the notion that blackmailers would not retain further evidence or leverage is unlikely and undependable, limiting the effectiveness of the theory. The existence of efficient techniques for fabricating false evidence certainly suggests that any combination of video, audio, photographic or other generable evidence alone as the basis for conviction of a crime is by now a perilous and tenuous standard owing to the possibility of maliciously fabricated evidence, raising the importance of multiple firsthand witnesses to a crime, especially for more serious allegations.[95]\nOn June 8, 2022,[96]Daniel Emmet, a formerAGTcontestant, teamed up with theAIstartup[97][98]Metaphysic AI, to create a hyperrealistic deepfake to make it appear asSimon Cowell. Cowell, notoriously known for severely critiquing contestants,[99]was on stage interpreting \"You're The Inspiration\" byChicago. Emmet sang on stage as an image of Simon Cowell emerged on the screen behind him in flawless synchronicity.[100]\nOn August 30, 2022, Metaphysic AI had 'deep-fake'Simon Cowell,Howie MandelandTerry Crewssingingoperaon stage.[101]\nOn September 13, 2022, Metaphysic AI performed with asyntheticversion ofElvis Presleyfor the finals ofAmerica's Got Talent.[102]\nTheMITartificial intelligence project15.aihas been used for content creation for multiple Internetfandoms, particularly on social media.[103][104][105]\nIn 2023 the bandsABBAandKISSpartnered withIndustrial Light & MagicandPophouse Entertainmentto develop deepfake avatars capable of performingvirtual concerts.[106]\nFraudsters and scammers make use of deepfakes to trick people into fake investment schemes,financial fraud,cryptocurrencies,sending money, and followingendorsements. The likenesses of celebrities and politicians have been used for large-scale scams, as well as those of private individuals, which are used inspearphishingattacks. According to theBetter Business Bureau, deepfake scams are becoming more prevalent.[107]These scams are responsible for an estimated $12 billion in fraud losses globally.[108]According to a recent report these numbers are expected to reach $40 Billion over the next three years.[108]\nFake endorsements have misused the identities of celebrities likeTaylor Swift,[109][107]Tom Hanks,[110]Oprah Winfrey,[111]andElon Musk;[112]news anchors[113]likeGayle King[110]andSally Bundock;[114]and politicians likeLee Hsien Loong[115]andJim Chalmers.[116][117]Videos of them have appeared inonline advertisementsonYouTube,Facebook, andTikTok, who have policies againstsynthetic and manipulated media.[118][109][119]Ads running these videos are seen by millions of people. A singleMedicare fraudcampaign had been viewed more than 195 million times across thousands of videos.[118][120]Deepfakes have been used for: a fake giveaway ofLe Creusetcookware for a \"shipping fee\" without receiving the products, except for hidden monthly charges;[109]weight-loss gummies that charge significantly more than what was said;[111]a fake iPhone giveaway;[109][119]and fraudulentget-rich-quick,[112][121]investment,[122]andcryptocurrencyschemes.[115][123]\nMany ads pair AIvoice cloningwith \"decontextualized video of the celebrity\" to mimic authenticity. Others use a whole clip from a celebrity before moving to a different actor or voice.[118]Some scams may involve real-time deepfakes.[119]\nCelebrities have been warning people of these fake endorsements, and to be more vigilant against them.[107][109][111]Celebrities are unlikely to file lawsuits against every person operating deepfake scams, as \"finding and suing anonymous social media users is resource intensive,\" thoughcease and desistletters to social media companies work in getting videos and ads taken down.[124]\nAudio deepfakeshave been used as part ofsocial engineeringscams, fooling people into thinking they are receiving instructions from a trusted individual.[125]In 2019, a U.K.-based energy firm's CEO was scammed over the phone when he was ordered to transfer €220,000 into a Hungarian bank account by an individual who reportedly used audio deepfake technology to impersonate the voice of the firm's parent company's chief executive.[126][127]\nAs of 2023, the combination advances in deepfake technology, which could clone an individual's voice from a recording of a few seconds to a minute, and newtext generation tools, enabled automated impersonation scams, targeting victims using a convincing digital clone of a friend or relative.[128]\nAudio deepfakes can be used to mask a user's real identity. Inonline gaming, for example, aplayermay want to choose a voice that sounds like theirin-game characterwhen speaking to other players. Those who are subject toharassment, such as women, children, and transgender people, can use these \"voice skins\" to hide their gender or age.[129]\nIn 2020, aninternet memeemerged utilizing deepfakes to generate videos of people singing the chorus of \"Baka Mitai\"(ばかみたい), a song from the gameYakuza 0in the video game seriesLike a Dragon. In the series, the melancholic song is sung by the player in akaraokeminigame. Most iterations of this meme use a 2017 video uploaded by user Dobbsyrules, wholip syncsthe song, as a template.[130][131]\nDeepfakes have been used to misrepresent well-known politicians in videos.\nIn 2017, Deepfake pornography prominently surfaced on the Internet, particularly onReddit.[158]As of 2019, many deepfakes on the internet feature pornography of female celebrities whose likeness is typically used without their consent.[159]A report published in October 2019 by Dutch cybersecurity startup Deeptrace estimated that 96% of all deepfakes online were pornographic.[160]As of 2018, aDaisy Ridleydeepfake first captured attention,[158]among others.[161][162][163]As of October 2019, most of the deepfake subjects on the internet were British and American actors.[159]However, around a quarter of the subjects are South Korean, the majority of which are K-pop stars.[159][164]\nIn June 2019, a downloadableWindowsandLinuxapplication called DeepNude was released that used neural networks, specificallygenerative adversarial networks, to remove clothing from images of women. The app had both a paid and unpaid version, the paid version costing $50.[165][166]On 27 June the creators removed the application and refunded consumers.[167]\nFemale celebrities are often a main target when it comes to deepfake pornography. In 2023, deepfake porn videos appeared online ofEmma WatsonandScarlett Johanssonin a face swapping app.[168]In 2024, deepfake porn images circulated online ofTaylor Swift.[169]\nAcademic studies have reported that women, LGBT people and people of color (particularly activists, politicians and those questioning power) are at higher risk of being targets of promulgation of deepfake pornography.[170]\nDeepfakes have begun to see use in popular social media platforms, notably through Zao, a Chinese deepfake app that allows users to substitute their own faces onto those of characters in scenes from films and television shows such asRomeo + JulietandGame of Thrones.[171]The app originally faced scrutiny over its invasive user data and privacy policy, after which the company put out a statement claiming it would revise the policy.[19]In January 2020 Facebook announced that it was introducing new measures to counter this on its platforms.[172]\nTheCongressional Research Servicecited unspecified evidence as showing that foreignintelligence operativesused deepfakes to create social media accounts with the purposes ofrecruitingindividuals with access toclassified information.[92]\nIn 2021, realistic deepfake videos of actorTom Cruisewere released onTikTok, which went viral and garnered more than tens of millions of views. The deepfake videos featured an \"artificial intelligence-generated doppelganger\" of Cruise doing various activities such as teeing off at the golf course, showing off a coin trick, and biting into a lollipop. The creator of the clips,BelgianVFXArtist Chris Umé,[173]said he first got interested in deepfakes in 2018 and saw the \"creative potential\" of them.[174][175]\nDeepfake photographs can be used to createsockpuppets, non-existent people, who are active both online and in traditional media. A deepfake photograph appears to have been generated together with a legend for an apparently non-existent person named Oliver Taylor, whose identity was described as a university student in the United Kingdom. The Oliver Taylor persona submitted opinion pieces in several newspapers and was active in online media attacking a British legal academic and his wife, as \"terrorist sympathizers.\" The academic had drawn international attention in 2018 when he commenced a lawsuit in Israel against NSO, a surveillance company, on behalf of people in Mexico who alleged they were victims of NSO'sphone hackingtechnology.Reuterscould find only scant records for Oliver Taylor and \"his\" university had no records for him. Many experts agreed that the profile photo is a deepfake. Several newspapers have not retracted articles attributed to him or removed them from their websites. It is feared that such techniques are a new battleground indisinformation.[176]\nCollections of deepfake photographs of non-existent people onsocial networkshave also been deployed as part of Israelipartisanpropaganda. TheFacebookpage \"Zionist Spring\" featured photos of non-existent persons along with their \"testimonies\" purporting to explain why they have abandoned their left-leaning politics to embraceright-wing politics, and the page also contained large numbers of posts fromPrime Minister of IsraelBenjamin Netanyahuand his son and from other Israeli right wing sources. The photographs appear to have been generated by \"human image synthesis\" technology, computer software that takes data from photos of real people to produce a realistic composite image of a non-existent person. In much of the \"testimonies,\" the reason given for embracing the political right was the shock of learning of allegedincitementto violence against the prime minister. Right wing Israeli television broadcasters then broadcast the \"testimonies\" of these non-existent people based on the fact that they were being \"shared\" online. The broadcasters aired these \"testimonies\" despite being unable to find such people, explaining \"Why does the origin matter?\" Other Facebook profiles of fictitious individuals posted material that allegedly contained material critical of the prime minister which the prime minister claimed was a plot to murder him.[177][178]\nThough fake photos have long been plentiful, faking motion pictures has been more difficult, and the presence of deepfakes increases the difficulty of classifying videos as genuine or not.[132]AI researcher Alex Champandard has said people should know how fast things can be corrupted with deepfake technology, and that the problem is not a technical one, but rather one to be solved by trust in information and journalism.[132]Computer science associate professorHao Liof theUniversity of Southern Californiastates that deepfakes created for malicious use, such asfake news, will be even more harmful if nothing is done to spread awareness of deepfake technology.[179]Li predicted that genuine videos and deepfakes would become indistinguishable in as soon as half a year, as of October 2019, due to rapid advancement[180]inartificial intelligenceand computer graphics.[179]FormerGooglefraud czarShuman Ghosemajumderhas called deepfakes an area of \"societal concern\" and said that they will inevitably evolve to a point at which they can be generated automatically, and an individual could use that technology to produce millions of deepfake videos.[181]\nA primary pitfall is that humanity could fall into an age in which it can no longer be determined whether a medium's content corresponds to the truth.[132][182]Deepfakes are one of a number of tools fordisinformation attack, creating doubt, and undermining trust. They have a potential to interfere with democratic functions in societies, such as identifying collective agendas, debating issues, informing decisions, and solving problems though the exercise of political will.[183]People may also start to dismiss real events as fake.[129]\nDeepfakes possess the ability to damage individual entities tremendously.[184]This is because deepfakes are often targeted at one individual, and/or their relations to others in hopes to create a narrative powerful enough to influence public opinion or beliefs. This can be done through deepfake voice phishing, which manipulates audio to create fake phone calls or conversations.[184]Another method of deepfake use is fabricated private remarks, which manipulate media to convey individuals voicing damaging comments.[184]The quality of a negative video or audio does not need to be that high. As long as someone's likeness and actions are recognizable, a deepfake can hurt their reputation.[129]\nIn September 2020 Microsoft made public that they are developing a Deepfake detection software tool.[185]\nDetecting fake audio is a highly complex task that requires careful attention to the audio signal in order to achieve good performance. Using deep learning, preprocessing of feature design and masking augmentation have been proven effective in improving performance.[186]\nMost of the academic research surrounding deepfakes focuses on the detection of deepfake videos.[187]One approach to deepfake detection is to use algorithms to recognize patterns and pick up subtle inconsistencies that arise in deepfake videos.[187]For example, researchers have developed automatic systems that examine videos for errors such as irregular blinking patterns of lighting.[188][17]This approach has been criticized because deepfake detection is characterized by a \"moving goal post\" where the production of deepfakes continues to change and improve as algorithms to detect deepfakes improve.[187]In order to assess the most effective algorithms for detecting deepfakes, a coalition of leading technology companies hosted the Deepfake Detection Challenge to accelerate the technology for identifying manipulated content.[189]The winning model of the Deepfake Detection Challenge was 65% accurate on the holdout set of 4,000 videos.[190]A team at Massachusetts Institute of Technology published a paper in December 2021 demonstrating that ordinary humans are 69–72% accurate at identifying a random sample of 50 of these videos.[191]\nA team at the University of Buffalo published a paper in October 2020 outlining their technique of using reflections of light in the eyes of those depicted to spot deepfakes with a high rate of success, even without the use of an AI detection tool, at least for the time being.[192]\nIn the case of well-documented individuals such as political leaders, algorithms have been developed to distinguish identity-based features such as patterns of facial, gestural, and vocal mannerisms and detect deep-fake impersonators.[193]\nAnother team led by Wael AbdAlmageed with Visual Intelligence and Multimedia Analytics Laboratory (VIMAL) of theInformation Sciences Instituteat theUniversity Of Southern Californiadeveloped two generations[194][195]of deepfake detectors based onconvolutional neural networks. The first generation[194]usedrecurrent neural networksto spot spatio-temporal inconsistencies to identify visual artifacts left by the deepfake generation process. The algorithm achieved 96% accuracy on FaceForensics++, the only large-scale deepfake benchmark available at that time. The second generation[195]used end-to-end deep networks to differentiate between artifacts and high-level semantic facial information using two-branch networks. The first branch propagates color information while the other branch suppresses facial content and amplifies low-level frequencies usingLaplacian of Gaussian (LoG). Further, they included a new loss function that learns a compact representation of bona fide faces, while dispersing the representations (i.e. features) of deepfakes. VIMAL's approach showed state-of-the-art performance on FaceForensics++ and Celeb-DF benchmarks, and onMarch 16, 2022(the same day of the release), was used to identify the deepfake of Volodymyr Zelensky out-of-the-box without any retraining or knowledge of the algorithm with which the deepfake was created.[citation needed]\nOther techniques suggest thatblockchaincould be used to verify the source of the media.[196]For instance, a video might have to be verified through the ledger before it is shown on social media platforms.[196]With this technology, only videos from trusted sources would be approved, decreasing the spread of possibly harmful deepfake media.[196]\nDigitally signing of all video and imagery by cameras and video cameras, including smartphone cameras, was suggested to fight deepfakes.[197]That allows tracing every photograph or video back to its original owner that can be used to pursue dissidents.[197]\nOne easy way to uncover deepfake video calls consists in asking the caller to turn sideways.[198]\nLegal experts are actively questioning whether current and emerging regulatory frameworks adequately balance the advancements in deepfake detection with the protection of individual rights. Relevant legislation being scrutinized includes theEU AI Act, theGeneral Data Protection Regulation(GDPR), theDigital Services Actin the European Union, as well as the fragmented state and federal laws in the United States, theOnline Safety Act 2023in the United Kingdom, and China'sAdministrative Provisions on Deep Synthesis in Internet-Based Information Services(commonly known as the Deep Synthesis Provisions)[199]. Scholars are evaluating if these frameworks effectively address the complex interplay between technology, rights, and responsibilities in the context of deepfakes.[200]\nHenry Ajder who works for Deeptrace, a company that detects deepfakes, says there are several ways to protect against deepfakes in the workplace. Semantic passwords or secret questions can be used when holding important conversations. Voice authentication and otherbiometric security featuresshould be up to date. Educate employees about deepfakes.[129]\nDue to the capability of deepfakes to fool viewers and believably mimic a person, research has indicated that the concept of truth through observation cannot be fully relied on.[201]Additionally, literacy of the technology among populations could be called into question due to the relatively new success of convincing deepfakes.[201]When combined with increasing ease of access to the technology, this has led to the concern amongst some experts that some societies are not prepared to interact with deepfakes organically without potential consequences from sharing misinformation and disinformation.[201]Media literacyhas been considered as a potential counter to \"prime\" a viewer to identify a deepfake when they encounter one organically by engendering critical thinking.[201]While media literacy education can have conflicting results in the overall success in detecting deepfakes,[202]research has indicated that critical thinking and a skeptical outlook toward a presented piece of media are effective at assisting an individual in determining a deepfake.[202][203]Media literacy frameworks promote critical analysis of media and the motivations behind the presentation of the associated content. Media literacy shows promise as a potential cognitive countermeasure when interacting with malicious deepfakes.[202]\nIn March 2024, a video clip was released byBuckingham Palaceannouncing thatKate Middletonhad cancer and was undergoing chemotherapy. The appearance of a ring worn by Middleton in the clip fueled rumors that the clip was a deepfake.[204]Johnathan Perkins,UCLA's Director of Race and Equity, doubted Middleton had cancer, and further speculated that she could be in critical condition or dead.[205]\nRecently, the use of deepfakes has inspired research on deepfake's capability and effects when used in disinformation campaigns. This capability has raised concerns, partly due to the potential of deepfakes to circumvent a person's skepticism and influence their views on an issue.[206][182]Due to the continued advancement in technology that improves deceptive capabilities of deepfakes, some scholars believe that deepfakes could pose a significant threat to democratic societies.[207]Studies have investigated the effects of political deepfakes.[206][207][182]In two separate studies focusing on Dutch participants, it was found that deepfakes have varying effects on an audience. As a tool of disinformation, deepfakes did not necessarily produce stronger reactions or shifts in viewpoints than traditional textual disinformation.[206]However, deepfakes did produce a reassuring effect on individuals who held preconceived notions that aligned with the viewpoint promoted by the deepfake disinformation in the study.[206]Additionally, deepfakes are effective when designed to target a specific demographic segment related to a particular issue.[207]\"Microtargeting\" involves understanding nuanced political issues of a specific demographic to create a targeted deepfake. The targeted deepfake is then used to connect with and influence the viewpoint of that demographic. Targeted deepfakes were found to be notably effective by the researchers.[207]Research has also found that the political effects of deepfakes are not necessarily as straightforward or assured. Researchers in the United Kingdom uncovered that deepfake political disinformation does not have a guaranteed effect on populations beyond indications that it may sow distrust or uncertainty in a source that provides the deepfake.[182]The implications of distrust in sources led researchers to conclude that deepfakes may have outsized effect in a \"low-trust\" information environment where public institutions are not trusted by the public.[182]\nAcross the world, there are key instances where deepfakes have been used to misrepresent well-known politicians and other public figures.[208]\nChat siteDiscordhas taken action against deepfakes in the past,[237]and has taken a general stance against deepfakes.[238][239]\nGfycatbegan removing all deepfakes from its site on January 31, 2018.[240][238]\nRedditbanned the r/deepfakes subreddit on February 7, 2018, due to the policy violation of \"involuntary pornography\".[241][242][243][244][245]That same month, representatives fromTwitterstated that they would suspend accounts suspected of posting non-consensual deepfake content.[246]\nIn February 2018,Pornhubsaid that it would ban deepfake videos on its website because it is considered \"non consensual content\" which violates their terms of service.[247]They also stated previously to Mashable that they will take down content flagged as deepfakes.[248]Writers from Motherboard reported that searching \"deepfakes\" onPornhubstill returned multiple recent deepfake videos.[247]\nGoogleadded \"involuntary synthetic pornographic imagery\" to its ban list in September 2018, allowing anyone to request the block of results showing their fake nudes.[249][check quotation syntax]In May 2022, Google officially changed the terms of service for theirJupyter Notebook colabs, banning the use of their colab service for the purpose of creating deepfakes.[250]This came a few days after the publication of aVICEarticle in which its author, Emanuel Maiberg, reported \"Most deepfakes are non-consensual porn\", and that the main use of popular deepfake software DeepFaceLab (DFL), \"the most important technology powering the vast majority of this generation of deepfakes\", which often was used in combination with Google colabs, was to create non-consensual pornography. Maiberg pointed to the fact that among many other well-known examples of third-party DFL implementations, such as deepfakes commissioned byThe Walt Disney Company, official music videos, and web seriesSassy Justiceby the creators ofSouth Park, DFL'sGitHubpage, also linked to deepfake porn websiteMr.‍Deepfakesand participants of the DFL Discord server also participate onMr.‍Deepfakes.[251]\nFacebookhas previously stated that they would not remove deepfakes from their platforms.[252]The videos will instead be flagged as fake by third-parties and then have a lessened priority in user's feeds.[253]This response was prompted in June 2019 after a deepfake featuring a 2016 video ofMark Zuckerbergcirculated on Facebook andInstagram.[252]Subsequently, Facebook has taken efforts towards encouraging the creation of deepfakes in order to develop state of the art deepfake detection software. Facebook was the prominent partner in hosting the Deepfake Detection Challenge (DFDC), held December 2019, to 2114 participants who generated more than 35,000 models.[254]The top performing models with the highest detection accuracy were analyzed for similarities and differences; these findings are areas of interest in further research to improve and refine deepfake detection models.[254]Facebook has also detailed that the platform will be taking down media generated with artificial intelligence used to alter an individual's speech.[255]However, media that has been edited to alter the order or context of words in one's message would remain on the site but be labeled as false, since it was not generated by artificial intelligence.[255]\nTwitter(now known as X) is taking active measures to handle synthetic and manipulated media on their platform. In order to prevent disinformation from spreading, Twitter is placing a notice on tweets that contain manipulated media and/or deepfakes that signal to viewers that the media is manipulated.[256]There will also be a warning that appears to users who plan on retweeting, liking, or engaging with the tweet.[256]Twitter will also work to provide users a link next to the tweet containing manipulated or synthetic media that links to a Twitter Moment or credible news article on the related topic—as a debunking action.[256]Twitter also has the ability to remove any tweets containing deepfakes or manipulated media that may pose a harm to users' safety.[256]In order to better improve Twitter's detection of deepfakes and manipulated media, Twitter asked users who are interested in partnering with them to work on deepfake detection solutions to fill out a form.[257]\nIn August 2024, thesecretaries of stateof Minnesota, Pennsylvania, Washington, Michigan and New Mexico penned an open letter to X owner Elon Musk urging modifications to its AI chatbotGrok's newtext-to-videogenerator, added in August 2024, stating that it had disseminated election misinformation.[258][259][260]\nIn the United States, there have been some responses to the problems posed by deepfakes. In 2018, the Malicious Deep Fake Prohibition Act was introduced to theUS Senate;[261]in 2019, the Deepfakes Accountability Act was introduced in the116th United States CongressbyU.S. representativeforNew York's 9th congressional districtYvette Clarke.[262]\nIn 2024, over half of documented identity fraud involved AI-created forgeries,[263]leading several states to introduce legislation regarding deepfakes, including Virginia,[264]Texas, California, and New York;[265]charges as varied asidentity theft,cyberstalking, andrevenge pornhave been pursued, while more comprehensive statutes are urged.[249]\nAmong U.S. legislative efforts, on 3 October 2019, California governorGavin Newsomsigned into law Assembly Bills No. 602 and No. 730.[266][267]Assembly Bill No. 602 provides individuals targeted by sexually explicit deepfake content made without their consent with a cause of action against the content's creator.[266]Assembly Bill No. 730 prohibits the distribution of malicious deepfake audio or visual media targeting a candidate running for public office within 60 days of their election.[267]U.S. representative Yvette Clarke introduced H.R. 5586: Deepfakes Accountability Act into the118th United States Congresson September 20, 2023 in an effort to protect national security from threats posed by deepfake technology.[268]U.S. representativeMaría Salazarintroduced H.R. 6943: No AI Fraud Act into the118th United States Congresson January 10, 2024, to establish specific property rights of individual physicality, including voice.[269]\nIn November 2019, China announced that deepfakes and other synthetically faked footage should bear a clear notice about their fakeness starting in 2020. Failure to comply could be considered acrimetheCyberspace Administration of Chinastated on its website.[270]The Chinese government seems to be reserving the right to prosecute both users andonline video platformsfailing to abide by the rules.[271]The Cyberspace Administration of China, theMinistry of Industry and Information Technology, and theMinistry of Public Securityjointly issued the Provision on the Administration of Deep Synthesis Internet Information Service in November 2022.[272]China's updated Deep Synthesis Provisions (Administrative Provisions on Deep Synthesis in Internet-Based Information Services) went into effect in January 2023.[273]\nIn the United Kingdom, producers of deepfake material could be prosecuted for harassment, but deepfake production was not a specific crime[274]until 2023, when theOnline Safety Actwas passed, which made deepfakes illegal; the UK plans to expand the Act's scope to criminalize deepfakes created with \"intention to cause distress\" in 2024.[275][276]\nIn Canada, in 2019, theCommunications Security Establishmentreleased a report which said that deepfakes could be used to interfere in Canadian politics, particularly to discredit politicians and influence voters.[277][278]As a result, there are multiple ways for citizens in Canada to deal with deepfakes if they are targeted by them.[279]In February 2024,billC-63 was tabled in the44th Canadian Parliamentin order to enact theOnline Harms Act, which would amend Criminal Code, and other Acts. An earlier version of the Bill, C-36, was ended by the dissolution of the 43rd Canadian Parliament in September 2021.[280][281]\nIn India, there are no direct laws or regulation on AI or deepfakes, but there are provisions under the Indian Penal Code and Information Technology Act 2000/2008, which can be looked at for legal remedies, and the new proposed Digital India Act will have a chapter on AI and deepfakes in particular, as per the MoS Rajeev Chandrasekhar.[282]\nIn Europe, the European Union's 2024Artificial Intelligence Act(AI Act) takes a risk-based approach to regulating AI systems, including deepfakes. It establishes categories of \"unacceptable risk,\" \"high risk,\" \"specific/limited or transparency risk\", and \"minimal risk\" to determine the level of regulatory obligations for AI providers and users. However, the lack of clear definitions for these risk categories in the context of deepfakes creates potential challenges for effective implementation. Legal scholars have raised concerns about the classification of deepfakes intended for political misinformation or the creation of non-consensual intimate imagery. Debate exists over whether such uses should always be considered \"high-risk\" AI systems, which would lead to stricter regulatory requirements.[283]\nIn August 2024, the IrishData Protection Commission(DPC) launched court proceedings againstXfor its unlawful use of the personal data of over 60 million EU/EEA users, in order to train its AI technologies, such as itschatbotGrok.[284]\nIn 2016, theDefense Advanced Research Projects Agency(DARPA) launched the Media Forensics (MediFor) program which was funded through 2020.[285]MediFor aimed at automatically spotting digital manipulation in images and videos, includingDeepfakes.[286][287]In the summer of 2018, MediFor held an event where individuals competed to create AI-generated videos, audio, and images as well as automated tools to detect these deepfakes.[288]According to the MediFor program, it established a framework of three tiers of information—digital integrity, physical integrity and semantic integrity—to generate one integrity score in an effort to enable accurate detection of manipulated media.[289]\nIn 2019, DARPA hosted a \"proposers day\" for the Semantic Forensics (SemaFor) program where researchers were driven to prevent viral spread of AI-manipulated media.[290]DARPA and the Semantic Forensics Program were also working together to detect AI-manipulated media through efforts in training computers to utilize common sense, logical reasoning.[290]Built on the MediFor's technologies, SemaFor's attribution algorithms infer if digital media originates from a particular organization or individual, while characterization algorithms determine whether media was generated or manipulated for malicious purposes.[291]In March 2024, SemaFor published an analytic catalog that offers the public access to open-source resources developed under SemaFor.[292][293]\nTheInternational Panel on the Information Environmentwas launched in 2023 as a consortium of over 250 scientists working to develop effective countermeasures to deepfakes and other problems created by perverse incentives in organizations disseminating information via the Internet.[294]\nMedia related toDeepfakeat Wikimedia Commons"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Machine_learning_in_earth_sciences",
        "title": "Machine learning in earth sciences - Wikipedia",
        "content": "Applications ofmachine learning(ML) inearth sciencesincludegeological mapping,gas leakage detectionandgeologicalfeature identification. Machine learning is a subdiscipline ofartificial intelligenceaimed at developing programs that are able to classify, cluster, identify, and analyze vast and complexdata setswithout the need for explicit programming to do so.[1]Earth scienceis the study of the origin, evolution, and future[2]of theEarth. The earth's system can be subdivided into four major components including thesolid earth,atmosphere,hydrosphere, andbiosphere.[3]\nA variety of algorithms may be applied depending on the nature of the task. Some algorithms may perform significantly better than others for particular objectives. For example,convolutional neural networks(CNNs) are good at interpreting images, whilst more generalneural networksmay be used forsoil classification,[4]but can be more computationally expensive to train than alternatives such assupport vector machines. The range of tasks to which ML (includingdeep learning) is applied has been ever-growing in recent decades, as has the development of other technologies such asunmanned aerial vehicles(UAVs),[5]ultra-high resolutionremote sensingtechnology, andhigh-performance computing.[6]This has led to the availability of large high-quality datasets and more advanced algorithms.\nProblems in earth science are often complex.[7]It is difficult to apply well-known and describedmathematical modelsto the natural environment, therefore machine learning is commonly a better alternative for such non-linear problems.[8]Ecologicaldata are commonly non-linear and consist of higher-order interactions, and together with missing data,traditional statisticsmay underperform as unrealistic assumptions such as linearity are applied to the model.[9][10]A number of researchers found that machine learning outperforms traditional statistical models in earth science, such as in characterizing forestcanopystructure,[11]predictingclimate-induced range shifts,[12]and delineating geologic facies.[13]Characterizing forest canopy structure enables scientists to study vegetation response to climate change.[14]Predicting climate-induced range shifts enable policy makers to adopt suitable conversation method to overcome the consequences of climate change.[12]Delineating geologic facies helps geologists to understand thegeologyof an area, which is essential for the development and management of an area.[15]\nIn Earth Sciences, some data are often difficult to access or collect, therefore inferring data from data that are easily available by machine learning method is desirable.[10]For example, geological mapping intropical rainforestsis challenging because the thick vegetation cover and rock outcrops are poorly exposed.[16]Applying remote sensing with machine learning approaches provides an alternative way for rapid mapping without the need of manually mapping in the unreachable areas.[16]\nMachine learning can also reduce the efforts done by experts, as manual tasks of classification and annotation etc. are the bottlenecks in the workflow of the research of earth science.[10]Geological mapping, especially in a vast, remote area is labour, cost and time-intensive with traditional methods.[17]Incorporation of remote sensing and machine learning approaches can provide an alternative solution to eliminate some field mapping needs.[17]\nConsistency and bias-free is also an advantage of machine learning compared to manual works by humans. In research comparing the performance of human and machine learning in the identification ofdinoflagellates, machine learning is found to be not as prone to systematic bias as humans.[18]A recency effect that is present in humans is that the classification often biases towards the most recently recalled classes.[18]In a labelling task of the research, if one kind of dinoflagellates occurs rarely in the samples, then expert ecologists commonly will not classify it correctly.[18]The systematic bias strongly deteriorate the classification accuracies of humans.[18]\nThe extensive usage of machine learning in various fields has led to a wide range of algorithms of learning methods being applied. Choosing the optimal algorithm for a specific purpose can lead to a significant boost in accuracy:[19]for example, the lithological mapping of gold-bearing granite-greenstone rocks in Hutti, India with AVIRIS-NG hyperspectral data, shows more than 10% difference in overall accuracy between usingsupport vector machines(SVMs) andrandom forest.[20]\nSome algorithms can also reveal hidden important information:white boxmodels aretransparent models, the outputs of which can be easily explained, whileblack boxmodels are the opposite.[19]For example, although an SVM yielded the best result in landslide susceptibility assessment accuracy, the result cannot be rewritten in the form of expert rules that explain how and why an area was classified as that specific class.[7]In contrast,decision treesare transparent and easily understood, and the user can observe and fix thebiasif any is present in such models.[7]\nIf computational resource is a concern, more computationally demanding learning methods such as deep neural networks are less preferred, despite the fact that they may outperform other algorithms, such as in soil classification.[4]\nGeological or lithological mappingproduces maps showing geological features and geological units. Mineralprospectivity mappingutilizes a variety of datasets such as geological maps andaeromagnetic imageryto produce maps that are specialized for mineral exploration.[21]Geological, lithological, and mineral prospectivity mapping can be carried out by processing data with ML techniques, with the input of spectral imagery obtained from remote sensing andgeophysicaldata.[22]Spectral imagingis also used – the imaging of wavelength bands in the electromagnetic spectrum, while conventional imaging captures three wavelength bands (red, green, blue) in the electromagnetic spectrum.[23]\nRandom forestsand SVMs are some algorithms commonly used with remotely-sensed geophysical data, while Simple Linear Iterative Clustering-Convolutional Neural Network (SLIC-CNN)[5]and Convolutional Neural Networks (CNNs)[17]are commonly applied to aerial imagery. Large scale mapping can be carried out with geophysical data from airborne and satellite remote sensing geophysical data,[20]and smaller-scale mapping can be carried out with images from Unmanned Aerial Vehicles (UAVs) for higher resolution.[5]\nVegetation cover is one of the major obstacles for geological mapping with remote sensing, as reported in various research, both in large-scale and small-scale mapping. Vegetation affects the quality of spectral images,[22]or obscures the rock information in aerial images.[5]\nRandom Forest,\nSupport Vector Machine (SVM)\n(1) Map generated with remote sensing data only has a 52.7% accuracy when compared to the geological map, but several new possible lithological units are identified\n(2) Map generated with remote sensing data and spatial constraints has a 78.7% accuracy but no new possible lithological units are identified\ngeophysical data\nMorocco\nfrequency electromagnetic, radiometric measurements, ground gravity measurements\nLiaoning Province, China\nRemote Predictive Mapping (RPM)\nLandsatReflectance, High-Resolution Digital Elevation Data\nNorthwest Territories, Canada\nRandom Forest\nLandslidesusceptibility refers to the probability of landslide of a certain geographical location, which is dependent on local terrain conditions.[26]Landslide susceptibility mapping can highlight areas prone to landslide risks, which is useful for urban planning and disaster management.[7]Such datasets for ML algorithms usually include topographic information, lithological information, satellite images, etc., and some may include land use, land cover, drainage information, and vegetation cover[7][27][28][29]according to the study requirements. As usual, for training an ML model for landslide susceptibility mapping, training and testing datasets are required.[7]There are two methods of allocating datasets for training and testing: one is to randomly split the study area for the datasets; another is to split the whole study into two adjacent parts for the two datasets. To test classification models, the common practice is to split the study area randomly;[7][30]however, it is more useful if the study area can be split into two adjacent parts so that an automation algorithm can carry out mapping of a new area with the input of expert-processed data of adjacent land.[7]\nDecision Trees,Logistic Regression\nDiscontinuitiessuch asfault planesandbedding planeshave important implications in civil engineering.[31]Rock fractures can be recognized automatically by machine learning throughphotogrammetricanalysis, even with the presence of interfering objects such as vegetation.[32]In ML training for classifying images,data augmentationis a common practice to avoidoverfittingand increase the training dataset size and variability.[32]For example, in a study of rock fracture recognition, 68 images for training and 23 images for testing were prepared via random splitting.[32]Data augmentationwas performed, increasing the training dataset size to 8704 images by flipping and random cropping.[32]The approach was able to recognize rock fractures accurately in most cases.[32]Both thenegative prediction value (NPV)and thespecificitywere over 0.99.[32]This demonstrated the robustness of discontinuity analyses with machine learning.\nSeoul, Korea and Jeongseon-gun, Gangwon-do, Korea\nQuantifying carbon dioxide leakage from ageological sequestration sitehas gained increased attention as the public is interested in whether carbon dioxide is stored underground safely and effectively.[33]Carbon dioxide leakage from a geological sequestration site can be detected indirectly with the aid of remote sensing and anunsupervised clustering algorithmsuch as Iterative Self-Organizing Data Analysis Technique (ISODATA).[34]The increase in soil CO2concentration causes a stress response for plants by inhibiting plant respiration, as oxygen is displaced by carbon dioxide.[35]The vegetation stress signal can be detected with theNormalized Difference Red Edge Index(NDRE).[35]Thehyperspectral imagesare processed by the unsupervised algorithm, clustering pixels with similar plant responses.[35]The hyperspectral information in areas with known CO2leakage is extracted so that areas with leakage can be matched with the clustered pixels with spectral anomalies.[35]Although the approach can identify CO2leakage efficiently, there are some limitations that require further study.[35]The NDRE may not be accurate due to reasons like higher chlorophyll absorption, variation in vegetation, and shadowing effects; therefore, some stressed pixels can be incorrectly classed as healthy.[35]Seasonality, groundwater table height may also affect the stress response to CO2of the vegetation.[35]\nTherock mass rating (RMR)[36]system is a widely adopted rock mass classification system by geomechanical means with the input of six parameters. The amount of water inflow is one of the inputs of the classification scheme, representing the groundwater condition. Quantification of the water inflow in the faces of a rock tunnel was traditionally carried out by visual observation in the field, which is labour and time-consuming, and fraught with safety concerns.[37]Machine learning can determine water inflow by analyzing images taken on the construction site.[37]The classification of the approach mostly follows the RMR system, but combining damp and wet states, as it is difficult to distinguish only by visual inspection.[37][36]The images were classified into the non-damaged state, wet state, dripping state, flowing state, and gushing state.[37]The accuracy of classifying the images was approximately 90%.[37]\nThe most popular cost-effective method od soil investigation method iscone penetration testing (CPT).[38]The test is carried out by pushing a metallic cone through the soil: the force required to push at a constant rate is recorded as a quasi-continuous log.[4]Machine learning can classify soil with the input of CPT data.[4]In an attempt to classify with ML, there are two tasks required to analyze the data, namely segmentation and classification.[4]Segmentation can be carried out with theConstraint Clustering and Classification(CONCC) algorithm to split a single series data into segments.[4]Classification can then be carried out by algorithms such as decision trees, SVMs, or neural networks.[4]\nExposed geological structures such asanticlines,ripple marks, andxenolithscan be identified automatically withdeep learningmodels.[39]Research has demonstrated that three-layer CNNs andtransfer learninghave strong accuracy (about 80% and 90% respectively), while others likek-nearest neighbors(k-NN), regular neural nets, andextreme gradient boosting(XGBoost) have low accuracies (ranging from 10% - 30%).[39]Thegrayscale imagesand colour images were both tested, with the accuracy difference being little, implying that colour is not very important in identifying geological structures.[39]\nEarthquake warning systemsare often vulnerable to local impulsive noise, therefore giving out false alerts.[40]False alerts can be eliminated by discriminating the earthquake waveforms from noise signals with the aid of ML methods. The method consists of two parts, the first being unsupervised learning with agenerative adversarial network(GAN) to learn and extract features of first-arrivalP-waves, and the second being use of a random forest to discriminate P-waves. This approach achieved 99.2% in recognizing P-waves, and can avoid false triggers by noise signals with 98.4% accuracy.[40]\nEarthquakes can be produced in a laboratory settings to mimic real-world ones. With the help of machine learning, the patterns of acoustic signals as precursors for earthquakes can be identified. Predicting the time remaining before failure was demonstrated in a study with continuous acoustic time series data recorded from a fault. The algorithm applied was a random forest, trained with a set of slip events, performing strongly in predicting the time to failure. It identified acoustic signals to predict failures, with one of them being previously unidentified. Although this laboratory earthquake is not as complex as a natural one, progress was made that guides future earthquake prediction work.[41]\nReal-timestreamflowdata is integral for decision making (e.g., evacuations, or regulation of reservoir water levels during flooding).[42]Streamflow data can be estimated by data provided bystream gauges, which measure the water level of a river. However, water and debris from flooding may damage stream gauges, resulting in lack of essential real-time data. The ability of machine learning to infer missing data[10]enables it to predict streamflow with both historical stream gauge data and real-time data.\nStreamflow Hydrology Estimate using Machine Learning (SHEM) is a model that can serve this purpose. To verify its accuracies, the prediction result was compared with the actual recorded data, and the accuracies were found to be between 0.78 and 0.99.\nAn adequate amount of training and validation data is required for machine learning.[10]However, some very useful products like satellite remote sensing data only have decades of data since the 1970s. If one is interested in the yearly data, then only less than 50 samples are available.[44]Such amount of data may not be adequate. In a study of automatic classification of geological structures, the weakness of the model is the small training dataset, even though with the help of data augmentation to increase the size of the dataset.[39]Another study of predicting streamflow found that the accuracies depend on the availability of sufficient historical data, therefore sufficient training data determine the performance of machine learning.[43]Inadequate training data may lead to a problem called overfitting. Overfitting causes inaccuracies in machine learning[45]as the model learns about the noise and undesired details.\nMachine learning cannot carry out some of the tasks as a human does easily. For example, in the quantification of water inflow in rock tunnel faces by images for Rock Mass Rating system (RMR),[37]the damp and the wet state was not classified by machine learning because discriminating the two only by visual inspection is not possible. In some tasks, machine learning may not able to fully substitute manual work by a human.\nIn many machine learning algorithms, for example, Artificial Neural Network (ANN), it is considered as 'black box' approach as clear relationships and descriptions of how the results are generated in the hidden layers are unknown.[46]'White-box' approach such as decision tree can reveal the algorithm details to the users.[47]If one wants to investigate the relationships, such 'black-box' approaches are not suitable[48]. However, the performances of 'black-box' algorithms are usually better.[49]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Generative_artificial_intelligence",
        "title": "Generative artificial intelligence - Wikipedia",
        "content": "Generative artificial intelligence(Generative AI,GenAI,[1]orGAI) is a subfield ofartificial intelligencethat usesgenerative modelsto producetext,images,videos,audio,software codeor other forms of data.[2][3][4]These modelslearnthe underlying patterns and structures of theirtraining dataand use them to produce new data[5][6]based on the input, which often comes in the form of natural languageprompts.[7][8]\nGenerative AI tools have become more common since theAI boomin the 2020s. This boom was made possible by improvements intransformer-baseddeepneural networks, particularlylarge language models(LLMs). Major tools includechatbotssuch asChatGPT,Copilot,Gemini,Claude,Grok, andDeepSeek;text-to-imagemodels such asStable Diffusion,Midjourney, andDALL-E; andtext-to-videomodels such asVeoandSora.[9][10][11][12][13]Technology companies developing generative AI includeOpenAI,xAI,Anthropic,Meta AI,Microsoft,Google,DeepSeek, andBaidu.[7][14][15]\nGenerative AI is used across many industries, including software development,[16]healthcare,[17]finance,[18]entertainment,[19]customer service,[20]sales and marketing,[21]art, writing,[22]fashion,[23]and product design.[24]The production of generative AI systems requires large scale data centers using specialized chips which require a lot of electricity for processing and water for cooling.[25]\nGenerative AI has raised many ethical questions and governance challenges as it can be used forcybercrime, or to deceive or manipulate people throughfake newsordeepfakes.[26][27]Even if used ethically, it may lead tomass replacement of human jobs.[28]The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works.[29]The material and energy intensity of the AI systems has raised concerns about theenvironmental impact of AI, especially in light of the challenges created by theenergy transition.\nThe first example of an algorithmically generated media is likely theMarkov chain. Markov chains have long been used to model natural languages since their development by Russian mathematicianAndrey Markovin the early 20th century. Markov published his first paper on the topic in 1906,[30][31]and analyzed the pattern of vowels and consonants in the novelEugeny Oneginusing Markov chains. Once a Markov chain is trained on atext corpus, it can then be used as a probabilistic text generator.[32][33]\nComputers were needed to go beyond Markov chains. By the early 1970s,Harold Cohenwas creating and exhibiting generative AI works created byAARON, the computer program Cohen created to generate paintings.[34]\nThe terms generativeAI planningor generative planning were used in the 1980s and 1990s to refer toAI planningsystems, especiallycomputer-aided process planning, used to generate sequences of actions to reach a specified goal.[35][36]Generative AI planning systems usedsymbolic AImethods such asstate space searchandconstraint satisfactionand were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use,[37]process plans for manufacturing[35]and decision plans such as in prototype autonomous spacecraft.[38]\nSince its inception, the field ofmachine learninghas used bothdiscriminative modelsandgenerative modelsto model and predict data. Beginning in the late 2000s, the emergence ofdeep learningdrove progress, and research inimage classification,speech recognition,natural language processingand other tasks.Neural networksin this era were typically trained asdiscriminativemodels due to the difficulty of generative modeling.[39]\nIn 2014, advancements such as thevariational autoencoderandgenerative adversarial networkproduced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also entire images.\nIn 2017, theTransformernetwork enabled advancements in generative models compared to olderlong short-term memory(LSTM) models,[40]leading to the firstgenerative pre-trained transformer(GPT), known asGPT-1, in 2018.[41]This was followed in 2019 byGPT-2, which demonstrated the ability to generalize unsupervised to many different tasks as aFoundation model.[42]\nThe new generative models introduced during this period allowed for large neural networks to be trained usingunsupervised learningorsemi-supervised learning, rather than thesupervised learningtypical of discriminative models. Unsupervised learning removed the need for humans tomanually label data, allowing for larger networks to be trained.[43]\nIn March 2020, the release of15.ai, a freeweb applicationcreated by an anonymousMITresearcher that could generate convincing character voices using minimal training data, marked one of the earliest popular use cases of generative AI.[44]The platform is credited as the first mainstream service to popularize AI voice cloning (audio deepfakes) inmemesandcontent creation, influencing subsequent developments invoice AI technology.[45][46]\nIn 2021, the emergence ofDALL-E, atransformer-based pixel generative model, marked an advance in AI-generated imagery.[47]This was followed by the releases ofMidjourneyandStable Diffusionin 2022, which further democratized access to high-qualityartificial intelligence artcreation fromnatural language prompts.[48]These systems demonstrated unprecedented capabilities in generating photorealistic images, artwork, and designs based on text descriptions, leading to widespread adoption among artists, designers, and the general public.\nIn late 2022, the public release ofChatGPTrevolutionized the accessibility andapplication of generative AIfor general-purpose text-based tasks.[49]The system's ability toengage in natural conversations,generate creative content, assist with coding, and perform various analytical tasks captured global attention and sparked widespread discussion about AI's potential impact onwork,education, andcreativity.[50][51]\nIn March 2023,GPT-4's release represented another jump in generative AI capabilities. A team fromMicrosoft Researchcontroversially argued that it \"could reasonably be viewed as an early (yet still incomplete) version of anartificial general intelligence(AGI) system.\"[52]However, this assessment was contested by other scholars who maintained that generative AI remained \"still far from reaching the benchmark of 'general human intelligence'\" as of 2023.[53]Later in 2023,MetareleasedImageBind, an AI model combining multiplemodalitiesincluding text, images, video, thermal data, 3D data, audio, and motion, paving the way for more immersive generative AI applications.[54]\nIn December 2023,GoogleunveiledGemini, a multimodal AI model available in four versions: Ultra, Pro, Flash, and Nano.[55]The company integrated Gemini Pro into itsBard chatbotand announced plans for \"Bard Advanced\" powered by the larger Gemini Ultra model.[56]In February 2024, Google unified Bard and Duet AI under the Gemini brand, launching a mobile app onAndroidand integrating the service into the Google app oniOS.[57]\nIn March 2024,Anthropicreleased theClaude3 family of large language models, including Claude 3 Haiku, Sonnet, and Opus.[58]The models demonstrated significant improvements in capabilities across various benchmarks, with Claude 3 Opus notably outperforming leading models from OpenAI and Google.[59]In June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated improved performance compared to the larger Claude 3 Opus, particularly in areas such as coding, multistep workflows, and image analysis.[60]\nAsia–Pacificcountries are significantly more optimistic than Western societies about generative AI and show higher adoption rates. Despite expressing concerns about privacy and the pace of change, in a 2024 survey, 68% of Asia-Pacific respondents believed that AI was having a positive impact on the world, compared to 57% globally.[61]According to a survey bySASand Coleman Parkes Research,Chinain particular has emerged as a global leader in generative AI adoption, with 83% of Chinese respondents using the technology, exceeding both the global average of 54% and the U.S. rate of 65%. This leadership is further evidenced by China'sintellectual propertydevelopments in the field, with aUNreport revealing that Chinese entities filed over 38,000 generative AIpatentsfrom 2014 to 2023, substantially surpassing the United States in patent applications.[62]A 2024 survey on the Chinese social app Soul reported that 18% of respondents born after 2000 used generative AI \"almost every day\", and that over 60% of respondents like or love AI-generated content, while less than 3% dislike or hate it.[63]\nBy mid 2025, despite continued consumer growth, many companies were increasingly abandoning generative AI pilot projects as they had difficulties with integration, data quality and unmet returns, leading analysts to characterize the period as entering theGartner hype cycle's \"trough of disillusionment\" phase.[64][65]\nNotable types of generative AI models includegenerative pre-trained transformers(GPTs),generative adversarial networks(GANs), andvariational autoencoders(VAEs). Generative AI systems aremultimodalif they can process multiple types of inputs or generate multiple types of outputs.[66]For example,GPT-4ocan both process and generate text, images and audio.[67]\nGenerative AI has made its appearance in a wide variety of industries, radically changing the dynamics of content creation, analysis, and delivery. In healthcare,[68]for instance, generative AI acceleratesdrug discoveryby creating molecular structures with target characteristics[69]and generatesradiologyimages for training diagnostic models. This ability not only enables faster and cheaper development but also enhances medical decision-making. In finance, generative AI services help create datasets and automate reports using natural language. It automates content creation, produces synthetic financial data, and tailors customer communications. It also powers chatbots and virtual agents. Collectively, these technologies enhance efficiency, reduceoperational costs, and support data-driven decision-making in financial institutions.[70]The media industry makes use of generative AI for numerous creative activities such as music composition, scriptwriting, video editing, and digital art. The educational sector is impacted as well, since the tools make learning personalized through creating quizzes, study aids, and essay composition. Both the teachers and the learners benefit from AI-based platforms that suit various learning patterns.[71]In the educational field, inColombia, student use ofMeta's generative AI programs resulted in a decline in scores.[72]\nGenerative AI systems trained on words orword tokensincludeGPT-3,GPT-4,GPT-4o,LaMDA,LLaMA,BLOOM,Gemini,Claudeand others (seeList of large language models). They are capable ofnatural language processing,machine translation, andnatural language generationand can be used asfoundation modelsfor other tasks.[74]Data sets includeBookCorpus,Wikipedia, and others (seeList of text corpora).\nIn addition tonatural languagetext, large language models can be trained onprogramming languagetext, allowing them to generatesource codefor newcomputer programs.[75]Examples includeOpenAI Codex,Tabnine,GitHub Copilot,Microsoft Copilot, andVS CodeforkCursor.[76]\nSome AI assistants help candidates cheat during onlinecoding interviewsby providing code, improvements, and explanations. Their clandestine interfaces minimize the need for eye movements that would expose cheating to the interviewer.[77]\nProducing high-quality visual art is a prominent application of generative AI.[78]Generative AI systems trained on sets of images withtext captionsincludeImagen,DALL-E,Midjourney,Adobe Firefly,FLUX.1, Stable Diffusion and others (seeArtificial intelligence art,Generative art, andSynthetic media). They are commonly used fortext-to-imagegeneration andneural style transfer.[79]Datasets includeLAION-5Band others (seeList of datasets in computer vision and image processing).\nGenerative AI can also be trained extensively on audio clips to produce natural-soundingspeech synthesisandtext-to-speechcapabilities. An early pioneer in this field was15.ai, launched in March 2020, which demonstrated the ability to clone character voices using as little as 15 seconds of training data.[80]The website gained widespread attention for its ability to generate emotionally expressive speech for various fictional characters, though it was later taken offline in 2022 due to copyright concerns.[81][82][83]Commercial alternatives subsequently emerged, includingElevenLabs' context-aware synthesis tools andMeta Platform's Voicebox.[84]\nGenerative AI systems such asMusicLM[85]and MusicGen[86]can also be trained on the audio waveforms of recorded music along with text annotations, in order to generate new musical samples based on text descriptions such asa calming violin melody backed by a distorted guitar riff.\nAudio deepfakesof musiclyricshave been generated, like the song Savages, which used AI to mimic rapperJay-Z's vocals. Music artist's instrumentals and lyrics are copyrighted but their voices are not protected from regenerative AI yet, raising a debate about whether artists should get royalties from audio deepfakes.[87]\nGenerative AI trained on annotated video cangeneratetemporally-coherent, detailed andphotorealisticvideo clips. Examples includeSorabyOpenAI,[12]Runway,[88]Make-A-Video byMeta Platformsand the open source LTX Video byLightricks.[89][13][90]\nGenerative AI can also be trained on the motions of aroboticsystem to generate new trajectories formotion planningornavigation. For example, UniPi from Google Research uses prompts like\"pick up blue bowl\"or\"wipe plate with yellow sponge\"to control movements of a robot arm.[91]Multimodalvision-language-action modelssuch as Google's RT-2 can perform rudimentary reasoning in response to user prompts and visual input, such as picking up a toydinosaurwhen given the promptpick up the extinct animalat a table filled with toy animals and other objects.[92]\nArtificially intelligentcomputer-aided design(CAD) can use text-to-3D, image-to-3D, and video-to-3D toautomate3D modeling.[93]AI-basedCAD librariescould also be developed usinglinkedopen dataofschematicsanddiagrams.[94]AI CADassistantsare used as tools to help streamline workflow.[95]\nGenerative AI models are used to powerchatbotproducts such asChatGPT,programming toolssuch asGitHub Copilot,[96]text-to-imageproducts such as Midjourney, and text-to-video products such asRunwayGen-2.[97]Generative AI features have been integrated into a variety of existing commercially available products such asMicrosoft Office(Microsoft Copilot),[98]Google Photos,[99]and theAdobe Suite(Adobe Firefly).[100]Many generative AI models are also available asopen-source software, including Stable Diffusion and the LLaMA[101]language model.\nSmaller generative AI models with up to a few billion parameters can run onsmartphones, embedded devices, andpersonal computers. For example, LLaMA-7B (a version with 7 billion parameters) can run on aRaspberry Pi 4[102]and one version of Stable Diffusion can run on aniPhone 11.[103]\nLarger models with tens of billions of parameters can run onlaptopordesktop computers. To achieve an acceptable speed, models of this size may requireacceleratorssuch as theGPUchips produced byNVIDIAandAMDor the Neural Engine included inApple siliconproducts. For example, the 65 billion parameter version of LLaMA can be configured to run on a desktop PC.[104]\nThe advantages of running generative AI locally include protection ofprivacyandintellectual property, and avoidance ofrate limitingandcensorship. Thesubredditr/LocalLLaMA in particular focuses on usingconsumer-grade gaminggraphics cards[105]through such techniques ascompression. That forum is one of only two sourcesAndrej Karpathytrusts forlanguage model benchmarks.[106]Yann LeCunhas advocated open-source models for their value tovertical applications[107]and for improvingAI safety.[108]\nLanguage models with hundreds of billions of parameters, such as GPT-4 orPaLM, typically run ondatacentercomputers equipped with arrays ofGPUs(such as NVIDIA'sH100) orAI acceleratorchips (such as Google'sTPU). These very large models are typically accessed ascloudservices over the Internet.\nIn 2022, theUnited States New Export Controls on Advanced Computing and Semiconductors to Chinaimposed restrictions on exports to China ofGPUand AI accelerator chips used for generative AI.[109]Chips such as the NVIDIA A800[110]and theBiren TechnologyBR104[111]were developed to meet the requirements of the sanctions.\nThere is free software on the market capable of recognizing text generated by generative artificial intelligence (such asGPTZero), as well as images, audio or video coming from it.[112]Potential mitigation strategies fordetecting generative AI contentincludedigital watermarking,content authentication,information retrieval, andmachine learning classifier models.[113]Despite claims of accuracy, both free and paid AI text detectors have frequently produced false positives, mistakenly accusing students of submitting AI-generated work.[114][115]\nGenerative adversarial networks(GANs) are an influential generative modeling technique. GANs consist of two neural networks—the generator and the discriminator—trained simultaneously in a competitive setting. The generator createssynthetic databy transforming random noise into samples that resemble the training dataset. The discriminator is trained to distinguish the authentic data from synthetic data produced by the generator.[116]The two models engage in aminimaxgame: the generator aims to create increasingly realistic data to \"fool\" the discriminator, while the discriminator improves its ability to distinguish real from fake data. This continuous training setup enables the generator to produce high-quality and realistic outputs.[117]\nVariational autoencoders(VAEs) are deep learning models that probabilistically encode data. They are typically used for tasks such asnoise reductionfrom images,data compression, identifying unusual patterns, andfacial recognition. Unlikestandard autoencoders, which compress input data into a fixed latent representation, VAEs model thelatent spaceas a probability distribution,[118]allowing for smooth sampling and interpolation between data points. The encoder (\"recognition model\") maps input data to a latent space, producing means and variances that define a probability distribution. The decoder (\"generative model\") samples from this latent distribution and attempts to reconstruct the original input. VAEs optimize a loss function that includes both the reconstruction error and aKullback–Leibler divergenceterm, which ensures the latent space follows a known prior distribution. VAEs are particularly suitable for tasks that require structured but smooth latent spaces, although they may create blurrier images than GANs. They are used for applications like image generation, data interpolation andanomaly detection.\nTransformers became the foundation for many powerful generative models, most notably thegenerative pre-trained transformer(GPT) series developed by OpenAI. They marked a major shift in natural language processing by replacing traditionalrecurrentandconvolutionalmodels.[119]This architecture allows models to process entire sequences simultaneously and capture long-range dependencies more efficiently. Theself-attention mechanismenables the model to capture the significance of every word in a sequence when predicting the subsequent word, thus improving its contextual understanding. Unlike recurrent neural networks, transformers process all the tokens in parallel, which improves the training efficiency and scalability. Transformers are typically pre-trained on enormous corpora in aself-supervisedmanner, prior to beingfine-tuned.\nIn the United States, a group of companies including OpenAI, Alphabet, and Meta signed a voluntary agreement with theBiden administrationin July 2023 to watermark AI-generated content.[120]In October 2023,Executive Order 14110applied theDefense Production Actto require all US companies to report information to the federal government when training certain high-impact AI models.[121][122]\nIn the European Union, the proposedArtificial Intelligence Actincludes requirements to disclose copyrighted material used to train generative AI systems, and to label any AI-generated output as such.[123][124]\nIn China, theInterim Measures for the Management of Generative AI Servicesintroduced by theCyberspace Administration of Chinaregulates any public-facing generative AI. It includes requirements to watermark generated images or videos, regulations on training data and label quality, restrictions on personal data collection, and a guideline that generative AI services must \"adhere to socialist core values\".[125][126]\nGenerative AI systems such asChatGPTandMidjourneyare trained on large, publicly available datasets that include copyrighted works. AI developers have argued that such training is protected underfair use, while copyright holders have argued that it infringes their rights.[127]\nProponents of fair use training have argued that it is atransformative useand does not involve making copies of copyrighted works available to the public.[127]Critics have argued that image generators such asMidjourneycan create nearly-identical copies of some copyrighted images,[128]and that generative AI programs compete with the content they are trained on.[129]\nAs of 2024, several lawsuits related to the use of copyrighted material in training are ongoing.Getty Imageshas suedStability AIover the use of its images to trainStable Diffusion.[130]Both theAuthors GuildandThe New York Timeshave suedMicrosoftandOpenAIover the use of their works to trainChatGPT.[131][132]\nA separate question is whether AI-generated works can qualify for copyright protection. TheUnited States Copyright Officehas ruled that works created by artificial intelligence without any human input cannot be copyrighted, because they lack human authorship.[133]Some legal professionals have suggested thatNaruto v. Slater(2018), in which theU.S. 9th Circuit Court of Appealsheld thatnon-humanscannot be copyright holders ofartistic works, could be a potential precedent in copyright litigation over works created by generative AI.[134]However, the office has also begun taking public input to determine if these rules need to be refined for generative AI.[135]\nIn January 2025, theUnited States Copyright Office(USCO) released extensive guidance regarding the use of AI tools in the creative process, and established that \"...generative AI systems also offer tools that similarly allow users to exert control. [These] can enable the user to control the selection and placement of individual creative elements. Whether such modifications rise to the minimum standard of originality required underFeistwill depend on a case-by-case determination. In those cases where they do, the output should be copyrightable\"[136]Subsequently, the USCO registered the first visual artwork to be composed of entirely AI-generated materials, titled \"A Single Piece of American Cheese\".[137]\nThe development of generative AI has raised concerns fromgovernments, businesses, and individuals, resulting in protests, legal actions, calls topause AI experiments, and actions by multiple governments. In a July 2023 briefing of theUnited Nations Security Council,Secretary-GeneralAntónio Guterresstated \"Generative AI has enormous potential for good and evil at scale\", that AI may \"turbocharge global development\" and contribute between $10 and $15 trillion to the global economy by 2030, but that its malicious use \"could cause horrific levels of death and destruction, widespread trauma, and deep psychological damage on an unimaginable scale\".[138]In addition, generative AI has a significantcarbon footprint.[139][140]\nGenerative AI can be used to generate and modify academic prose, to paraphrasing sources, and translate languages. The use of generative AI in a classroom setting can be a form ofacademic plagiarism. Some schools have banned ChatGPT and similar tools.[141][142][better source needed]\nA commonly proposed use for teachers is grading and giving feedback. Companies like Pearson and ETS use AI to score grammar, mechanics, usage, and style, but not for main ideas or overall structure.[141]The National Council of Teachers of English says machine scoring makes students feel their writing isn't worth reading.[143]AI scoring has also given unfair results for students from different ethnic backgrounds.[144]\nFrom the early days of the development of AI, there have been arguments put forward byELIZAcreatorJoseph Weizenbaumand others about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculations and qualitative, value-based judgements.[146]In April 2023, it was reported that image generation AI has resulted in 70% of the jobs for video game illustrators in China being lost.[147][148]In July 2023, developments in generative AI contributed to the2023 Hollywood labor disputes.Fran Drescher, president of theScreen Actors Guild, declared that \"artificial intelligence poses an existential threat to creative professions\" during the2023 SAG-AFTRA strike.[149]Voice generation AI has been seen as a potential challenge to thevoice actingsector.[150][151]\nThe intersection of AI and employment concerns among underrepresented groups globally remains a critical facet. While AI promises efficiency enhancements and skill acquisition, concerns about job displacement and biased recruiting processes persist among these groups, as outlined in surveys byFast Company. To leverage AI for a more equitable society, proactive steps encompass mitigating biases, advocating transparency, respecting privacy and consent, and embracing diverse teams and ethical considerations. Strategies involve redirecting policy emphasis on regulation, inclusive design, and education's potential for personalized teaching to maximize benefits while minimizing harms.[152]\nGenerative AI models can reflect and amplify anycultural biaspresent in the underlying data. For example, a language model might assume that doctors and judges are male, and that secretaries or nurses are female, if those biases are common in the training data.[153]Similarly, an image model prompted with the text \"a photo of a CEO\" might disproportionately generate images of white male CEOs,[154]if trained on a racially biased data set. A number of methods for mitigating bias have been attempted, such as altering input prompts[155]and reweighting training data.[156]\nDeepfakes (aportmanteauof \"deep learning\" and \"fake\"[157]) are AI-generated media that take a person in an existing image or video and replace them with someone else's likeness usingartificial neural networks.[158]Deepfakes have garnered widespread attention and concerns for their uses indeepfake celebrity pornographic videos,revenge porn,fake news,hoaxes, healthdisinformation,financial fraud, and covertforeign election interference.[159][160][161][162][163][164][165]This has elicited responses from both industry and government to detect and limit their use.[166][167]\nIn July 2023, the fact-checking companyLogicallyfound that the popular generative AI modelsMidjourney,DALL-E 2andStable Diffusionwould produce plausible disinformation images when prompted to do so, such as images ofelectoral fraudin the United States and Muslim women supporting India'sHindu nationalistBharatiya Janata Party.[168][169]\nIn April 2024, a paper proposed to useblockchain(distributed ledgertechnology) to promote \"transparency, verifiability, and decentralization in AI development and usage\".[170]\nInstances of users abusing software to generate controversial statements in the vocal style of celebrities, public officials, and other famous individuals have raised ethical concerns over voice generation AI.[171][172][173][174][175][176]In response, companies such as ElevenLabs have stated that they would work on mitigating potential abuse through safeguards andidentity verification.[177]\nConcerns and fandoms have spawned fromAI-generated music. The same software used to clone voices has been used on famous musicians' voices to create songs that mimic their voices, gaining both tremendous popularity and criticism.[178][179][180]Similar techniques have also been used to create improved quality or full-length versions of songs that have been leaked or have yet to be released.[181]\nGenerative AI has also been used to create new digital artist personalities, with some of these receiving enough attention to receive record deals at major labels.[182]The developers of these virtual artists have also faced their fair share of criticism for their personified programs, including backlash for \"dehumanizing\" an artform, and also creating artists which create unrealistic or immoral appeals to their audiences.[183]\nMany websites that allowexplicit AI generated images or videoshave been created,[184]and this has been used to create illegal content, such asrape,child sexual abuse material,[185][186]necrophilia, andzoophilia.\nGenerative AI's ability to create realistic fake content has been exploited in numerous types of cybercrime, includingphishingscams.[187]Deepfakevideo and audio have been used to create disinformation and fraud. In 2020, former Googleclick fraudczarShuman Ghosemajumderargued that once deepfake videos become perfectly realistic, they would stop appearing remarkable to viewers, potentially leading to uncritical acceptance of false information.[188]Additionally,large language modelsand other forms of text-generation AI have been used to create fake reviews ofe-commercewebsites to boost ratings.[189]Cybercriminals have created large language models focused on fraud, including WormGPT and FraudGPT.[190]\nA 2023 study showed that generative AI can be vulnerable to jailbreaks,reverse psychologyandprompt injectionattacks, enabling attackers to obtain help with harmful requests, such as for craftingsocial engineeringandphishing attacks.[191]Additionally, other researchers have demonstrated that open-source models can befine-tunedto remove their safety restrictions at low cost.[192]\nGenerative AI has been noted for its use bystate-sponsored propaganda campaignsininformation laundering. According to a 2025 report byGraphika, generative AI is used to launder articles from Chinesestate mediasuch asChina Global Television Networkthrough various social media sites in an attempt to disguise the articles' origin.[193]\nTrainingfrontier AI modelsrequires an enormous amount of computing power. Usually onlyBig Techcompanies have the financial resources to make such investments. Smaller start-ups such asCohereandOpenAIend up buying access todata centersfromGoogleandMicrosoftrespectively.[194]\nAI has a significant carbon footprint due to growing energy consumption from both training and usage.[139][140]Scientists and journalists have expressed concerns about the environmental impact that the development and deployment of generative models are having: high CO2emissions,[195][196][197]large amounts of freshwater used for data centers,[198][199]and high amounts of electricity usage.[196][200][201]There is also concern that these impacts may increase as these models are incorporated into widely used search engines such as Google Search and Bing,[200]aschatbotsand other applications become more popular,[199][200]and as models need to be retrained.[200]\nThe carbon footprint of generative AI globally is estimated to be growing steadily, with potential annual emissions ranging from 18.21 to 245.94 million tons of CO2by 2035,[202]with the highest estimates for 2035 nearing the impact of the United Statesbeef industryon emissions (currently estimated to emit 257.5 million tons annually as of 2024).[203]\nProposed mitigation strategies include factoring potential environmental costs prior to model development or data collection,[195]increasing efficiency of data centers to reduce electricity/energy usage,[197][200][201]building more efficientmachine learning models,[196][198][199]minimizing the number of times that models need to be retrained,[197]developing a government-directed framework for auditing the environmental impact of these models,[197][198]regulating for transparency of these models,[197]regulating their energy and water usage,[198]encouraging researchers to publish data on their models' carbon footprint,[197][200]and increasing the number of subject matter experts who understand both machine learning and climate science.[197]\nThe New York Timesdefinesslopas analogous tospam: \"shoddy or unwanted A.I. content in social media, art, books, and ... in search results.\"[204]Journalists have expressed concerns about the scale of low-quality generated content with respect to social media content moderation,[205]the monetary incentives from social media companies to spread such content,[205][206]false political messaging,[206]spamming of scientific research paper submissions,[207]increased time and effort to find higher quality or desired content on the Internet,[208]the indexing of generated content by search engines,[209]and on journalism itself.[210]\nA paper published by researchers at Amazon Web Services AI Labs found that over 57% of sentences from a sample of over 6 billion sentences fromCommon Crawl, a snapshot of web pages, weremachine translated. Many of these automated translations were seen as lower quality, especially for sentences that were translated into at least three languages. Many lower-resource languages (ex.Wolof,Xhosa) were translated across more languages than higher-resource languages (ex. English, French).[211][212]\nIn September 2024,Robyn Speer, the author of wordfreq, an open source database that calculated word frequencies based on text from the Internet, announced that she had stopped updating the data for several reasons: high costs for obtaining data fromRedditandTwitter, excessive focus on generative AI compared to other methods in thenatural language processingcommunity, and that \"generative AI has polluted the data\".[213]\nThe adoption of generative AI tools led to an explosion of AI-generated content across multiple domains. A study fromUniversity College Londonestimated that in 2023, more than 60,000 scholarly articles—over 1% of all publications—were likely written with LLM assistance.[214]According toStanford University's Institute for Human-Centered AI, approximately 17.5% of newly published computer science papers and 16.9% of peer review text now incorporate content generated by LLMs.[215]Many academic disciplines have concerns about the factual reliability of academic content generated by AI.[216]\nVisual content follows a similar trend. Since the launch ofDALL-E2 in 2022, it is estimated that an average of 34 million images have been created daily. As of August 2023, more than 15 billion images had been generated using text-to-image algorithms, with 80% of these created by models based onStable Diffusion.[217]\nIf AI-generated content is included in new data crawls from the Internet for additional training of AI models, defects in the resulting models may occur.[218]Training an AI model exclusively on the output of another AI model produces a lower-quality model. Repeating this process, where each new model is trained on the previous model's output, leads to progressive degradation and eventually results in a \"model collapse\" after multiple iterations.[219]Tests have been conducted with pattern recognition of handwritten letters and with pictures of human faces.[220]As a consequence, the value of data collected from genuine human interactions with systems may become increasingly valuable in the presence of LLM-generated content in data crawled from the Internet.\nOn the other side,synthetic datais often used as an alternative to data produced by real-world events. Such data can be deployed to validate mathematical models and to train machine learning models while preserving user privacy,[221]including for structured data.[222]The approach is not limited to text generation; image generation has been employed to train computer vision models.[223]\nGenerative AI's potential to generate a large amount of content with little effort is also affecting journalism.[224]In January 2023,Futurismbroke the story thatCNEThad been using an undisclosed internal AI tool to write at least 77 of its stories; after the news broke, CNET posted corrections to 41 of the stories.[225]In April 2023,Die Aktuellepublished an AI-generated fake interview ofMichael Schumacher.[226]In May 2024,Futurismnoted that a content management system video by AdVon Commerce, which had used generative AI to produce articles for many of the aforementioned outlets, appeared to show that they \"had produced tens of thousands of articles for more than 150 publishers.\"[227]In 2025, a report from the American Sunlight Project stated thatPravda networkwas publishing as many as 10,000 articles a day, and concluded that much of this content aimed to push Russian narratives intolarge language modelsthrough their training data.[228]\nIn June 2024,Reuters Institutepublished itsDigital News Report for 2024. In a survey of people in America and Europe, Reuters Institute reports that 52% and 47% respectively are uncomfortable with news produced by \"mostly AI with some human oversight\", and 23% and 15% respectively report being comfortable. 42% of Americans and 33% of Europeans reported that they were comfortable with news produced by \"mainly human with some help from AI\". The results of global surveys reported that people were more uncomfortable with news topics including politics (46%), crime (43%), and local news (37%) produced by AI than other news topics.[229]\nOnline users have falsely assumed media of using generative artificial intelligence for content, such as video gamesLittle DroidandCatly.[230]\nDue to various concerns about citizens' unknowingly consuming generative AI media content, proponents argue for labeling such content to provide context. TheCyberspace Administration of Chinaissued rules obligating service providers to labeling this content online.[231]\nThe popularity of ChatGPT caused the emergence of tools that detect whether content was AI-generated, such asGPTZero, but the risk of false accusations (false positives) has remained a concern.[232]Digital watermarkingallows to reach high detection accuracy by subtly altering the generated content in a way that can be detected by software, but without being noticeable by users.OpenAIdeveloped in 2023 a digital watermarking tool that allowed to detect content generated byChatGPTwith an estimated accuracy of 99.9%, when given enough text. But OpenAI chose not to release it, worrying that users would switch to competitor products, and arguing that digital watermarking can be circumvented by bad actors, for example with superficial rephrasing.[233][234]Google's digital watermarking tool called SynthID was integrated in 2025 into products like Gemini, Imagen and Veo. Google also created the portal SynthID detector for users to check whether text, images or videos were produced with Google's generative AI products.[235]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_art",
        "title": "Artificial intelligence visual art - Wikipedia",
        "content": "Artificial intelligence visual art, orAI art, isvisual artworkgenerated (or enhanced) through the use ofartificial intelligence(AI) programs.\nAutomated art has been created since ancient times.[citation needed]The field of artificial intelligence was founded in the 1950s, and artists began to create art with artificial intelligence shortly after the discipline was founded. Throughoutits history, AI has raised manyphilosophical concernsrelated to thehuman mind,artificial beings, and also what can be consideredartin human–AI collaboration. Since the 20th century, people have used AI to create art, some of which has been exhibited in museums and won awards.[1]\nDuring theAI boomof the 2020s,text-to-image modelssuch asMidjourney,DALL-E,Stable Diffusion, andFLUX.1became widely available to the public, allowing users to quickly generate imagery with little effort.[2][3]Commentary about AI art in the 2020s has often focused on issues related tocopyright,deception,defamation, and its impact on more traditional artists, includingtechnological unemployment.\nAutomated art dates back at least to theautomataofancient Greek civilization, when inventors such asDaedalusandHero of Alexandriawere described as designing machines capable of writing text, generating sounds, and playing music.[4][5]Creative automatons have flourished throughout history, such asMaillardet's automaton, created around 1800 and capable of creating multiple drawings and poems.[6]\nAlso in the 19th century,Ada Lovelace, wrote that \"computing operations\" could potentially be used to generate music and poems.[7][8]In 1950,Alan Turing's paper \"Computing Machinery and Intelligence\" focused on whether machines can mimic human behavior convincingly.[9]Shortly after, the academic discipline of artificial intelligence was founded at a researchworkshopatDartmouth Collegein 1956.[10]\nSince its founding, AI researchers have explored philosophical questions about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence; these issues have previously been explored bymyth,fiction, andphilosophysince antiquity.[11]\nSince the founding of AI in the 1950s, artists have used artificial intelligence to create artistic works. These works were sometimes referred to asalgorithmic art,[12]computer art,digital art, ornew media art.[13]\nOne of the first significant AI art systems isAARON, developed byHarold Cohenbeginning in the late 1960s at theUniversity of Californiaat San Diego.[14]AARON uses a symbolic rule-based approach to generate technical images in the era ofGOFAIprogramming, and it was developed by Cohen with the goal of being able to code the act of drawing.[15]AARON was exhibited in 1972 at theLos Angeles County Museum of Art.[16]From 1973 to 1975, Cohen refined AARON during a residency at theArtificial Intelligence LaboratoryatStanford University.[17]In 2024, theWhitney Museum of American Artexhibited AI art from throughout Cohen's career, including re-created versions of his early robotic drawing machines.[17]\nKarl Simshas exhibited art created withartificial lifesince the 1980s. He received an M.S. in computer graphics from theMIT Media Labin 1987 and was artist-in-residence from 1990 to 1996 at thesupercomputermanufacturer and artificial intelligence companyThinking Machines.[18][19][20]In both 1991 and 1992, Sims won the Golden Nica award atPrix Ars Electronicafor his videos using artificial evolution.[21][22][23]In 1997, Sims created the interactive artificial evolution  installationGalápagosfor theNTT InterCommunication Centerin Tokyo.[24]Sims received anEmmy Awardin 2019 for outstanding achievement in engineering development.[25]\nIn 1999,Scott Dravesand a team of several engineers created and releasedElectric Sheepas afree softwarescreensaver.[26]Electric Sheepis a volunteer computing project for animating and evolvingfractal flames, which are distributed to networked computers which display them as a screensaver. The screensaver used AI to create an infinite animation by learning from its audience. In 2001, Draves won the Fundacion Telefónica Life 4.0 prize forElectric Sheep.[27][unreliable source?]\nIn 2014,Stephanie Dinkinsbegan working onConversations with Bina48.[28]For the series, Dinkins recorded her conversations withBINA48, a social robot that resembles a middle-aged black woman.[29][30]In 2019, Dinkins won theCreative Capitalaward for her creation of an evolving artificial intelligence based on the \"interests and culture(s) of people of color.\"[31]\nIn 2015,Sougwen ChungbeganMimicry (Drawing Operations Unit: Generation 1), an ongoing collaboration between the artist and a robotic arm.[32]In 2019, Chung won theLumen Prizefor her continued performances with a robotic arm that uses AI to attempt to draw in a manner similar to Chung.[33]\nIn 2018, an auction sale of artificial intelligence art was held atChristie'sin New York where the AI artworkEdmond de Belamysold forUS$432,500, which was almost 45 times higher than its estimate of US$7,000–10,000. The artwork was created by Obvious, a Paris-based collective.[34][35][36]\nIn 2024, Japanese filmgenerAIdoscopewas released. The film was co-directed byHirotaka Adachi, Takeshi Sone, and Hiroki Yamaguchi. All video, audio, and music in the film were created with artificial intelligence.[37]\nIn 2025, Japaneseanimetelevision seriesTwins Hinahimawas released. The anime was produced and animated with AI assistance during the process of cutting and conversion of photographs into anime illustrations and later retouched by art staff. Most of the remaining parts such as characters and logos were hand-drawn with various software.[38][39]\nDeep learning, characterized by its multi-layer structure that attempts to mimic the human brain, first came about in the 2010s, causing a significant shift in the world of AI art.[40]During the deep learning era, there are mainly these types of designs for generative art:autoregressive models,diffusion models,GANs,normalizing flows.\nIn 2014,Ian Goodfellowand colleagues atUniversité de Montréaldeveloped thegenerative adversarial network(GAN), a type ofdeep neural networkcapable of learning to mimic thestatistical distributionof input data such as images. The GAN uses a \"generator\" to create new images and a \"discriminator\" to decide which created images are considered successful.[41]Unlike previous algorithmic art that followed hand-coded rules, generative adversarial networks could learn a specificaestheticby analyzing adatasetof example images.[12]\nIn 2015, a team atGooglereleasedDeepDream, a program that uses aconvolutional neural networkto find and enhance patterns in images via algorithmicpareidolia.[42][43][44]The process creates deliberately over-processed images with a dream-like appearance reminiscent of apsychedelic experience.[45]Later, in 2017, a conditional GAN learned to generate 1000 image classes ofImageNet, a large visualdatabasedesigned for use invisual object recognition softwareresearch.[46][47]By conditioning the GAN on both random noise and a specific class label, this approach enhanced the quality of image synthesis for class-conditional models.[48]\nAutoregressive modelswere used for image generation, such as PixelRNN (2016), which autoregressively generates one pixel after another with arecurrent neural network.[49]Immediately after theTransformerarchitecture was proposed inAttention Is All You Need(2018), it was used for autoregressive generation of images, but without text conditioning.[50]\nThe websiteArtbreeder, launched in 2018, uses the modelsStyleGANand BigGAN[51][52]to allow users to generate and modify images such as faces, landscapes, and paintings.[53]\nIn the 2020s,text-to-image models, which generate images based onprompts, became widely used, marking yet another shift in the creation of AI-generated artworks.[2]\nIn 2021, using the influentiallarge languagegenerative pre-trained transformermodels that are used inGPT-2andGPT-3,OpenAIreleased a series of images created with the text-to-image AI modelDALL-E 1.[54]It is an autoregressive generative model with essentially the same architecture as GPT-3. Along with this, later in 2021,EleutherAIreleased theopen sourceVQGAN-CLIP[55]based on OpenAI's CLIP model.[56]Diffusion models, generative models used to create synthetic data based on existing data,[57]were first proposed in 2015,[58]but they only became better than GANs in early 2021.[59]Latent diffusion modelwas published in December 2021 and became the basis for the laterStable Diffusion(August 2022).[60]\nIn 2022,Midjourney[61]was released, followed byGoogle Brain'sImagenand Parti, which were announced in May 2022,Microsoft's NUWA-Infinity,[62][2]and thesource-availableStable Diffusion, which was released in August 2022.[63][64][65]DALL-E2, a successor to DALL-E, was beta-tested and released (with the further successor DALL-E3 being released in 2023). Stability AI has a Stable Diffusion web interface called DreamStudio,[66]plugins forKrita,Photoshop,Blender, andGIMP,[67]and theAutomatic1111web-based open sourceuser interface.[68][69][70]Stable Diffusion's main pre-trained model is shared on theHugging Face Hub.[71]\nIdeogramwas released in August 2023, this model is known for its ability to generate legible text.[72][73]\nIn 2024,Fluxwas released. This model can generate realistic images and was integrated intoGrok, the chatbot used onX (formerly Twitter), andLe Chat, the chatbot ofMistral AI.[3][74][75][76]Flux was developed by Black Forest Labs, founded by the researchers behind Stable Diffusion.[77]Grok later switched to its own text-to-image modelAurorain December of the same year.[78]Several companies, along with their products, have also developed an AI model integrated with an image editing service.Adobehas released and integrated the AI modelFireflyintoPremiere Pro,Photoshop, andIllustrator.[79][80]Microsoft has also publicly announced AI image-generator features forMicrosoft Paint.[81]Along with this, some examples oftext-to-video modelsof the mid-2020s areRunway's Gen-2, Google'sVideoPoet, and OpenAI'sSora, which was released in December 2024.[82][83]\nIn 2025, several models were released.GPT Image 1fromOpenAI, launched in March 2025, introduced new text rendering and multimodal capabilities, enabling image generation from diverse inputs like sketches and text.[84]MidJourney v7debuted in April 2025, providing improved text prompt processing.[85]In May 2025Flux.1 Kontextby Black Forest Labs emerged as an efficient model for high-fidelity image generation,[86]whileGoogle'sImagen 4was released with improved photorealism.[87]\nThere are many approaches used by artists to develop AI visual art. Whentext-to-imageis used, AI generates images based on textual descriptions, using models like diffusion or transformer-based architectures. Users input prompts and the AI produces corresponding visuals.[88][89]When image-to-image is used, AI transforms an input image into a new style or form based on a prompt or style reference, such as turning a sketch into a photorealistic image or applying an artistic style.[90][91]When image-to-video is used, AI generates short video clips or animations from a single image or a sequence of images, often adding motion or transitions. This can include animating still portraits or creating dynamic scenes.[92][93]Whentext-to-videois used, AI creates videos directly from text prompts, producing animations, realistic scenes, or abstract visuals. This is an extension of text-to-image but focuses on temporal sequences.[94]\nThere are many tools available to the artist when working with diffusion models. They can define both positive and negative prompts, but they are also afforded a choice in using (or omitting the use of)VAEs,LoRAs, hypernetworks, IP-adapter, and embedding/textual inversions. Artists can tweak settings like guidance scale (which balances creativity and accuracy), seed (to control randomness), and upscalers (to enhance image resolution), among others. Additional influence can be exerted during pre-inference by means of noise manipulation, while traditional post-processing techniques are frequently used post-inference. People can also train their own models.\nIn addition, procedural \"rule-based\" image generation techniques have been developed, utilizing mathematical patterns, algorithms that simulate brush strokes and other painterly effects, as well as deep learning models such asgenerative adversarial networks(GANs) and transformers. Several companies have released applications and websites that allow users to focus exclusively on positive prompts, bypassing the need for manual configuration of other parameters. There are also programs capable of transforming photographs into stylized images that mimic the aesthetics of well-known painting styles.[95][96]\nThere are many options, ranging from simple consumer-facing mobile apps toJupyternotebooks and web UIs that require powerful GPUs to run effectively.[97]Additional functionalities include \"textual inversion,\" which refers to enabling the use of user-provided concepts (like an object or a style) learned from a few images. Novel art can then be generated from the associated word(s) (the text that has been assigned to the learned, often abstract, concept)[98][99]and model extensions or fine-tuning (such asDreamBooth).\nAI has the potential for asocietal transformation, which may include enabling the expansion of noncommercial niche genres (such ascyberpunk derivativeslikesolarpunk) by amateurs, novel entertainment, fast prototyping,[100]increasing art-making accessibility,[100]and artistic output per effort or expenses or time[100]—e.g., via generating drafts, draft-definitions, and image components (inpainting). Generated images are sometimes used as sketches,[101]low-cost experiments,[102]inspiration, or illustrations ofproof-of-concept-stage ideas. Additional functionalities or improvements may also relate to post-generation manual editing (i.e., polishing), such as subsequent tweaking with an image editor.[102]\nPromptsfor some text-to-image models can also include images and keywords and configurable parameters, such as artistic style, which is often used via keyphrases like \"in the style of [name of an artist]\" in the prompt[103]/or selection of a broad aesthetic/art style.[104][101]There are platforms for sharing, trading, searching, forking/refining, or collaborating on prompts for generating specific imagery from image generators.[105][106][107][108]Prompts are often shared along with images onimage-sharingwebsites such asRedditand AI art-dedicated websites. A prompt is not the complete input needed for the generation of an image; additional inputs that determine the generated image include theoutput resolution,random seed, and random sampling parameters.[109]\nSynthetic media, which includes AI art, was described in 2022 as a major technology-driven trend that will affect business in the coming years.[100]Harvard Kennedy Schoolresearchers voiced concerns about synthetic media serving as a vector for political misinformation soon after studying the proliferation of AI art on the X platform.[110]Synthographyis a proposed term for the practice of generating images that are similar to photographs using AI.[111]\nA major concern raised about AI-generated images and art issampling biaswithin model training data leading towards discriminatory output from AI art models. In 2023,University of Washingtonresearchers found evidence of racial bias within the Stable Diffusion model, with images of a \"person\" corresponding most frequently with images of males from Europe or North America.[112]\nLooking more into thesampling biasfound within AI training data, in 2017, researchers at Princeton University used AI software to link over 2 million words, finding that European names were viewed as more \"pleasant\" than African-Americans names, and that the words \"woman\" and \"girl\" were more likely to be associated with the arts instead of science and math, \"which were most likely connected to males.\"[113]Generative AI models typically work based on user-entered word-based prompts, especially in the case ofdiffusion models, and this word-related bias may lead to biased results.\nAlong with this, generative AI can perpetuate harmful stereotypes regarding women. For example,Lensa, an AI app that trended onTikTokin 2023, was known to lighten black skin, make users thinner, and generate hypersexualized images of women.[114]Melissa Heikkilä, a senior reporter atMIT Technology Review, shared the findings of an experiment using Lensa, noting that the generated avatars did not resemble her and often depicted her in a hypersexualized manner.[115]Experts suggest that such outcomes can result from biases in the datasets used to train AI models, which can sometimes contain imbalanced representations, including hypersexual or nude imagery.[116][117]\nIn 2024, Google'schatbotGemini's AI image generator was criticized forracial bias, with claims that Gemini deliberately underrepresented white people in its results.[118]Users reported that it generated images of white historical figures like theFounding Fathers,Nazi soldiers, andVikingsas other races, and that it refused to process prompts such as \"happy white people\" and \"idealnuclear family\".[118][119]Google later apologized for \"missing the mark\" and took Gemini's image generator offline for updates.[120]This prompted discussions about the ethical implications[121]of representing historical figures through a contemporary lens, leading critics to argue that these outputs could mislead audiences regarding actual historical contexts.[122]In addition to the well-documented representational issues such as racial and gender bias, some scholars have also pointed out deeper conceptual assumptions that shape how we perceive AI-generated art. For instance, framing AI strictly as a passive tool overlooks how cultural and technological factors influence its outputs. Others suggest viewing AI as part of a collaborative creative process, where both human and machine contribute to the artistic result.[123]\nLegal scholars, artists, and media corporations have considered the legal and ethical implications of artificial intelligence art since the 20th century. Some artists use AI art to critique and explore the ethics of usinggathered datato produce new artwork.[124]\nIn 1985, intellectual property law professorPamela Samuelsonargued thatUS copyrightshould allocate algorithmically generated artworks to the user of the computer program.[125]A 2019Florida Law Reviewarticle presented three perspectives on the issue. In the first, artificial intelligence itself would become the copyright owner; to do this, Section 101 of the US Copyright Act would need to be amended to define \"author\" as a computer. In the second, following Samuelson's argument, the user, programmer, or artificial intelligence company would be the copyright owner. This would be an expansion of the \"work for hire\" doctrine, under which ownership of a copyright is transferred to the \"employer.\" In the third situation, copyright assignments would never take place, and such works would be in thepublic domain, as copyright assignments require an act of authorship.[126]\nIn 2022, coinciding with the rising availability of consumer-grade AI image generation services, popular discussion renewed over the legality and ethics of AI-generated art. A particular topic is the inclusion of copyrighted artwork and images in AI training datasets, with artists objecting to commercial AI products using their works without consent, credit, or financial compensation.[127]In September 2022, Reema Selhi, of theDesign and Artists Copyright Society, stated that \"there are no safeguards for artists to be able to identify works in databases that are being used and opt out.\"[128]Some have claimed that images generated with these models can bear resemblance to extant artwork, sometimes including the remains of the original artist's signature.[128][129]In December 2022, users of the portfolio platform ArtStation staged an online protest against non-consensual use of their artwork within datasets; this resulted in opt-out services, such as \"Have I Been Trained?\" increasing in profile, as well as some online art platforms promising to offer their own opt-out options.[130]According to theUS Copyright Office, artificial intelligence programs are unable to hold copyright,[131][132][133]a decision upheld at the Federal District level as of August 2023 followed the reasoning from themonkey selfie copyright dispute.[134]\nOpenAI, the developer ofDALL-E, has its own policy on who owns generated art. They assign the right and title of a generated image to the creator, meaning the user who inputted the prompt owns the image generated, along with the right to sell, reprint, and merchandise it.[135]\nIn January 2023, three artists—Sarah Andersen,Kelly McKernan, and Karla Ortiz—filed acopyright infringementlawsuit against Stability AI,Midjourney, andDeviantArt, claiming that it is legally required to obtain the consent of artists before training neural nets on their work and that these companies infringed on the rights of millions of artists by doing so on five billion images scraped from the web.[136]In July 2023, U.S. District JudgeWilliam Orrickwas inclined to dismiss most of the lawsuits filed by Andersen, McKernan, and Ortiz, but allowed them to file a new complaint.[137]Also in 2023, Stability AI was sued byGetty Imagesfor using its images in the training data.[138]A tool built bySimon Willisonallowed people to search 0.5% of the training data for Stable Diffusion V1.1, i.e., 12 million of the 2.3 billion instances fromLAION2B. Artist Karen Hallion discovered that her copyrighted images were used as training data without their consent.[139]\nIn March 2024, Tennessee enacted theELVIS Act, which prohibits the use of AI to mimic a musician's voice without permission.[140]A month later in that year,Adam Schiffintroduced theGenerative AI Copyright Disclosure Actwhich, if passed, would require that AI companies to submit copyrighted works in their datasets to theRegister of Copyrightsbefore releasing new generative AI systems.[141]In November 2024, a group of artists and activists shared early access to OpenAI's unreleased video generation model,Sora, viaHuggingface. The action, accompanied by a statement, criticized the exploitative use of artists' work by major corporations.'[142][143][144]\nOn June 11, 2025,Universal Pictures(owned byComcast) andThe Walt Disney Companyfiled a copyright infringement lawsuit against Midjourney.[145]The suit described Midjourney as \"a bottomless pit of plagiarism.\"[145]\nAs with other types ofphoto manipulationsince the early 19th century, some people in the early 21st century have been concerned that AI could be used to create content that is misleading and can be made to damage a person's reputation, such asdeepfakes.[146]ArtistSarah Andersen, who previously had her art copied and edited to depictNeo-Nazibeliefs, stated that the spread ofhate speechonline can be worsened by the use of image generators.[139]Some also generate images or videos for the purpose ofcatfishing.\nAI systems have the ability to create deepfake content, which is often viewed as harmful and offensive. The creation of deepfakes poses a risk to individuals who have not consented to it.[147]This mainly refers todeepfake pornographywhich is used asrevenge porn, where sexually explicit material is disseminated to humiliate or harm another person. AI-generatedchild pornographyhas been deemed a potential danger to society due to its unlawful nature.[148]\nAfter winning the 2023 \"Creative\" \"Open competition\" Sony World Photography Awards, Boris Eldagsen stated that his entry was actually created with artificial intelligence. PhotographerFeroz Khancommented to theBBCthat Eldagsen had \"clearly shown that even experienced photographers and art experts can be fooled\".[150]Smaller contests have been affected as well; in 2023, a contest run by authorMark LawrenceasSelf-Published Fantasy Blog-Offwas cancelled after the winning entry was allegedly exposed to be a collage of images generated with Midjourney.[151]\nIn May 2023, on social media sites such as Reddit and Twitter, attention was given to a Midjourney-generated image ofPope Franciswearing a white puffer coat.[152][153]Additionally, an AI-generated image of an attack on thePentagonwent viral as part of ahoax news storyon Twitter.[154][155]\nIn the days beforeMarch 2023 indictment of Donald Trumpas part of theStormy Daniels–Donald Trump scandal, several AI-generated images allegedly depicting Trump's arrest went viral online.[156][157]On March 20, British journalistEliot Higginsgenerated various images of Donald Trump being arrested or imprisoned using Midjourney v5 and posted them on Twitter; two images of Trump struggling against arresting officers went viral under the mistaken impression that they were genuine, accruing more than 5 million views in three days.[158][159]According to Higgins, the images were not meant to mislead, but he was banned from using Midjourney services as a result. As of April 2024, the tweet had garnered more than 6.8 million views.\nIn February 2024, the paperCellular functions of spermatogonial stem cells in relation to JAK/STAT signaling pathwaywas published using AI-generated images. It was later retracted fromFrontiers in Cell and Developmental Biologybecause the paper \"does not meet the standards\".[160]\nTo mitigate some deceptions, OpenAI developed a tool in 2024 to detect images that were generated by DALL-E 3.[161]In testing, this tool accurately identified DALL-E 3-generated images approximately 98% of the time. The tool is also fairly capable of recognizing images that have been visually modified by users post-generation.[162]\nAs generative AI image software such asStable DiffusionandDALL-Econtinue to advance, the potential problems and concerns that these systems pose for creativity and artistry have risen.[139]In 2022, artists working in various media raised concerns about the impact thatgenerative artificial intelligencecould have on their ability to earn money, particularly if AI-based images started replacing artists working in theillustration and design industries.[163][164]In August 2022, digital artist R. J. Palmer stated that \"I could easily envision a scenario where using AI, a single artist or art director could take the place of 5–10 entry level artists... I have seen a lot of self-published authors and such say how great it will be that they don't have to hire an artist.\"[129]Scholars Jiang et al. state that \"Leaders of companies like Open AI and Stability AI have openly stated that they expect generative AI systems to replace creatives imminently.\"[139]A 2022 case study found that AI-produced images created by technology likeDALL-Ecaused some traditional artists to be concerned about losing work, while others use it to their advantage and view it as a tool.[147]\nAI-based images have become more commonplace in art markets and search engines because AI-basedtext-to-image systemsare trained from pre-existing artistic images, sometimes without the original artist's consent, allowing the software to mimic specific artists' styles.[139][165]For example, Polish digital artist Greg Rutkowski has stated that it is more difficult to search for his work online because many of the images in the results are AI-generated specifically to mimic his style.[64]Furthermore, some training databases on which AI systems are based are not accessible to the public.\nThe ability of AI-based art software to mimic or forge artistic style also raises concerns of malice or greed.[139][166][167]Works of AI-generated art, such asThéâtre D'opéra Spatial, a text-to-image AI illustration that won the grand prize in the August 2022 digital art competition at theColorado State Fair, have begun to overwhelm art contests and other submission forums meant for small artists.[139][166][167]TheNetflixshort filmThe Dog & the Boy, released in January 2023, received backlash online for its use of artificial intelligence art to create the film's background artwork.[168]Within the same vein,DisneyreleasedSecret Invasion, aMarvelTV show with an AI-generated intro, on Disney+ in 2023, causing concern and backlash regarding the idea that artists could be made obsolete by machine-learning tools.[169]\nAI art has sometimes been deemed to be able to replace traditionalstock images.[170]In 2023,Shutterstockannounced a beta test of an AI tool that can regenerate partial content of other Shutterstock's images.Getty ImagesandNvidiahave partnered with the launch of Generative AI byiStock, a model trained on Getty's library and iStock's photo library using Nvidia's Picasso model.[171]\nResearchers fromHugging FaceandCarnegie Mellon Universityreported in a 2023 paper that generating one thousand 1024×1024 images usingStable Diffusion's XL 1.0 base model requires 11.49kWhof energy and generates 1,594 grams (56.2 oz) ofcarbon dioxide, which is roughly equivalent to driving an average gas-powered car a distance of 4.1 miles (6.6 km). Comparing 88 different models, the paper concluded that image-generation models used on average around 2.9kWh of energy per 1,000inferences.[172]\nIn addition to the creation of original art, research methods that use AI have been generated to quantitatively analyze digital art collections. This has been made possible due to the large-scale digitization of artwork in the past few decades. According to CETINIC and SHE (2022), using artificial intelligence to analyze already-existing art collections can provide new perspectives on the development of artistic styles and the identification of artistic influences.[173][174]\nTwo computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art.[175]Close reading focuses on specific visual aspects of one piece. Some tasks performed by machines in close reading methods include computational artist authentication and analysis of brushstrokes or texture properties. In contrast, through distant viewing methods, the similarity across an entire collection for a specific feature can be statistically visualized. Common tasks relating to this method include automatic classification,object detection,multimodal tasks, knowledge discovery in art history, and computational aesthetics.[174]Synthetic images can also be used to train AI algorithms forart authenticationand to detect forgeries.[176]\nResearchers have also introduced models that predict emotional responses to art. One such model is ArtEmis, a large-scale dataset paired with machine learning models. ArtEmis includes emotional annotations from over 6,500 participants along with textual explanations. By analyzing both visual inputs and the accompanying text descriptions from this dataset, ArtEmis enables the generation of nuanced emotional predictions.[177][178]\nAI has also been used in arts outside of visual arts. Generative AI has been used to createmusic, as well as in video game productionbeyond imagery, especially forlevel design(e.g., forcustom maps) and creating new content (e.g., quests or dialogue) orinteractive storiesin video games.[179][180]AI has also been used in theliterary arts,[181]such as helping withwriter's block, inspiration, or rewriting segments.[182][183][184][185]In the culinary arts, some prototypecooking robotscan dynamicallytaste, which can assist chefs in analyzing the content and flavor of dishes during the cooking process.[186]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Generative_audio",
        "title": "Generative audio - Wikipedia",
        "content": "Generative audiorefers to the creation ofaudiofiles from databases ofaudio clips.[citation needed]This technology differs fromsynthesized voicessuch as Apple'sSirior Amazon'sAlexa, which use a collection of fragments that are stitched together on demand.\nGenerative audio works by using neural networks to learn the statistical properties of an audio source, then reproduces those properties.[1]\nWith this technology, a person's voicecan be replicatedto speak phrases that they may have never spoken. This could lead to a synthetic version of a public figure's voice being used against them.[2]\nModern generative audio systems employ various deep learning architectures. One notable approach usesgenerative adversarial networks(GANs), where two machine learning models work against each other to create realistic audio. Other architectures includeWaveNet, which uses dilated causal convolutions to model raw audio waveforms, and implementations like15.ai, which demonstrated in 2020 the ability to clone voices using as little as 15 seconds of training data through specialized neural network architectures.[3][4]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Music_and_artificial_intelligence",
        "title": "Music and artificial intelligence - Wikipedia",
        "content": "Music and artificial intelligence(music and AI) is the development ofmusic softwareprograms which useAIto generate music.[1]As with applications in other fields, AI in music also simulates mental tasks. A prominent feature is the capability of an AI algorithm to learn based on past data, such as in computer accompaniment technology, wherein the AI is capable of listening to a human performer and performing accompaniment.[2]Artificial intelligence also drives interactive composition technology, wherein a computer composes music in response to a live performance. There are other AI applications in music that cover not only music composition, production, and performance but also how music is marketed and consumed. Several music player programs have also been developed to use voice recognition and natural language processing technology for music voice control. Current research includes the application of AI inmusic composition,performance, theory and digitalsound processing. Composers/artists likeJennifer WalsheorHolly Herndonhave been exploring aspects of music AI for years in their performances and musical works. Another original approach of humans “imitating AI” can be found in the 43-hour sound installationString Quartet(s)byGeorges Lentz(see interview withChatGPT-4on music and AI).[3]\n20th century art historianErwin Panofskyproposed that in all art, there existed three levels of meaning: primary meaning, or the natural subject; secondary meaning, or the conventional subject; and tertiary meaning, the intrinsic content of the subject.[4][5]AI music explores the foremost of these, creating music without the \"intention\" which is usually behind it, leaving composers who listen to machine-generated pieces feeling unsettled by the lack of apparent meaning.[6]\nIn the 1950s and the 1960s, music made by artificial intelligence was not fully original, but generated from templates that people had already defined and given to theAI, with this being known asrule-based systems. As time passed, computers became more powerful, which allowed machine learning and artificial neural networks to help in the music industry by giving AI large amounts of data to learn how music is made instead of predefined templates. By the early 2000s, more advancements in artificial intelligence had been made, withgenerative adversarial networks(GANs) anddeep learningbeing used to help AI compose more original music that is more complex and varied than possible before. Notable AI-driven projects, such as OpenAI’sMuseNetand Google’s Magenta, have demonstrated AI’s ability to generate compositions that mimic various musical styles.[7]\nArtificial intelligence finds its beginnings in music with the transcription problem: accurately recording a performance into musical notation as it is played.Père Engramelle's schematic of a \"piano roll\", a mode of automatically recording note timing and duration in a way which could be easily transcribed to proper musical notation by hand, was first implemented by German engineers J.F. Unger and J. Hohlfield in 1952.[8]\nIn 1957, the ILLIAC I (Illinois Automatic Computer) produced the \"Illiac Suite for String Quartet\", a completely computer-generated piece of music. The computer was programmed to accomplish this by composerLeonard Isaacsonand mathematicianLejaren Hiller.[6]: v–viiIn 1960, Russian researcher Rudolf Zaripov published worldwide first paper on algorithmic music composing using theUral-1computer.[9]\nIn 1965, inventorRay Kurzweildeveloped software capable of recognizing musical patterns and synthesizing new compositions from them. The computer first appeared on the quiz showI've Got a Secretthat same year.[10]\nBy 1983,Yamaha Corporation's Kansei Music System had gained momentum, and a paper was published on its development in 1989. The software utilized music information processing and artificial intelligence techniques to essentially solve the transcription problem for simpler melodies, although higher-level melodies and musical complexities are regarded even today as difficult deep-learning tasks, and near-perfect transcription is still a subject of research.[8][11]\nIn 1997, an artificial intelligence program named Experiments in Musical Intelligence (EMI) appeared to outperform a human composer at the task of composing a piece of music to imitate the style ofBach.[12]EMI would later become the basis for a more sophisticated algorithm calledEmily Howell, named for its creator.\nIn 2002, the music research team at the Sony Computer Science Laboratory Paris, led by French composer and scientistFrançois Pachet, designed the Continuator, an algorithm uniquely capable of resuming a composition after a live musician stopped.[13]\nEmily Howellwould continue to make advancements in musical artificial intelligence, publishing its first albumFrom Darkness, Lightin 2009.[14]Since then, many more pieces by artificial intelligence and various groups have been published.\nIn 2010,Iamusbecame the first AI to produce a fragment of original contemporary classical music, in its own style: \"Iamus' Opus 1\". Located at the Universidad de Malága (Malága University) in Spain, the computer can generate a fully original piece in a variety of musical styles.[15][6]: 468–481In August 2019, a large dataset consisting of 12,197 MIDI songs, each with their lyrics and melodies,[16]was created to investigate the feasibility of neural melody generation from lyrics using a deep conditional LSTM-GAN method.\nWith progress ingenerative AI, models capable of creating complete musical compositions (including lyrics) from a simple text description have begun to emerge. Two notable web applications in this field areSuno AI, launched in December 2023, andUdio, which followed in April 2024.[17]\nDeveloped at Princeton University by Ge Wang and Perry Cook, ChucK is a text-based, cross-platform language.[18]By extracting and classifying the theoretical techniques it finds in musical pieces, the software is able to synthesize entirely new pieces from the techniques it has learned.[19]The technology is used bySLOrk(Stanford Laptop Orchestra)[20]andPLOrk(Princeton Laptop Orchestra).\nJukedeck was a website that let people use artificial intelligence to generate original, royalty-free music for use in videos.[21][22]The team started building the music generation technology in 2010,[23]formed a company around it in 2012,[24]and launched the website publicly in 2015.[22]The technology used was originally a rule-basedalgorithmic compositionsystem,[25]which was later replaced withartificial neural networks.[21]The website was used to create over 1 million pieces of music, and brands that used it includedCoca-Cola,Google,UKTV, and theNatural History Museum, London.[26]In 2019, the company was acquired byByteDance.[27][28][29]\nMorpheuS[30]is a research project byDorien Herremansand Elaine Chew atQueen Mary University of London, funded by a Marie Skłodowská-Curie EU project. The system uses an optimization approach based on avariable neighborhood searchalgorithm to morph existing template pieces into novel pieces with a set level of tonal tension that changes dynamically throughout the piece. This optimization approach allows for the integration of a pattern detection technique in order to enforce long term structure and recurring themes in the generated music. Pieces composed by MorpheuS have been performed at concerts in both Stanford and London.\nCreated in February 2016, inLuxembourg,AIVAis a program that produces soundtracks for any type of media. The algorithms behind AIVA are based on deep learning architectures[31]AIVA has also been used to compose a Rock track calledOn the Edge,[32]as well as a pop tuneLove Sick[33]in collaboration with singerTaryn Southern,[34]for the creation of her 2018 album \"I am AI\".\nGoogle's Magenta team has published several AI music applications and technical papers since their launch in 2016.[35]In 2017 they released theNSynthalgorithm and dataset,[36]and anopen sourcehardware musical instrument, designed to facilitate musicians in using the algorithm.[37]The instrument was used by notable artists such asGrimesandYACHTin their albums.[38][39]In 2018, they released a piano improvisation app called Piano Genie. This was later followed by Magenta Studio, a suite of 5 MIDI plugins that allow music producers to elaborate on existing music in their DAW.[40]In 2023, their machine learning team published a technical paper on GitHub that described MusicLM, a private text-to-music generator which they'd developed.[41][42]\nRiffusionis aneural network, designed by Seth Forsgren and Hayk Martiros, that generates music using images of sound rather than audio.[43]\nThe resulting music has been described as \"de otro mundo\" (otherworldly),[44]although unlikely to replace man-made music.[44]The model was made available on December 15, 2022, with the code also freely available onGitHub.[45]\nThe first version of Riffusion was created as afine-tuningofStable Diffusion, an existing open-source model for generating images from text prompts, onspectrograms,[43]resulting in a model which used text prompts to generate image files which could then be put through aninverse Fourier transformand converted into audio files.[45]While these files were only several seconds long, the model could also uselatent spacebetween outputs tointerpolatedifferent files together[43][46](using theimg2imgcapabilities of SD).[47]It was one of many models derived from Stable Diffusion.[47]\nIn December 2022, Mubert[48]similarly used Stable Diffusion to turn descriptive text into music loops. In January 2023, Google published a paper on their own text-to-music generator called MusicLM.[49][50]\nSpike AI is an AI-basedaudio plug-in, developed bySpike Stentin collaboration with his son Joshua Stent and friend Henry Ramsey, that analyzes tracks and provides suggestions to increase clarity and other aspects duringmixing. Communication is done by using achatbottrained on Spike Stent's personal data. The plug-in integrates intodigital audio workstation.[53][54]\nArtificial intelligence can potentially impact how producers create music by giving reiterations of a track that follow a prompt given by the creator. These prompts allow the AI to follow a certain style that the artist is trying to go for.[6]AI has also been seen in musical analysis where it has been used for feature extraction, pattern recognition, and musical recommendations.[55]New tools that are powered by artificial intelligence have been made to help aid in generating original music compositions, likeAIVA(Artificial Intelligence Virtual Artist) andUdio. This is done by giving an AI model data of already-existing music and having it analyze the data using deep learning techniques to generate music in many different genres, such as classical music or electronic music.[56]\nSeveral musicians such asDua Lipa,Elton John,Nick Cave,Paul McCartneyandStinghave criticized the use of AI in music and are encouraging theUK governmentto act on this matter.[57][58][59][60][61]Another example of this protest is the silent 2025 albumIs This What We Want?.\nSome artists have encouraged the use of AI in music such asGrimes.[62]\nWhile helpful in generating new music, many issues have come up since artificial intelligence has begun making music. Some major concerns include how the economy will be impacted with AI taking over music production, who truly owns music generated by AI, and a lower demand for human-made musical compositions. Some critics argue that AI diminishes the value of human creativity, while proponents see it as an augmentative tool that expands artistic possibilities rather than replacing human musicians.[63]\nAdditionally, concerns have been raised about AI's potential to homogenize music. AI-driven models often generate compositions based on existing trends, which some fear could limit musical diversity. Addressing this concern, researchers are working on AI systems that incorporate more nuanced creative elements, allowing for greater stylistic variation.[56]\nAnother major concern about artificial intelligence in music is copyright laws. Many questions have been asked about who owns AI generated music and productions, as today’s copyright laws require the work to be human-authorized in order to be granted copyright protection. One proposed solution is to create hybrid laws that recognize both the artificial intelligence that generated the creation and the humans that contributed to the creation.[citation needed]\nIn the United States, the current legal framework tends to apply traditional copyright laws to AI, despite its differences with the human creative process.[64]However, music outputs solely generated by AI are not granted copyright protection. In thecompendium of the U.S. Copyright Office Practices, the Copyright Office has stated that it would not grant copyrights to \"works that lack human authorship\" and \"the Office will not register works produced by a machine or mere mechanical process that operates randomly or automatically without any creative input or intervention from a human author.\"[65]In February 2022, the Copyright Review Board rejected an application to copyright AI-generated artwork on the basis that it \"lacked the required human authorship necessary to sustain a claim in copyright.\"[66]The usage of copyrighted music in training AI has also been a topic of contention. One instance of this was seen whenSACEM, a professional organization of songwriters, composers, and music publishers demanded that PozaLabs, an AI music generation startup refrain from utilizing any music affiliated with them for training models.[67]\nThe situation in the European Union (EU) is similar to the US, because its legal framework also emphasizes the role of human involvement in a copyright-protected work.[68]According to theEuropean Union Intellectual Property Officeand the recent jurisprudence of theCourt of Justice of the European Union, the originality criterion requires the work to be the author's own intellectual creation, reflecting the personality of the author evidenced by the creative choices made during its production, requires distinct level of human involvement.[68]The reCreating Europe project, funded by the European Union's Horizon 2020 research and innovation program, delves into the challenges posed by AI-generated contents including music, suggesting legal certainty and balanced protection that encourages innovation while respecting copyright norms.[68]The recognition ofAIVAmarks a significant departure from traditional views on authorship and copyrights in the realm of music composition, allowing AI artists capable of releasing music and earning royalties. This acceptance marks AIVA as a pioneering instance where an AI has been formally acknowledged within the music production.[69]\nThe recent advancements in artificial intelligence made by groups such asStability AI,OpenAI, andGooglehas incurred an enormous sum of copyright claims leveled against generative technology, including AI music. Should these lawsuits succeed, the machine learning models behind these technologies would have their datasets restricted to the public domain.[70]Strides towards addressing ethical issues have been made as well, such as the collaboration between Sound Ethics(a company promoting ethical AI usage in the music industry) and UC Irvine, focusing on ethical frameworks and the responsible usage of AI.[71]\nA more nascent development of AI in music is the application ofaudio deepfakesto cast the lyrics or musical style of a pre-existing song to the voice or style of another artist. This has raised many concerns regarding the legality of technology, as well as the ethics of employing it, particularly in the context of artistic identity.[72]Furthermore, it has also raised the question of to whom the authorship of these works is attributed. As AI cannot hold authorship of its own, current speculation suggests that there will be no clear answer until further rulings are made regarding machine learning technologies as a whole.[73]Most recently, preventative measures have started to be developed byGoogleand Universal Music group who have taken in royalties and credited attribution in order to allow producers to replicate the voices and styles of artists.[74]\nIn 2023, an artist known as ghostwriter977 created a musical deepfake called \"Heart on My Sleeve\" that cloned the voices ofDrakeandThe Weekndby inputting an assortment of vocal-only tracks from the respective artists into a deep-learning algorithm, creating an artificial model of the voices of each artist, to which this model could be mapped onto originalreference vocalswith original lyrics.[75]The track was submitted forGrammyconsideration for the best rap song and song of the year.[76]It went viral and gained traction onTikTokand received a positive response from the audience, leading to its official release onApple Music,Spotify, andYouTubein April 2023.[77]Many believed the track was fully composed by an AI software, but the producer claimed the songwriting, production, and original vocals (pre-conversion) were still done by him.[75]It would later be rescinded from any Grammy considerations due to it not following the guidelines necessary to be considered for a Grammy award.[77]The track would end up being removed from all music platforms byUniversal Music Group.[77]The song was a watershed moment for AI voice cloning, and models have since been created for hundreds, if not thousands, of popular singers and rappers.\nIn 2013, country music singerRandy Travissuffered astrokewhich left him unable to sing. In the meantime, vocalist James Dupré toured on his behalf, singing his songs for him. Travis and longtime producerKyle Lehningreleased a new song in May 2024 titled \"Where That Came From\", Travis's first new song since his stroke. The recording uses AI technology to re-create Travis's singing voice, having been composited from over 40 existing vocal recordings alongside those of Dupré.[78][79]\nSince 2024, rapperKanye Westhas been using artificial intelligence deepfakes of his own voice. His usage of deepfakes started during the production of his albumVultures 2, where the songs \"Field Trip\" and \"Sky City\" drew suspicion of artificial intelligence usage;[80]further updates to the songs \"Forever\"[80]and \"530\"[81]would also be accused of using AI. ArtistTy Dolla Sign, who releasedVultures 2with West, would confirm the allegations in 2025 during an interview.[82]West would subsequently confirm that his subsequent solo album,Bully, also used artificial intelligence,[83]and his later albumIn a Perfect Worldand the 2025 updated version ofDonda 2would also draw accusations of AI usage.[citation needed]\nPlayboi Cartihas also been accused of using artificial intelligence deepfakes. The allegations followed the release of the song \"Timeless\", a collaboration withThe Weeknd, where his verse was accused of using artificial intelligence.[84]The allegations would further intensify after the release of his albumMusic, where \"Rather Lie\" and \"Fine Shit\" were also accused of using artificial intelligence. Playboi Carti would deny the accusations.[85]\nArtificial intelligence music encompasses a number of technical approaches used for music composition, analysis, classification, and suggestion. Techniques used are drawn from deep learning, machine learning, natural language processing, and signal processing. Current systems are able to compose entire musical compositions, parse affective content, accompany human players in real-time, and acquire patterns of user and context-dependent preferences.[86][87][88][89]\nSymbolic music generation is the generation of music in discrete symbolic forms such as MIDI, where note and timing are precisely defined. Early systems employed rule-based systems and Markov models, but modern systems employ deep learning to a large extent. Recurrent Neural Networks (RNNs), and more precisely Long Short-Term Memory (LSTM) networks, have been employed in modeling temporal dependencies of musical sequences. They may be used to generate melodies, harmonies, and counterpoints in various musical genres.[90]\nTransformer models such as Music Transformer and MuseNet became more popular for symbolic generation due to their ability to model long-range dependencies and scalability. These models were employed to generate multi-instrument polyphonic music and stylistic imitations.[91]\nThis method generates music as raw audio waveforms instead of symbolic notation. DeepMind's WaveNet is an early example that uses autoregressive sampling to generate high-fidelity audio. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) are being used more and more in new audio texture synthesis and timbre combination of different instruments.[87]\nNSynth (Neural Synthesizer), a Google Magenta project, uses a WaveNet-like autoencoder to learn latent audio representations and thereby generate completely novel instrumental sounds.[92]\nMusic Information Retrieval (MIR) is the extraction of musically relevant information from audio recordings to be utilized in applications such as genre classification, instrument recognition, mood recognition, beat detection, and similarity estimation. CNNs on spectrogram features have been very accurate on these tasks.[89]SVMs and k-Nearest Neighbors (k-NN) are also used for classification on features such as Mel-frequency cepstral coefficients (MFCCs).\nHybrid systems combine symbolic and sound-based methods to draw on their respective strengths. They can compose high-level symbolic compositions and synthesize them as natural sound. Interactive systems in real-time allow for AI to instantaneously respond to human input to support live performance. Reinforcement learning and rule-based agents tend to be utilized to allow for human–AI co-creation in improvisation contexts.[88]\nAffective computing techniques enable AI systems to classify or create music based on some affective content. The models use musical features such as tempo, mode, and timbre to classify or influence listener emotions. Deep learning models have been trained for classifying music based on affective content and even creating music intended to have affective impacts.[93]\nMusic recommenders employ AI to suggest tracks to users based on what they have heard, their tastes, and information available in context. Collaborative filtering, content-based filtering, and hybrid filtering are most widely applied, deep learning being utilized for fine-tuning. Graph-based and matrix factorization methods are used within commercial systems like Spotify and YouTube Music to represent complex user-item relationships.[94]\nAI is also used in audio engineering automation such as mixing and mastering. Such systems level, equalize, pan, and compress to give well-balanced sound outputs. Software such as LANDR and iZotope Ozone utilize machine learning in emulating professional audio engineers' decisions.[95]\nNatural language generation also applies to songwriting assistance and lyrics generation. Transformer language models like GPT-3 have also been proven to be able to generate stylistic and coherent lyrics from input prompts, themes, or feeling. There even exist AI programs that assist with rhyme scheme, syllable count, and poem form.[96]\nRecent developments include multimodal AI systems that integrate music with other media, e.g., dance, video, and text. These can generate background scores in synchronization with video sequences or generate dance choreography from audio input. Cross-modal retrieval systems allow one to search for music using images, text, or gestures.[97]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_government",
        "title": "Artificial intelligence in government - Wikipedia",
        "content": "Artificial intelligence(AI) has a range of uses ingovernment. It can be used to furtherpublic policyobjectives (in areas such as emergency services, health and welfare), as well as assist the public to interact with the government  (through the use ofvirtual assistants, for example). According to theHarvard Business Review, \"Applications of artificial intelligence to the public sector are broad and growing, with early experiments taking place around the world.\"[1]Hila Mehr from theAsh Center for Democratic Governance and InnovationatHarvard Universitynotes that AI in government is not new, with postal services using machine methods in the late 1990s torecognise handwritingon envelopes to automatically route letters.[2]The use of AI in government comes with significant benefits, including efficiencies resulting in cost savings (for instance by reducing the number of front office staff), and reducing the opportunities for corruption.[3]However, it also carries risks (described below).\nThe potential uses of AI in government are wide and varied,[4]withDeloitteconsidering that \"Cognitive technologies could eventually revolutionize every facet of government operations\".[5]Mehr suggests that six types of government problems are appropriate for AI applications:[2]\nMehr states that \"While applications of AI in government work have not kept pace with the rapid expansion of AI in the private sector, the potential use cases in the public sector mirror common applications in the private sector.\"[2]\nPotential and actual uses of AI in government can be divided into three broad categories: those that contribute to public policy objectives; those that assist public interactions with the government; and other uses.\nThere are a range of examples of where AI can contribute to public policy objectives.[4]These include:\nAI can be used to assist members of the public to interact with government and access government services,[4]for example by:\nVarious governments, including those of Australia[10]and Estonia,[11]have implemented virtual assistants to aid citizens in navigating services, with applications ranging from tax inquiries to life-event registrations.\nGerrymanderingis an insidious method of influencing political process.[12]Depending on the objective of its use, the application of artificial intelligence to redraw districts based on voter distribution and demographic datasets can either contribute to impartiality, or sustain partisan gains for interested stakeholders in the election process.[13]\nOther uses of AI in government include:\nAI offers potential efficiencies and costs savings for the government. For example,Deloittehas estimated that automation could saveUS Governmentemployees between 96.7 million to 1.2 billion hours a year, resulting in potential savings of between $3.3 billion to $41.1 billion a year.[5]TheHarvard Business Reviewhas stated that while this may lead a government to reduce employee numbers, \"Governments could instead choose to invest in the quality of its services. They can re-employ workers' time towards more rewarding work that requires lateral thinking, empathy, and creativity — all things at which humans continue to outperform even the most sophisticated AI program.\"[1]\nRisks associated with the use of AI in government include AI becoming susceptible to bias,[2]a lack of transparency in how an AI application may make decisions,[7]and the accountability for any such decisions.[7]\nAI in governance and the economic world might make the market more difficult for companies to keep up with the increases in technology. Large U.S. companies like Apple and Google are able to dominate the market with their latest and most advanced technologies. This gives them an advantage over smaller companies that do not have the means of advancing as far in the digital technology fields with AI.[14]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_healthcare",
        "title": "Artificial intelligence in healthcare - Wikipedia",
        "content": "Artificial intelligence in healthcareis theapplication of artificial intelligence(AI) to analyze and understand complex medical and healthcare data. In some cases, it can exceed or augment human capabilities by providing better or faster ways to diagnose, treat, or prevent disease.[1][2][3][4]\nAs the widespread use ofartificial intelligencein healthcare is still relatively new, research is ongoing into its applications across various medical subdisciplines and related industries. AI programs are being applied to practices such asdiagnostics,[5]treatment protocoldevelopment,[6]drug development,[7]personalized medicine,[8]andpatient monitoringand care.[9]Sinceradiographsare the most commonly performedimaging testsin radiology, the potential for AI to assist with triage and interpretation of radiographs is particularly significant.[10]\nUsing AI in healthcare presents unprecedented ethical concerns related to issues such asdata privacy, automation of jobs, and amplifying already existingalgorithmic bias.[11]New technologies such as AI are often met with resistance by healthcare leaders, leading to slow and erratic adoption.[12]There have been cases where AI has been put to use in healthcare without proper testing.[13][14][15][16]A systematic review and thematic analysis in 2023 showed that most stakeholders including health professionals, patients, and the general public doubted that care involving AI could be empathetic.[17]Meta-studies have found that the scientific literature on AI in healthcare often suffers from a lack ofreproducibility.[18][19][20][21]\nAccurate and early diagnosis of diseases is still a challenge in healthcare. Recognizing medical conditions and their symptoms is a complex problem. AI can assist clinicians with its data processing capabilities to save time and improve accuracy.[22]Through the use of machine learning, artificial intelligence can be able to substantially aid doctors in patient diagnosis through the analysis of masselectronic health records(EHRs).[23]AI can help early prediction, for example, ofAlzheimer's diseaseanddementias, by looking through large numbers of similar cases and possible treatments.[24]\nDoctors' decision making could also be supported by AI in urgent situations, for example in theemergency department. Here AI algorithms can help prioritize more serious cases and reduce waiting time.Decision support systemsaugmented with AI can offer real-time suggestions and faster data interpretation to aid the decisions made by healthcare professionals.[22]\nIn 2023 a study reported higher satisfaction rates withChatGPT-generated responses compared with those from physicians for medical questions posted onReddit's r/AskDocs.[25]Evaluators preferred ChatGPT's responses to physician responses in 78.6% of 585 evaluations, noting better quality and empathy. The authors noted that these were isolated questions taken from an online forum, not in the context of an established patient-physician relationship.[25]Moreover, responses were not graded on the accuracy of medical information, and some have argued that the experiment was not properlyblinded, with the evaluators being coauthors of the study.[26][27][28]\nRecent developments instatistical physics,machine learning, andinferencealgorithms are also being explored for their potential in improving medical diagnostic approaches.[29]Also, the establishment of largehealthcare-related data warehousesof sometimes hundreds of millions of patients provides extensive training data for AI models.[30]\nElectronic health records (EHR) are crucial to the digitalization and information spread of the healthcare industry. Now that around 80% of medical practices use EHR, some anticipate the use of artificial intelligence to interpret the records and provide new information to physicians.[31]\nOne application uses natural language processing (NLP) to make more succinct reports that limit the variation between medical terms by matching similar medical terms.[31]For example, the term heart attack andmyocardial infarctionmean the same things, but physicians may use one over the other based on personal preferences.[31]NLP algorithms consolidate these differences so that larger datasets can be analyzed.[31]Another use of NLP identifies phrases that are redundant due to repetition in a physician's notes and keeps the relevant information to make it easier to read.[31]Other applications useconcept processingto analyze the information entered by the current patient's doctor to present similar cases and help the physician remember to include all relevant details.[32]\nBeyond making content edits to an EHR, there are AI algorithms that evaluate an individual patient's record andpredict a riskfor a disease based on their previous information and family history.[33]One general algorithm is a rule-based system that makes decisions similarly to how humans use flow charts.[34]This system takes in large amounts of data and creates a set of rules that connect specific observations to concluded diagnoses.[34]Thus, the algorithm can take in a new patient's data and try to predict the likeliness that they will have a certain condition or disease.[34]Since the algorithms can evaluate a patient's information based on collective data, they can find any outstanding issues to bring to a physician's attention and save time.[33]One study conducted by the Centerstone research institute found that predictive modeling of EHR data has achieved 70–72% accuracy in predicting individualized treatment response.[35]These methods are helpful due to the fact that the amount of online health records doubles every five years.[33]Physicians do not have the bandwidth to process all this data manually, and AI can leverage this data to assist physicians in treating their patients.[33]\nAlphaFoldhas the ability to predict protein structures based on the constituent amino acid sequence, expected to have benefits in the life sciences--accelerating drug discovery and enabling better understanding of diseases.[36]Nobel laureateVenki Ramakrishnancalled the result \"a stunning advance on the protein folding problem\", adding that \"It has occurred decades before many people in the field would have predicted. It will be exciting to see the many ways in which it will fundamentally change biological research.\" In 2023,Demis HassabisandJohn Jumperwon theBreakthrough Prize in Life Sciences[37]as well as theAlbert Lasker Award for Basic Medical Researchfor their management of the AlphaFold project.[38]Hassabis and Jumper proceeded to win theNobel Prize in Chemistryin 2024 for their work on \"protein structure prediction\" withDavid Bakerof the University of Washington.[39][40]\nImprovements innatural language processingled to the development of algorithms to identifydrug-drug interactionsin medical literature.[41][42][43][44]Drug-drug interactions pose a threat to those taking multiple medications simultaneously, and the danger increases with the number of medications being taken.[45]To address the difficulty of tracking all known or suspected drug-drug interactions, machine learning algorithms have been created to extract information on interacting drugs and their possible effects from medical literature. Efforts were consolidated in 2013 in the DDIExtraction Challenge, in which a team of researchers atCarlos III Universityassembled a corpus of literature on drug-drug interactions to form a standardized test for such algorithms.[46]Competitors were tested on their ability to accurately determine, from the text, which drugs were shown to interact and what the characteristics of their interactions were.[47]Researchers continue to use this corpus to standardize the measurement of the effectiveness of their algorithms.[41][42][44]\nOther algorithms identify drug-drug interactions from patterns inuser-generated content, especially electronic health records and/or adverse event reports.[42][43]Organizations such as theFDA Adverse Event Reporting System(FAERS) and the World Health Organization'sVigiBaseallow doctors to submit reports of possible negative reactions to medications. Deep learning algorithms have been developed to parse these reports and detect patterns that imply drug-drug interactions.[48]\nThe increase oftelemedicine, the treatment of patients remotely, has shown the rise of possible AI applications.[49]AI can assist in caring for patients remotely by monitoring their information through sensors.[50]A wearable device may allow for constant monitoring of a patient and the ability to notice changes that may be less distinguishable by humans. The information can be compared to other data that has already been collected using artificial intelligence algorithms that alert physicians if there are any issues to be aware of.[50]\nAnother application of artificial intelligence is chat-bot therapy. Some researchers charge that the reliance onchatbots for mental healthcaredoes not offer the reciprocity and accountability of care that should exist in the relationship between the consumer of mental healthcare and the care provider (be it a chat-bot or psychologist), though.[51]Some examples of these chatbots include Woebot, Earkick and Wysa.[52][53][54]\nSince the average age has risen due to a longer life expectancy, artificial intelligence could be useful in helping take care of older populations.[55]Tools such as environment and personal sensors can identify a person's regular activities and alert a caretaker if a behavior or a measured vital is abnormal.[55]Although the technology is useful, there are also discussions about limitations of monitoring in order to respect a person's privacy since there are technologies that are designed to map out home layouts and detect human interactions.[55]\nAI has the potential to streamline care coordination and reduce the workload. AI algorithms can automate administrative tasks, prioritize patient needs and facilitate seamless communication in a healthcare team.[56]This enables healthcare providers to focus more on direct patient care and ensures the efficient and coordinated delivery of healthcare services.\nArtificial intelligence algorithms have shown promising results in accurately diagnosing and risk stratifying patients with concern for coronary artery disease, showing potential as an initial triage tool.[57][58]Other algorithms have been used in predicting patient mortality, medication effects, and adverse events following treatment foracute coronary syndrome.[57]Wearables, smartphones, and internet-based technologies have also shown the ability to monitor patients' cardiac data points, expanding the amount of data and the various settings AI models can use and potentially enabling earlier detection of cardiac events occurring outside of the hospital.[59]A research in 2019 found that AI can be used to predict heart attack with up to 90% accuracy.[60]Another growing area of research is the utility of AI in classifyingheart soundsand diagnosingvalvular disease.[61]Challenges of AI in cardiovascular medicine have included the limited data available to train machine learning models, such as limited data onsocial determinants of healthas they pertain tocardiovascular disease.[62]\nA key limitation in early studies evaluating AI were omissions of data comparing algorithmic performance to humans. Examples of studies which assess AI performance relative to physicians includes how AI is non-inferior to humans in interpretation of cardiac echocardiograms[63]and that AI can diagnose heart attack better than human physicians in the emergency setting, reducing both low-value testing and missed diagnoses.[64]\nIn cardiovasculartissue engineeringandorganoidstudies, AI is increasingly used to analyze microscopy images, and integrate electrophysiological read outs.[65]\nMedical imaging(such as X-ray and photography) is a commonly used tool indermatology[66]and thedevelopment of deep learninghas been strongly tied toimage processing. Therefore, there is a natural fit between the dermatology and deep learning. Machine learning learning holds great potential to process these images for better diagnoses.[67]Han et al. showed keratinocytic skin cancer detection from face photographs.[68]Esteva et al. demonstrated dermatologist-level classification of skin cancer from lesion images.[69]Noyan et al. demonstrated aconvolutional neural networkthat achieved 94% accuracy at identifying skin cells from microscopicTzanck smearimages.[70]A concern raised with this work is that it has not engaged with disparities related to skin color or differential treatment of patients with non-white skin tones.[71]\nAccording to some researchers, AI algorithms have been shown to be more effective than dermatologists at identifying cancer.[72]However, a 2021 review article found that a majority of papers analyzing the performance of AI algorithms designed for skin cancer classification failed to use external test sets.[73]Only four research studies were found in which the AI algorithms were tested on clinics, regions, or populations distinct from those it was trained on, and in each of those four studies, the performance of dermatologists was found to be on par with that of the algorithm. Moreover, only one study[74]was set in the context of a full clinical examination; others were based on interaction through web-apps or online questionnaires, with most based entirely on context-free images of lesions. In this study, it was found that dermatologists significantly outperformed the algorithms. Many articles claiming superior performance of AI algorithms also fail to distinguish between trainees and board-certified dermatologists in their analyses.[73]\nIt has also been suggested that AI could be used to automatically evaluate the outcome ofmaxillo-facial surgeryorcleft palatetherapy in regard to facial attractiveness or age appearance.[75][76]\nAI can play a role in various facets of the field ofgastroenterology.Endoscopicexams such asesophagogastroduodenoscopies(EGD) andcolonoscopiesrely on rapid detection of abnormal tissue. By enhancing these endoscopic procedures with AI, clinicians can more rapidly identify diseases, determine their severity, and visualize blind spots. Early trials in using AI detection systems of earlystomach cancerhave shownsensitivityclose to expert endoscopists.[77]\nAI can assist doctors treatingulcerative colitisin detecting the microscopic activity of the disease in people and predicting when flare-ups will happen. For example, an AI-powered tool was developed to analyse digitised bowel samples (biopsies). The tool was able to distinguish with 80% accuracy between samples that showremissionof colitis and those with active disease. It also predicted the risk of a flare-up happening with the same accuracy. These rates of successfully using microscopic disease activity to predict disease flare are similar to the accuracy ofpathologists.[78][79]\nArtificial intelligence utilises massive amounts of data to help with predicting illness, prevention, and diagnosis, as well as patient monitoring. In obstetrics, artificial intelligence is utilized in magnetic resonance imaging, ultrasound, and foetal cardiotocography. AI contributes in the resolution of a variety of obstetrical diagnostic issues.[80]\nAI has shown potential in both the laboratory and clinical spheres ofinfectious diseasemedicine.[81]During theCOVID-19 pandemic, AI has been used for early detection, tracking virus spread and analysing virus behaviour, among other things.[82]However, there were only a few examples of AI being used directly in clinical practice during the pandemic itself.[83]\nOther applications of AI around infectious diseases includesupport-vector machinesidentifyingantimicrobial resistance, machine learning analysis of blood smears to detectmalaria, and improved point-of-care testing ofLyme diseasebased on antigen detection. Additionally, AI has been investigated for improving diagnosis ofmeningitis,sepsis, andtuberculosis, as well as predicting treatment complications inhepatitis Bandhepatitis Cpatients.[81]\nAI has been used to identify causes of knee pain that doctors miss, that disproportionately affect Black patients.[84]Underserved populations experience higher levels of pain. These disparities persist even after controlling for the objective severity of diseases like osteoarthritis, as graded by human physicians using medical images, raising the possibility that underserved patients' pain stems from factors external to the knee, such as stress. Researchers have conducted a study using a machine-learning algorithm to show that standard radiographic measures of severity overlook objective but undiagnosed features that disproportionately affect diagnosis and management of underserved populations with knee pain. They proposed that new algorithmic measure ALG-P could potentially enable expanded access to treatments for underserved patients.[85]\nThe use of AI technologies has been explored for use in the diagnosis and prognosis ofAlzheimer's disease(AD). For diagnostic purposes, machine learning models have been developed that rely on structural MRI inputs.[86]The input datasets for these models are drawn from databases such as the Alzheimer's Disease Neuroimaging Initiative.[87]Researchers have developed models that rely onconvolutional neural networkswith the aim of improving early diagnostic accuracy.[88]Generative adversarial networksare a form ofdeep learningthat have also performed well in diagnosing AD.[89]There have also been efforts to develop machine learning models into forecasting tools that can predict the prognosis of patients with AD. Forecasting patient outcomes through generative models has been proposed by researchers as a means of synthesizing training and validation sets.[90]They suggest that generated patient forecasts could be used to provide future models larger training datasets than current open access databases.\nAI has been explored for use incancerdiagnosis, risk stratification, molecular characterization of tumors, and cancer drug discovery. A particular challenge in oncologic care that AI is being developed to address is the ability to accurately predict which treatment protocols will be best suited for each patient based on their individual genetic, molecular, and tumor-based characteristics.[91]AI has been trialed in cancer diagnostics with the reading of imaging studies andpathologyslides.[92]\nIn January 2020,Google DeepMindannounced an algorithm capable of surpassing human experts inbreast cancer detectionin screening scans.[93][94]A number of researchers, includingTrevor Hastie,Joelle Pineau, andRobert Tibshiraniamong others, published a reply claiming that DeepMind's research publication inNaturelacked key details on methodology and code, \"effectively undermin[ing] its scientific value\" and making it impossible for the scientific community to confirm the work.[95]In theMIT Technology Review, author Benjamin Haibe-Kains characterized DeepMind's work as \"an advertisement\" having little to do with science.[96]\nIn July 2020, it was reported that an AI algorithm developed by the University of Pittsburgh achieves the highest accuracy to date inidentifyingprostate cancer, with 98% sensitivity and 97% specificity.[97][98]In 2023 a study reported the use of AI forCT-basedradiomicsclassification at grading the aggressiveness of retroperitonealsarcomawith 82% accuracy compared with 44% for lab analysis of biopsies.[99][100]\nArtificial intelligence-enhanced technology is being used as an aid in the screening of eye disease and prevention of blindness.[101]In 2018, the U.S. Food and Drug Administration authorized the marketing of the first medical device to diagnose a specific type of eye disease, diabetic retinopathy using an artificial intelligence algorithm.[102]Moreover, AI technology may be used to further improve \"diagnosis rates\" because of the potential to decrease detection time.[103]\nFor many diseases,pathologicalanalysis of cells and tissues is considered to be the gold standard of disease diagnosis. Methods ofdigital pathologyallows microscopy slides to be scanned and digitally analyzed. AI-assisted pathology tools have been developed to assist with the diagnosis of a number of diseases, including breast cancer, hepatitis B,gastric cancer, andcolorectal cancer. AI has also been used to predict genetic mutations and prognosticate disease outcomes.[77]AI is well-suited for use in low-complexity pathological analysis of large-scalescreeningsamples, such as colorectal orbreast cancerscreening, thus lessening the burden on pathologists and allowing for faster turnaround of sample analysis.[105]Several deep learning and artificialneural networkmodels have shown accuracy similar to that of human pathologists,[105]and a study of deep learning assistance in diagnosingmetastaticbreast cancer in lymph nodes showed that the accuracy of humans with the assistance of a deep learning program was higher than either the humans alone or the AI program alone.[106]Additionally, implementation of digital pathology is predicted to save over $12 million for a university center over the course of five years,[107]though savings attributed to AI specifically have not yet been widely researched. The use ofaugmentedandvirtual realitycould prove to be a stepping stone to wider implementation of AI-assisted pathology, as they can highlight areas of concern on a pathology sample and present them in real-time to a pathologist for more efficient review.[105]AI also has the potential to identifyhistologicalfindings at levels beyond what the human eye can see,[105]and has shown the ability to usegenotypicandphenotypicdata to more accurately detect the tumor of origin for metastatic cancer.[108]One of the major current barriers to widespread implementation of AI-assisted pathology tools is the lack of prospective, randomized, multi-center controlledtrialsin determining the true clinical utility of AI for pathologists and patients, highlighting a current area of need in AI and healthcare research.[105]\nPrimary care has become one key development area for AI technologies.[109][110]AI in primary care has been used for supporting decision making, predictive modeling, and business analytics.[111]There are only a few examples of AI decision support systems that were prospectively assessed on clinical efficacy when used in practice by physicians. But there are cases where the use of these systems yielded a positive effect on treatment choice by physicians.[112]\nAs of 2022 in relation to elder care, AIrobotshad been helpful in guiding older residents living in assisted living with entertainment and company. These bots are allowing staff in the home to have more one-on-one time with each resident, but the bots are also programmed with more ability in what they are able to do; such as knowing different languages and different types of care depending on the patient's conditions. The bot is an AI machine, which means it goes through the same training as any other machine - using algorithms to parse the given data, learn from it and predict the outcome in relation to what situation is at hand.[113]\nIn psychiatry, AI applications are still in a phase of proof-of-concept.[114]Areas where the evidence is widening quickly include predictive modelling of diagnosis and treatment outcomes,[115]chatbots, conversational agents that imitate human behaviour and which have been studied for anxiety and depression.[116]\nChallenges include the fact that many applications in the field are developed and proposed by private corporations, such as the screening for suicidal ideation implemented by Facebook in 2017.[117]Such applications outside the healthcare system raise various professional, ethical and regulatory questions.[118]Another issue is often with the validity and interpretability of the models. Small training datasets contain bias that is inherited by the models, and compromises the generalizability and stability of these models. Such models may also have the potential to be discriminatory against minority groups that are underrepresented in samples.[119]\nIn 2023, US-basedNational Eating Disorders Associationreplaced its humanhelplinestaff with achatbotbut had to take it offline after users reported receiving harmful advice from it.[120][121][122]\nAI is being studied within the field ofradiologyto detect and diagnose diseases throughcomputerized tomography(CT) andmagnetic resonance(MR) imaging.[123]It may be particularly useful in settings where demand for human expertise exceeds supply, or where data is too complex to be efficiently interpreted by human readers.[124]Several deep learning models have shown the capability to be roughly as accurate as healthcare professionals in identifying diseases through medical imaging, though few of the studies reporting these findings have been externally validated.[125]AI can also provide non-interpretive benefit to radiologists, such as reducing noise in images, creating high-quality images from lower doses of radiation, enhancing MR image quality,[126]and automatically assessing image quality.[127]Further research investigating the use of AI innuclear medicinefocuses on image reconstruction, anatomical landmarking, and the enablement of lower doses in imaging studies.[128]The analysis of images for supervised AI applications in radiology encompasses two primary techniques at present: (1)convolutional neural network-basedanalysis; and (2) utilization ofradiomics.[124]\nAI is also used in breast imaging for analyzing screening mammograms and can participate in improving breast cancer detection rate[129]as well as reducing radiologist's reading workload.\nAs of 2025, 77% (967 out of 1247) of all FDA-approved AI-enabled medical devices are in radiology.[130]\nIn pharmacy, AI helps discover, develop and delivermedications, and can enhance patient care through personalized treatment plans.[131][132]\nThe trend of large health companies merging has allowed for greater health data accessibility. Greater health data have laid the groundwork to implement AI algorithms.\nA large part of industry focus has been in theclinical decision support systems. As more data is collected, machine learning algorithms adapt and allow for more robust responses and solutions.[123]Numerous companies have been exploring the possibilities of the incorporation ofbig datain the healthcare industry, many of whom have been investigating market opportunities through \"data assessment, storage, management, and analysis technologies\".[133]With the market for AI expanding, large tech companies such as Apple, Google, Amazon, andBaiduall have their own AI research divisions, as well as millions of dollars allocated for acquisition of smaller AI based companies.[133]\nThe following are examples of large companies that are contributing to AI algorithms for use in healthcare:\nNeuralinkhas come up with a next-generationneuroprostheticwhich intricately interfaces with thousands of neural pathways in the brain.[123]Their process allows a chip, roughly the size of a quarter, to be inserted in the place of a chunk of a skull by a precision surgical robot to avoid accidental injury.[123]\nTencenthas been working on several medical systems and services. These include AI Medical Innovation System (AIMIS), an AI-powered diagnostic medical imaging service; WeChat Intelligent Healthcare; and Tencent Doctorwork[140]\nHeidi Healthdevelops anAI-powered medical scribethat transcribes clinician–patient conversations in real time and generates structured clinical notes. It has been adopted in multiplehealth systems, including MaineGeneral Health in the US.[141]\nThe Indian startupHaptikdeveloped aWhatsAppchatbot in 2021 which answered questions associated withcoronavirusinIndia. Similarly, a software platformChatBotin partnership withmedtechstartup Infermedica launchedCOVID-19Risk Assessment ChatBot.[142]\nArtificial intelligence continues to expand in its abilities to diagnose more people accurately in nations where fewer doctors are accessible to the public.  Many new technology companies such asSpaceXand theRaspberry Pi Foundationhave enabled more developing countries to have access to computers and the internet than ever before.[143]With the increasing capabilities of AI over the internet, advanced machine learning algorithms can allow patients to get accurately diagnosed when they would previously have no way of knowing if they had a life-threatening disease or not.[143]\nUsing AI in developing nations that do not have the resources will diminish the need for outsourcing and can improve patient care. AI can allow for not only diagnosis of patient in areas where healthcare is scarce, but also allow for a good patient experience by resourcing files to find the best treatment for a patient.[144]The ability of AI to adjust course as it goes also allows the patient to have their treatment modified based on what works for them; a level of individualized care that is nearly non-existent in developing countries.[144]\nChallenges of the clinical use of AI have brought about a potential need forregulations. AI studies need to be completely and transparently reported to have value to inform regulatory approval. Depending on the phase of study, international consensus-based reporting guidelines (TRIPOD+AI,[145]DECIDE-AI,[146]CONSORT-AI[147]) have been developed to provide recommendations on the key details that need to be reported.\nWhile regulations exist pertaining to the collection of patient data such as the Health Insurance Portability and Accountability Act in the US (HIPAA) and the European General Data Protection Regulation (GDPR) pertaining to patients within the EU, health care AI is \"severely under-regulated worldwide\" as of 2025.[148]Unclear is whether healthcare AI can be classified merely assoftwareor asmedical device.[148]\nTheITU-WHO Focus Group on Artificial Intelligence for Health(FG-AI4H) has built a platform known as the ITU-WHO AI for Health Framework for the testing and benchmarking of AI applications in health domain as a joint endeavor ofITUandWHO. As of November 2018, eight use cases were being benchmarked, including assessing breast cancer risk from histopathological imagery, guiding anti-venom selection from snake images, and diagnosing skin lesions.\nIn 2015, theOffice for Civil Rights(OCR) issued rules and regulations to protect the privacy of individuals' health information, requiring healthcare providers to follow certain privacy rules when using AI, to keep a record of how they use AI and to ensure that their AI systems are secure.[150]\nIn May 2016, theWhite Houseannounced its plan to host a series of workshops and formation of theNational Science and Technology Council(NSTC) Subcommittee on Machine Learning and Artificial Intelligence.[151][152]In October 2016, the group published The National Artificial Intelligence Research and Development Strategic Plan, outlining its proposed priorities for Federally-funded AI research and development (within government and academia). The report notes a strategic R&D plan for the subfield ofhealth information technologywas in development stages.[152]\nIn January 2021, the USFDApublished a new Action Plan, entitled Artificial Intelligence (AI) /Machine Learning (ML)-Based Software as a Medical Device (SaMD) Action Plan.[153]It layed out the FDA's future plans for regulation of medical devices that would include artificial intelligence in their software with five main actions: 1. Tailored Regulatory Framework for Ai/M:-based SaMD, 2. Good Machine Learning Practice (GMLP), 3. Patient-Centered Approach Incorporating Transparency to Users, 4. Regulatory Science Methods Related to Algorithm Bias & Robustness, and 5. Real-World Performance(RWP). This plan was in direct response to stakeholders' feedback on a 2019 discussion paper also published by the FDA.[153]\nUnderPresident Bidenthe DHSS and the National Institute of Standards and Technology were instructed to develop regulation of healthcare AI.[148]According to theU.S. Department of Health and Human Services, the OCR issued guidance on theethical use of AIin healthcare in 2021. It outlined four core ethical principles that must be followed: respect forautonomy,beneficence,non-maleficence, and justice. Respect for autonomy requires that individuals have control over their own data and decisions. Beneficence requires that AI be used to do good, such as improving the quality of care and reducing health disparities. Non-maleficence requires that AI be used to do no harm, such as avoiding discrimination in decisions. Finally, justice requires that AI be used fairly, such as using the same standards for decisions no matter a person's race, gender, or income level. As of March 2021, the OCR had hired a Chief Artificial Intelligence Officer (OCAIO) to pursue the \"implementation of the HHS AI strategy\".[154]\nWith thesecond Trump administrationderegulation of health AI began on January 20, 2025 with merely voluntary standards for collecting and sharing data, statutory definitions for algorithmic discrimination, automation bias, and equity being cancelled, cuts toNISTand 19% of FDA workforce eliminated.[148]\nOther countries have implemented data protection regulations, more specifically with company privacy invasions. In Denmark, the Danish Expert Group ondata ethicshas adopted recommendations on \"Data for the Benefit of the People\". These recommendations are intended to encourage the responsible use of data in the business sector, with a focus on data processing. The recommendations include a focus on equality and non-discrimination with regard to bias in AI, as well ashuman dignitywhich is to outweigh profit and must be respected in all data processes.[155]\nThe European Union has implemented theGeneral Data Protection Regulation(GDPR) to protect citizens' personal data, which applies to the use of AI in healthcare. In addition, the European Commission has established guidelines to ensure the ethical development of AI, including the use of algorithms to ensure fairness and transparency.[156]With GDPR, the European Union was the first to regulate AI through data protection legislation. The Union finds privacy as a fundamental human right, it wants to prevent unconsented and secondary uses of data by private or public health facilities. By streamlining access to personal data for health research and findings, they are able to instate the right and importance of patient privacy.[156]\nIn March 2024, the European Union approved the pivotalArtificial Intelligence Act(AI Act).[157]The regulation applies to European companies and organizations, and foreign providers of AI systems in the EU market. The EU AI Act has a risk-based structure, where AI enabled medical devices are in the \"high-risk\" category, the highest risk category of permitted uses for AI.[157]\nIn the United States, the Health Insurance Portability and Accountability Act (HIPAA) requires organizations to protect the privacy and security of patient information. The Centers for Medicare and Medicaid Services have also released guidelines for the development of AI-based medical applications.[158]\nIn 2025, Europe was leading the USA on AI regulation, while lagging in innovation and at least one California-based biotech company was \"engaging theEuropean Medicines Agencyearlier in development than previously anticipated to mitigate concerns about the FDA's ability to meet development timelines.\"[148]\nWhile research on the use of AI in healthcare aims to validate its efficacy in improving patient outcomes before its broader adoption, its use may introduce several new types of risk to patients and healthcare providers, such asalgorithmic bias,Do not resuscitateimplications, and othermachine moralityissues. AI may also compromise the protection of patients' rights, such as the right to informed consent and the right to medical data protection.[159]\nIn order to effectively train machine learning systems and use them in healthcare, massive amounts of data must be gathered. Acquiring this data, however, comes at the cost of patient privacy, which can be controversial. For example, a survey conducted in the UK estimated that 63% of the population is uncomfortable with sharing their personal data in order to improve AI technology.[160]The scarcity of real, accessible patient data is a hindrance that deters the progress of developing and deploying more AI in healthcare.\nThe lack of regulations surrounding AI in the United States has generated concerns about mismanagement of patient data, such as with corporations utilizing patient data for financial gain. For example, as of 2020, the Swiss healthcare companyRochereportedly purchased healthcare data for approximately 2 million cancer patients at an estimated total cost of $1.9 billion.[161]This generated ethical concerns about whether it was fair to sell patients' data, even considering the benefits. Ultimately, the current potential of AI in healthcare is additionally hindered by concerns about mismanagement of data collected, especially in the United States.\nThe use oflarge language modelsfor healthcare consultations introduces particular privacy risks, such as increased exposure of sensitive health information during consultations that may be collected for model retraining. A 2024 study of 846 Chinese users found that while 77.3% expressed willingness to use LLM-based healthcare services, privacy awareness varied significantly by demographics and cultural context. The research revealed a \"privacy paradox\" where users who claimed greater privacy knowledge and concern actually showed higher acceptance of information sharing, potentially due to better understanding of legitimate uses such as academic research and service improvement.[162]\nPrivacy expectations for LLMs vary significantly across cultural contexts. Research in China has shown that users may have different privacy norms compared to Western populations, with factors such as age, education level, and medical background influencing acceptance of data sharing. Younger and more educated users tend to be more privacy-conscious, while those with medical backgrounds show greater acceptance of health data sharing for legitimate medical purposes.[162]\nA systematic review and thematic analysis in 2023 showed that most stakeholders including health professionals, patients, and the general public doubted that care involving AI could beempathetic, or fulfillbeneficence.[17]\nAccording to a 2019 study, AI can replace up to 35% of jobs in the UK within the next 10 to 20 years.[163]However, of these jobs, it was concluded that AI has not eliminated any healthcare jobs so far. Though if AI were to automate healthcare-related jobs, the jobs most susceptible to automation would be those dealing with digital information, radiology, and pathology, as opposed to those dealing with doctor-to-patient interaction.[163]\nOutputs can be incorrect or incomplete and diagnosis and recommendations harm people.[148]\nSince AI makes decisions solely on the data it receives as input, it is important that this data represents accurate patient demographics. In a hospital setting, patients do not have full knowledge of how predictive algorithms are created or calibrated. Therefore, these medical establishments can unfairly code their algorithms todiscriminateagainst minorities and prioritize profits rather than providing optimal care, i.e. violating the ethical principle of social justice ornon-maleficence.[164]A recent scoping review identified 18 equity challenges along with 15 strategies that can be implemented to help address them when AI applications are developed usingmany-to-manymapping.[165]\nThere can be unintended bias in algorithms that can exacerbate social and healthcare inequities.[164]Since AI's decisions are a direct reflection of its input data, the data it receives must have accurate representation of patient demographics. For instance, if populations are less represented in healthcare data it is likely to create bias in AI tools that lead to incorrect assumptions of a demographic and impact the ability to provide appropriate care.[166]White males are overly represented in medical data sets.[167]Therefore, having minimal patient data on minorities can lead to AI making more accurate predictions for majority populations, leading to unintended worse medical outcomes for minority populations.[168]Collecting data from minority communities can also lead to medical discrimination. For instance, HIV is a prevalent virus among minority communities and HIV status can be used to discriminate against patients.[167]In addition to biases that may arise from sample selection, different clinical systems used to collect data may also impact AI functionality. For example, radiographic systems and their outcomes (e.g., resolution) vary by provider. Moreover, clinician work practices, such as the positioning of the patient for radiography, can also greatly influence the data and make comparability difficult.[169]However, these biases are able to be eliminated through careful implementation and a methodical collection of representative data.\nA final source ofalgorithmic bias, which has been called \"label choice bias\", arises when proxy measures are used to train algorithms, that build in bias against certain groups. For example, a widely used algorithm predicted health care costs as a proxy for health care needs, and used predictions to allocate resources to help patients with complex health needs. This introduced bias because Black patients have lower costs, even when they are just as unhealthy as White patients.[170]Solutions to the \"label choice bias\" aim to match the actual target (what the algorithm is predicting) more closely to the ideal target (what researchers want the algorithm to predict), so for the prior example, instead of predicting cost, researchers would focus on the variable of healthcare needs which is rather more significant. Adjusting the target led to almost double the number of Black patients being selected for the program.\nResearch in the 1960s and 1970s produced the first problem-solving program, orexpert system, known asDendral.[171][172]While it was designed for applications in organic chemistry, it provided the basis for a subsequent systemMYCIN,[173]considered one of the most significant early uses of artificial intelligence in medicine.[173][174]MYCIN and other systems such as INTERNIST-1 and CASNET did not achieve routine use by practitioners, however.[175]\nThe 1980s and 1990s brought the proliferation of the microcomputer and new levels of network connectivity. During this time, there was a recognition by researchers and developers that AI systems in healthcare must be designed to accommodate the absence of perfect data and build on the expertise of physicians.[176]Approaches involvingfuzzy settheory,[177]Bayesian networks,[178]andartificial neural networks,[179][180]have been applied to intelligent computing systems in healthcare.\nMedical and technological advancements occurring over this half-century period that have enabled the growth of healthcare-related applications of AI to include:"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_mental_health",
        "title": "Artificial intelligence in mental health - Wikipedia",
        "content": "Artificial intelligence in mental healthrefers to theapplication of artificial intelligence(AI), computational technologies andalgorithmsto support the understanding, diagnosis, and treatment ofmental health disorders.[1][2][3]In the context of mental health, AI is considered a component of digital healthcare, with the objective of improving accessibility and accuracy and addressing the growing prevalence of mental health concerns.[4]Applications of AI in this field include the identification and diagnosis of mental disorders, analysis of electronic health records, development of personalized treatment plans, andanalyticsfor suicide prevention.[4][5]There is also research into, and private companies offering,AI therapiststhat providetalk therapiessuch ascognitive behavioral therapy. Despite its many potential benefits, the implementation of AI in mental healthcare presents significant challenges and ethical considerations, and its adoption remains limited as researchers and practitioners work to address existing barriers.[4]There are concerns over data privacy andtraining datadiversity.\nImplementing AI in mental health can eliminate the stigma and seriousness of mental health issues globally. The recent grasp on mental health issues has brought out concerning facts like depression, affecting millions of people annually. The current application of Artificial intelligence in mental health does not meet the demand to mitigate global mental health concerns.[6]\nIn 2019, 1 in every 8 people, or 970 million people around the world were living with a mental disorder, withanxietyanddepressive disordersbeing the most common.[7]In 2020, the number of people living with anxiety and depressive disorders rose significantly because of theCOVID-19 pandemic.[8]Additionally, the prevalence of mental health and addiction disorders exhibits a nearly equal distribution across genders, emphasizing the widespread nature of the issue.[9]\nThe use of AI in mental health aims to support responsive and sustainable interventions against the global challenge posed by mental health disorders. Some issues common to the mental health industry are provider shortages, inefficient diagnoses, and ineffective treatments. The global market for AI-driven mental health applications is projected to grow significantly, with estimates suggesting an increase from US$0.92 billion in 2023 to US$14.89 billion by 2033.[citation needed]This growth indicates a growing interest in AI's ability to address critical challenges in mental healthcare provision through the development and implementation of innovative solutions.[10]\nSeveral AI technologies, includingmachine learning(ML),natural language processing(NLP),deep learning(DL),computer vision(CV) and LLMs andgenerative AIare currently applied in various mental health contexts. These technologies enable early detection of mental health conditions, personalized treatment recommendations, and real-time monitoring of patient well-being.\nMachine learning is an AI technique that enables computers to identify patterns in large datasets and make predictions based on those patterns. Unlike traditional medical research, which begins with a hypothesis, ML models analyze existing data to uncover correlations and develop predictive algorithms.[10]ML in psychiatry is limited by data availability and quality. Many psychiatric diagnoses rely on subjective assessments, interviews, and behavioral observations, making structured data collection difficult.[10]Some researchers have appliedtransfer learning, a technique that adapts ML models trained in other fields, to overcome these challenges in mental health applications.[11]\nDeep learning, a subset of ML, involvesneural networkswith many layers ofneurons, that can grasp complex patterns, similarly to human brains. It is particularly useful for identifying subtle patterns in speech, imaging, and physiological data.[12]Deep learning techniques have been applied in neuroimaging research to identify abnormalities in brain scans associated with conditions such as schizophrenia, depression, and PTSD.[13]However, deep learning models require extensive, high-quality datasets to function effectively. The limited availability of large, diverse mental health datasets poses a challenge, as patient privacy regulations restrict access to medical records. Additionally, deep learning models often operate as \"black boxes\", meaning their decision-making processes are not easily interpretable by clinicians, raising concerns about transparency and clinical trust.[14]\nNatural language processing allows AI systems to analyze and interpret human language, including speech, text, and tone of voice. In mental health, NLP is used to extract meaningful insights from conversations, clinical notes, and patient-reported symptoms. NLP can assess sentiment, speech patterns, and linguistic cues to detect signs of mental distress. This is crucial because many of the diagnoses andDSM-5mental health disorders are diagnosed via speech in doctor-patient interviews, utilizing the clinician's skill for behavioral pattern recognition and translating it into medically relevant information to be documented and used for diagnoses. As research continues, NLP models must address ethical concerns related to patient privacy, consent, and potential biases in language interpretation.[15]\nAdvancements in NLP such as sentiment analysis identifies distinctions in tone and speech to detect anxiety and depression. \"Woebot\", uses sentiment analysis to scrutinize and detect patterns for depression or despair and suggests professional help to patients. Similarly, \"Cogito\", an AI platform uses voice analysis to find changes in pitch and loudness to identify symptoms of depression or anxiety. The application of NLP can contribute to early diagnosis and improved treatment strategies.[16][17]\nComputer vision enables AI to analyze visual data, such as facial expressions, body language, and micro expressions, to assess emotional and psychological states. This technology is increasingly used in mental health research to detect signs of depression, anxiety, and PTSD through facial analysis.[18]Computer vision tools have been explored for their ability to detect nonverbal cues, such as hesitation or changes in eye contact, which may correlate with emotional distress. Despite its potential, computer vision in mental health raises ethical and accuracy concerns. Facial recognition algorithms can be influenced byculturalandracial biases, leading to potential misinterpretations of emotional expressions.[19]Additionally, concerns about informed consent and data privacy must be addressed before widespread clinical adoption.\nFrom the introduction of LLMs in the field of AI in correlation to mental health care, a lot of developments have come about. Popular examples of LLMs are ChatGPT and Gemini. LLMs have been trained on a lot of data which has made it capable of being considerate and even mimic how a human behaves but chatbots are only fed scripted data which gives it the lack of empathy when dealing with patients. This kind of LLM technology is very useful for people who hesitate to ask for assistance or do not have access to get treatment.[20]\nBut at the same time, LLMs have not exactly been known to be as effective as they seem capable of being. LLMs can experience a condition calledhallucinationwhere they can possibly give wrong medical advice to the patients that can be extremely dangerous. LLMs do not exhibit the required level of compassion or empathy needed specially in difficult situations.[20]\nAI with the use of NLP and ML can be used to help diagnose individuals with mental health disorders. It can be used to differentiate closely similar disorders based on their initial presentation to inform timely treatment before disease progression. For example, it may be able to differentiateunipolarfrombipolardepression by analyzing imaging and medical scans.[10]AI also has the potential to identify novel diseases that were overlooked due to the heterogeneity of presentation of a single disorder.[10]Doctors may overlook the presentation of a disorder because while many people get diagnosed with depression, that depression may take on different forms and be enacted in different behaviors. AI can parse through the variability found in human expression data and potentially identify different types of depression.\nAI can be used to create accurate predictions for disease progression once diagnosed.[10]AI algorithms can also use data-driven approaches to build new clinical risk prediction models[21]without relying primarily on current theories ofpsychopathology. However, internal and external validation of an AI algorithm is essential for its clinical utility.[10]In fact, some studies have usedneuroimaging, electronic health records, genetic data, and speech data to predict how depression would present in patients, their risk forsuicidalityorsubstance abuse, or functional outcomes.[10]The prognosis seems to be highly promising, though it comes with important challenges and ethical considerations such as:\nEarly detention AI can analyze patterns in speech, writing, facial expressions, and social media behavior to detect early signs of depression, anxiety, PTSD, and even schizophrenia.[22]\nIn psychiatry, in many cases multiple drugs are trialed with the patients until the correct combination or regimen is reached to effectively treat their ailment—AI systems have been investigated for their potential to predict treatment response based on observed data collected from various sources. This application of AI has the potential to reduce the time, effort, and resources required while alleviating the burden on both patients and clinicians.[10]\nArtificial intelligence offers several potential advantages in the field of mental health care:\nDespite its potential, the application of AI in mental health presents a number of ethical, practical, and technical challenges:\nAs of 2020, theFood and Drug Administration(FDA) had not yet approved any artificial intelligence-based tools for use inPsychiatry.[28]However, in 2022, the FDA granted authorization for the initial testing of an AI-driven mental health assessment tool known as the AI-Generated Clinical Outcome Assessment (AI-COA). This system employs multimodal behavioral signal processing and machine learning to track mental health symptoms and assess the severity of anxiety and depression. AI-COA was incorporated into a pilot program to evaluate its clinical effectiveness. As of 2025, it has not received full regulatory approval.[29]\nMental health tech startups continue to lead investment activity in digital health despite the ongoing impacts of macroeconomic factors like inflation, supply chain disruptions, and interest rates.[30]\nAccording to CB Insights, State of Mental Health Tech 2021 Report, mental health tech companies raised $5.5 billion worldwide (324 deals), a 139% increase from the previous year that recorded 258 deals.[31]\nA number of startups that are using AI in mental healthcare have closed notable deals in 2022 as well. Among them is the AI chatbot Wysa ($20 million in funding), BlueSkeye that is working on improving early diagnosis (£3.4 million), the Upheal smart notebook for mental health professionals ($10 million in funding)[32], and the AI-based mental health companion clare&me (€1 million).[33]Founded in 2021, Earkick serves as an 'AI therapist' for mental health support.[34][35]\nAn analysis of the investment landscape and ongoing research suggests that we are likely to see the emergence of more emotionally intelligent AI bots and new mental health applications driven by AI prediction and detection capabilities.\nFor instance, researchers atVanderbilt University Medical Centerin Tennessee, US, have developed an ML algorithm that uses a person's hospital admission data, including age, gender, and past medical diagnoses, to make an 80% accurate prediction of whether this individual is likely to take their own life.[36]And researchers at theUniversity of Floridaare about to test their new AI platform aimed at making an accurate diagnosis in patients with early Parkinson's disease.[37]Research is also underway to develop a tool combiningexplainable AIanddeep learningto prescribe personalized treatment plans for children with schizophrenia.[38]\nAI systems could predict and plan treatments accurately and effectively for all fields of medicine at levels similar to that of physicians and general clinical practices. For example, one AI model demonstrated higher diagnostic accuracy for depression and post-traumatic stress disorder compared to general practitioners in controlled studies.[39]\nAI systems that analyze social media data are being developed to detect mental health risks more efficiently and cost-effectively across broader populations. Ethical concerns include uneven performance between digital services, the possibility that biases could affect decision-making, and trust, privacy, and doctor-patient relationship issues.[39]\nIn January 2024, Cedars-Sinai physician-scientists developed a first-of-its-kind program that uses immersivevirtual realityand generative AI to provide mental health support.[40]The program is called XAIA which employs a large language model programmed to resemble a human therapist.[41]\nThe University of Southern California has researched the effectiveness of a virtual therapist named Ellie. Through a webcam and microphone, this AI is able to process and analyze the emotional cues derived from the patient's face and the variation in expressions and tone of voice.[42]\nA team of Stanford Psychologists and AI experts created \"Woebot\". Woebot is an app that makes therapy sessions available 24/7. WoeBot tracks its users' mood through brief daily chat conversations and offers curated videos or word games to assist users in managing their mental health.[42]A Scandinavian team of software engineers and a clinical psychologist created \"Heartfelt Services\". Heartfelt Services is an application meant to simulate conventional talk therapy with an AI therapist.[43]\nIncorporating AI with EHR records, genomic data and clinical prescriptions can contribute to precision treatment. \"Oura Ring\", a wearable technology scans the individual's heart rate and sleep routine in real time to give tailored suggestions. Such AI-based application has an increasing potential in combating the stigma of mental health.[20][6]\nResearch shows that AI-driven mental health tools, particularly those usingcognitive behavioral therapy(CBT), can improve symptoms of anxiety and depression, especially for mild to moderate cases. For example, chatbot-based interventions like Woebot significantly reduced depressive symptoms in young adults within two weeks, with results comparable to brief human-delivered interventions.[44]A 2022 meta-analysis of digital mental health tools, including AI-enhanced apps, found moderate effectiveness in reducing symptoms when user engagement was high, and interventions were evidence-based.[45]\nHowever, traditional therapy remains more effective for complex or high-risk mental health conditions that require emotional nuance and relational depth, such as PTSD, severe depression, or suicidality. The therapeutic alliance, or the relationship between patient and clinician, is frequently cited in clinical literature as a significant factor in treatment outcomes, accounting for up to 30% of positive outcomes.[46]While AI tools are capable of detecting patterns in behavior and speech, they are currently limited in replicating emotional nuance and the social context sensitivity typically provided by human clinicians. As such, most experts view AI in mental health as a complementary tool, best used for screening, monitoring, or augmenting care between human-led sessions.[47]\nWhile AI systems excel at processing large datasets and providing consistent, round-the-clock support, their rigidity and limitations in contextual understanding remain significant barriers. Human therapists can adapt in real time to tone, body language, and life circumstances—something machine learning models have yet to master.[45][47]Nonetheless, integrated models that pair AI-driven symptom tracking with clinician oversight are showing promise[citation needed]. These hybrid approaches may increase access, reduce administrative burden, and support early detection, allowing human clinicians to focus on relational care. Current research suggests that AI in mental health care is more likely to augment rather than replace clinician-led therapy, particularly by supporting data analysis and continuous monitoring[citation needed].\nAlthough artificial intelligence in mental health is a growing field with significant potential, several concerns and criticisms remain regarding its application:\nAI in mental health is progressing with personalized care to incorporate voice, speech and biometric data. But to preventalgorithmic bias, models need to be culturally inclusive too. Ethical issues, practical uses and bias in generative models need to be addressed to promote fair and reliable mental healthcare.[6][27]\nAlthough significant progress is still required, the integration of AI in mental health underscores the need for legal and regulatory frameworks to guide its development and implementation.[4]Achieving a balance between human interaction and AI in healthcare is challenging, as there is a risk that increased automation may lead to a more mechanized approach, potentially diminishing the human touch that has traditionally characterized the field.[5]Furthermore, granting patients a feeling of security and safety is a priority considering AI's reliance on individual data to perform and respond to inputs. Some experts caution that efforts to increase accessibility through automation may unintentionally affect aspects of the patient experience, such as trust or perceived support.[5]To avoid veering in the wrong direction, more research should continue to develop a deeper understanding of where the incorporation of AI produces advantages and disadvantages.[24]\nData privacy and confidentiality are one of the most common security threats to medical data. Chatbots are known to be used as virtual assistants for patients but the sensitive data they collect may not be protected because the US law does not consider them as medical devices. Pharmaceutical companies use this loophole to access sensitive information and use it for their own purpose which results, in a lack of trust in chatbots and patients can hesitate in providing information essential to their treatment. Conversational Artificial Intelligence stores and remembers every conversation with a patient with complete accuracy, smartphones also collect data from search history and track app activity. If such private information is leaked it could further increase the stigma around mental health. The danger of cybercrimes and the government's unprotected access to our data, all raise serious concerns about data security.[27][54]\nAdditionally, a lack of clarity and openness with AI models can lead to a loss of trust from the patient for their medical advisors or doctors as the regular person is unaware of how they reach conclusions into giving certain medical advice. Access to such information is necessary to build trust. However, many of these models act like \"black boxes\", providing very little insight into how they work. AI specialists have thus highlighted ethical standards, diverse data and the correct usage of AI tools in mental healthcare.[27]\nArtificial intelligence has shown promise in transforming mental health care through tools that support diagnosis, symptom tracking, and personalized interventions. However, significant concerns remain about the ways these systems may inadvertently reinforce existing disparities in care. Because AI models rely heavily on training data, they are particularly vulnerable to bias if that data fails to reflect the full range of racial, cultural, gender, and socioeconomic diversity found in the general population.\nFor example, a 2024 study from the University of California found that AI systems analyzing social media data to detect depression exhibited significantly reduced accuracy for Black Americans compared to white users, due to differences in language patterns and cultural expression that were not adequately represented in the training data.[62]Similarly, natural language processing (NLP) models used in mental health settings may misinterpret dialects or culturally specific forms of communication, leading to misdiagnoses or missed signs of distress. These kinds of errors can compound existing disparities, particularly for marginalized populations that already face reduced access to mental health services.\nBiases can also emerge during the design and deployment phases of AI development. Algorithms may inherit the implicit biases of their creators or reflect structural inequalities present in health systems and society at large. These issues have led to increased calls for fairness, transparency, and equity in the development of mental health technologies.\nIn response, researchers and healthcare institutions are taking steps to address bias and promote more equitable outcomes. Key strategies include:\nThese efforts are still in early stages, but they reflect a growing recognition that equity must be a foundational principle in the deployment of AI in mental health care. When designed thoughtfully, AI systems could eventually help reduce disparities in care by identifying underserved populations, tailoring interventions, and increasing access in remote or marginalized communities. Continued investment in ethical design, oversight, and participatory development will be essential to ensure that AI tools do not replicate historical injustices but instead help move mental health care toward greater equity."
    },
    {
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_industry",
        "title": "Artificial intelligence in industry - Wikipedia",
        "content": "Industrial artificial intelligence, orindustrial AI, usually refers to the application ofartificial intelligenceto industry and business. Unlike general artificial intelligence which is a frontier research discipline to build computerized systems that perform tasks requiring human intelligence, industrial AI is more concerned with the application of such technologies to address industrial pain-points for customer value creation, productivity improvement, cost reduction, site optimization, predictive analysis[1]and insight discovery.[2]\nArtificial intelligence andmachine learninghave become key enablers to leverage data in production in recent years due to a number of different factors: More affordable sensors and the automated process of data acquisition; More powerful computation capability of computers to perform more complex tasks at a faster speed with lower cost; Faster connectivity infrastructure and more accessible cloud services for data management and computing power outsourcing.[3]\nPossible applications of industrial AI and machine learning in the production domain can be divided into seven application areas:[4]\nEach application area can be further divided into specific application scenarios that describe concrete AI/ML scenarios in production. While some application areas have a direct connection to production processes, others cover production adjacent fields like logistics or the factory building.[4]\nAn example from the application scenarioProcess Design & Innovationarecollaborative robots. Collaborative robotic arms are able to learn the motion and path demonstrated by human operators and perform the same task.[5]Predictiveandpreventive maintenancethrough data-drivenmachine learningare exemplary application scenarios from theMachinery & Equipmentapplication area.[4]\nIn contrast to entirely virtual systems, in which ML applications are already widespread today, real-world production processes are characterized by the interaction between the virtual and the physical world. Data is recorded using sensors and processed on computational entities and, if desired, actions and decisions are translated back into the physical world via actuators or by human operators.[6]This poses major challenges for the application of ML in production engineering systems. These challenges are attributable to the encounter of process, data and model characteristics: The production domain's high reliability requirements, high risk and loss potential, the multitude of heterogeneous data sources and the non-transparency of ML model functionality impede a faster adoption of ML in real-world production processes.\nIn particular, production data comprises a variety of different modalities, semantics and quality.[7]Furthermore, production systems are dynamic, uncertain and complex,[7]and engineering and manufacturing problems are data-rich but information-sparse.[8]Besides that, due the variety of use cases and data characteristics, problem-specific data sets are required, which are difficult to acquire, hindering both practitioners and academic researchers in this domain.[9]\nThe domain of production engineering can be considered as a rather conservative industry when it comes to the adoption of advanced technology and their integration into existing processes. This is due to high demands on reliability of the production systems resulting from the potentially high economic harm of reduced process effectiveness due to e.g., additional unplanneddowntimeor insufficient product qualities. In addition, the specifics of machining equipment and products prevent area-wide adoptions across a variety of processes. Besides the technical reasons, the reluctant adoption of ML is fueled by a lack of IT and data science expertise across the domain.[4]\nThe data collected in production processes mainly stem from frequently sampling sensors to estimate the state of a product, a process, or the environment in the real world. Sensor readings are susceptible to noise and represent only an estimate of the reality under uncertainty. Production data typically comprises multiple distributed data sources resulting in various data modalities (e.g., images from visual quality control systems, time-series sensor readings, or cross-sectional job and product information). The inconsistencies in data acquisition lead to lowsignal-to-noise ratios, low data quality and great effort in data integration, cleaning and management. In addition, as a result from mechanical and chemical wear of production equipment, process data is subject to various forms ofdata drifts.\nML models are considered asblack-box systemsgiven their complexity and intransparency of input-output relation. This reduces the comprehensibility of the system behavior and thus also the acceptance by plant operators. Due to the lack of transparency and the stochasticity of these models, no deterministic proof of functional correctness can be achieved complicating the certification of production equipment. Given their inherent unrestricted prediction behavior, ML models are vulnerable against erroneous or manipulated data further risking the reliability of the production system because of lacking robustness and safety. In addition to high development and deployment costs, the data drifts cause high maintenance costs, which is disadvantageous compared to purelydeterministic programs.\nThe development of ML applications – starting with the identification and selection of the use case and ending with the deployment and maintenance of the application – follows dedicated phases that can be organized in standard process models. The process models assist in structuring the development process and defining requirements that must be met in each phase to enter the next phase. The standard processes can be classified into generic and domain-specific ones. Generic standard processes (e.g.,CRISP-DM, ASUM-DM,KDD,SEMMA, orTeam Data Science Process) describe a generally valid methodology and are thus independent of individual domains.[10]Domain-specific processes on the other hand consider specific peculiarities and challenges of special application areas.\nTheMachine Learning Pipeline in Productionis a domain-specific data science methodology that is inspired by the CRISP-DM model and was specifically designed to be applied in fields of engineering and production technology.[11]To address the core challenges of ML in engineering – process, data, and model characteristics – the methodology especially focuses on use-case assessment, achieving a common data and process understanding data integration, data preprocessing of real-world production data and the deployment and certification of real-world ML applications.\nThe foundation of most artificial intelligence and machine learning applications in industrial settings are comprehensive datasets from the respective fields. Those datasets act as the basis for training the employed models.[7]In other domains, like computer vision, speech recognition or language models, extensive reference datasets (e.g.ImageNet, Librispeech,[12]The People's Speech) and data scraped from the open internet[13]are frequently used for this purpose. Such datasets rarely exist in the industrial context because of high confidentiality requirements[9]and high specificity of the data. Industrial applications of artificial intelligence are therefore often faced with the problem of data availability.[9]\nFor these reasons, existing open datasets applicable to industrial applications, often originate from public institutions like governmental agencies or universities  and data analysis competitions hosted by companies. In addition to this, data sharing platforms exist. However, most of these platforms have no industrial focus and offer limited filtering abilities regarding industrial data sources.\nArtificial intelligence for business educationrefers to the academic programs offered by universities that integrateartificial intelligence(AI) with business management principles. These programs aim to prepare students for the increasing role of AI in business, equipping them with the skills necessary to apply AI technologies to areas such as predictive analytics, supply chain optimization, and decision-making.[14]AI for business education programs are offered at both undergraduate and graduate levels by several universities globally.\nBachelor in Artificial Intelligence for Business (BAIB), Bachelor in Computer Science and Artificial Intelligence (BCSAI), Master of Science in Artificial Intelligence in Business (MS-AIB) – These are new programs that are still in their first cohorts and have yet to prove themselves in the industry. The undergraduate degrees are often offered in conjunction with a BBA as a 5-year double degree program, the undergraduate degrees are going through the acreditation processes in their respective countries.\nPrograms that combine AI with business studies vary by institution and degree level. Below are some notable examples:\nThe Bachelor in Artificial Intelligence for Business (BAIB)- This program, started byEsadefocuses on the integration of AI and machine learning with core business disciplines such as management, marketing, and finance. TheEsade Business Schoolis a highly regarded institution for its business innovation, sustainability focus and future-proof outlook. During the BBA+BAIB, students are trained to apply AI in business environments to improve efficiency, innovation, and decision-making.[15]\nBachelor in Computer Science and Artificial Intelligence (BCSAI)– Offered along with a BBA byIE University, the BCSAI combines foundational studies in computer science with a specialization in artificial intelligence. The program also provides a strong grounding in business principles, preparing graduates to create AI solutions for business problems and drive technological innovation in the business world.[16]\nMaster in Artificial Intelligence for Business (MS-AIB)–Arizona State University(ASU) offers a graduate-level program focused on AI applications in business environments. This degree explores advanced topics such as AI-driven decision-making, big data analysis, and the ethical implications of AI in business. The program is designed for professionals seeking to leverage AI technologies to transform business practices and improve efficiency.\nThese programs typically include a combination of AI and business courses. Core subjects often cover topics such as machine learning, data science, business strategy, and financial management. The programs aim to give students a broad understanding of AI applications within a business environment, while also allowing them to specialize in areas such as supply chain management, marketing analytics, and AI-driven innovation.\nIn addition to technical courses, many programs include practical training, such as internships, real-world AI projects, and industry case studies. This helps students gain practical experience in applying AI tools and techniques to solve business challenges.\nMany universities offering these degrees hold accreditation from recognized educational bodies, ensuring that their programs meet rigorous academic and industry standards. For example,ESADEandIE Universityare both accredited by institutions such as EQUIS and AACSB, which evaluate the quality of business education programs. Similarly,Arizona State Universityholds accreditation for its graduate programs in business and technology."
    },
    {
        "url": "https://en.wikipedia.org/wiki/AI-assisted_software_development",
        "title": "AI-assisted software development - Wikipedia",
        "content": "AI-assisted software developmentis the use ofartificial intelligence agentsto augment thesoftware development life cycle. It useslarge language models(LLMs),natural language processing, and other AI technologies to assistsoftware developersin a range of tasks from initialcode generationto subsequentdebugging,testinganddocumentation.[1]\nLLMs that have been trained onsource coderepositoriesare able to generatefunctional codefrom natural languageprompts. Such models have knowledge ofprogramming syntax, commondesign patternsandbest practicesin a variety ofprogramming languages.[2]\nAI agents using pre-trained and fine-tuned LLMs can predict and suggestcode completionsbased on context, going beyond simple keyword matching to infer the developer's intent and picture the broader structure of the developingcodebase. An analysis has shown that such use of LLMs significantly enhances code completion performance across several programming languages and contexts, and the resulting capability of predicting relevant code snippets based on context and partial input boosts developer productivity substantially.[3]\nAI is used to automatically generatetest cases, identify potentialbugs, and suggest fixes. LLMs trained on historical bug data can enable prediction of likely failure points in generated code. Similarly, AI agents are used to performstatic code analysis, identifysecurity vulnerabilities, suggest performance improvements and ensure adherence tocoding standardsand best practices.[1]\nThe incorporation of AI tools has introduced newethical dilemmasandintellectual propertychallenges. The ownership of AI-generated code is unclear: who is responsible for the generated end-product? Also unclear are the ethical responsibilities of generated code.[4]Changes in the role ofsoftware engineersare inevitable.[5][6]\nThe outputs from AI-assisted software development require to be validated through a combination of automated testing, static analysis tools and human review, creating agovernance layerthat acts as a safeguard ensuring quality and accountability.[7]\nTechnology sector leaders have highlighted the transformative potential of AI-assisted software development. In an 'Unlocking AI Potential' session of 'Advancing AI 2025' hosted byAMDDeveloper Central,Andrew NgandLisa Suemphasized the strategic and operational implications of integrating AI tools into development workflows. Ng noted that AI systems are increasingly capable of \"helping programmers focus on higher-level problem solving\", while Su framed the shift as \"an opportunity to redefine performance and productivity across industries.\"[8]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Machine_translation",
        "title": "Machine translation - Wikipedia",
        "content": "Machine translationis use of computational techniques totranslatetext or speech from onelanguageto another, including the contextual, idiomatic and pragmatic nuances of both languages.\nEarly approaches were mostlyrule-basedorstatistical. These methods have since been superseded byneural machine translation[1]andlarge language models.[2]\nThe origins of machine translation can be traced back to the work ofAl-Kindi, a ninth-century Arabiccryptographerwho developed techniques for systemic language translation, includingcryptanalysis,frequency analysis, andprobabilityandstatistics, which are used in modern machine translation.[3]The idea of machine translation later appeared in the 17th century. In 1629,René Descartesproposed a universal language, with equivalent ideas in different tongues sharing one symbol.[4]\nThe idea of using digital computers for translation of natural languages was proposed as early as 1947 by England'sA. D. Booth[5]andWarren WeaveratRockefeller Foundationin the same year. \"The memorandum written byWarren Weaverin 1949 is perhaps the single most influential publication in the earliest days of machine translation.\"[6][7]Others followed. A demonstration was made in 1954 on theAPEXCmachine atBirkbeck College(University of London) of a rudimentary translation of English into French. Several papers on the topic were published at the time, and even articles in popular journals (for example an article by Cleave and Zacharov in the September 1955 issue ofWireless World).  A similar application, also pioneered at Birkbeck College at the time, was reading and composingBrailletexts by computer.\nThe first researcher in the field,Yehoshua Bar-Hillel, began his research at MIT (1951). AGeorgetown UniversityMT research team, led by Professor Michael Zarechnak, followed (1951) with a public demonstration of itsGeorgetown-IBM experimentsystem in 1954. MT research programs popped up in Japan[8][9]and Russia (1955), and the first MT conference was held in London (1956).[10][11]\nDavid G. Hays\"wrote about computer-assisted language processing as early as 1957\" and \"was project leader on computational linguistics\natRandfrom 1955 to 1968.\"[12]\nResearchers continued to join the field as the Association for Machine Translation and Computational Linguistics was formed in the U.S. (1962) and the National Academy of Sciences formed the Automatic Language Processing Advisory Committee (ALPAC) to study MT (1964). Real progress was much slower, however, and after theALPAC report(1966), which found that the ten-year-long research had failed to fulfill expectations, funding was greatly reduced.[13]According to a 1972 report by the Director of Defense Research and Engineering (DDR&E), the feasibility of large-scale MT was reestablished by the success of the Logos MT system in translating military manuals into Vietnamese during that conflict.\nThe French Textile Institute also used MT to translate abstracts from and into French, English, German and Spanish (1970); Brigham Young University started a project to translate Mormon texts by automated translation (1971).\nSYSTRAN, which \"pioneered the field under contracts from the U.S. government\"[14]in the 1960s, was used by Xerox to translate technical manuals (1978). Beginning in the late 1980s, ascomputationalpower increased and became less expensive, more interest was shown instatistical models for machine translation. MT became more popular after the advent of computers.[15]SYSTRAN's first implementation system was implemented in 1988 by the online service of theFrench Postal Servicecalled Minitel.[16]Various computer based translation companies were also launched, including Trados (1984), which was the first to develop and market Translation Memory technology (1989), though this is not the same as MT. The first commercial MT system for Russian / English / German-Ukrainian was developed at Kharkov State University (1991).\nBy 1998, \"for as little as $29.95\" one could \"buy a program for translating in one direction between English and a major European language of\nyour choice\" to run on a PC.[14]\nMT on the web started with SYSTRAN offering free translation of small texts (1996) and then providing this viaAltaVista Babelfish,[14]which racked up 500,000 requests a day (1997).[17]The second free translation service on the web wasLernout & Hauspie's GlobaLink.[14]Atlantic Magazinewrote in 1998 that \"Systran's Babelfish and GlobaLink's Comprende\" handled\n\"Don't bank on it\" with a \"competent performance.\"[18]\nFranz Josef Och(the future head of Translation Development AT Google) won DARPA's speed MT competition (2003).[19]More innovations during this time included MOSES, the open-source statistical MT engine (2007), a text/SMS translation service for mobiles in Japan (2008), and a mobile phone with built-in speech-to-speech translation functionality for English, Japanese and Chinese (2009). In 2012, Google announced thatGoogle Translatetranslates roughly enough text to fill 1 million books in one day.\nBefore the advent ofdeep learningmethods, statistical methods required a lot of rules accompanied bymorphological,syntactic, andsemanticannotations.\nThe rule-based machine translation approach was used mostly in the creation ofdictionariesand grammar programs. Its biggest downfall was that everything had to be made explicit: orthographical variation and erroneous input must be made part of the source language analyser in order to cope with it, and lexical selection rules must be written for all instances of ambiguity.\nTransfer-based machine translation was similar tointerlingual machine translationin that it created a translation from an intermediate representation that simulated the meaning of the original sentence. Unlike interlingual MT, it depended partially on the language pair involved in the translation.\nInterlingual machine translation was one instance of rule-based machine-translation approaches.  In this approach, the source language, i.e. the text to be translated, was transformed into an interlingual language, i.e. a \"language neutral\" representation that is independent of any language. The target language was then generated out of theinterlingua. The only interlingual machine translation system that was made operational at the commercial level was the KANT system (Nyberg and Mitamura, 1992), which was designed to translate Caterpillar Technical English (CTE) into other languages.\nMachine translation used a method based ondictionaryentries, which means that the words were translated as they are by a dictionary.\nStatistical machine translation tried to generate translations usingstatistical methodsbased on bilingual text corpora, such as theCanadian Hansardcorpus, the English-French record of the Canadian parliament andEUROPARL, the record of theEuropean Parliament. Where such corpora were available, good results were achieved translating similar texts, but such corpora were rare for many language pairs. The first statistical machine translation software wasCANDIDEfromIBM. In 2005, Google improved its internal translation capabilities by using approximately 200 billion words from United Nations materials to train their system; translation accuracy improved.[20]\nSMT's biggest downfall included it being dependent upon huge amounts of parallel texts, its problems with morphology-rich languages (especially with translatingintosuch languages), and its inability to correct singleton errors.\nSome work has been done in the utilization of multiparallelcorpora, that is a body of text that has been translated into 3 or more languages. Using these methods, a text that has been translated into 2 or more languages may be utilized in combination to provide a more accurate translation into a third language compared with if just one of those source languages were used alone.[21][22][23]\nAdeep learning-based approach to MT,neural machine translationhas made rapid progress in recent years. However, the current consensus is that the so-called human parity achieved is not real, being based wholly on limited domains, language pairs, and certain test benchmarks[24]i.e., it lacks statistical significance power.[25]\nTranslations by neural MT tools likeDeepL Translator, which is thought to usually deliver the best machine translation results as of 2022, typically still need post-editing by a human.[26][27][28]\nInstead of training specialized translation models on parallel datasets, one can alsodirectly promptgenerativelarge language modelslikeGPTto translate a text.[29][30][31]This approach is considered promising,[32]but is still more resource-intensive than specialized translation models.\nStudies using human evaluation (e.g. by professional literary translators or human readers) havesystematically identified various issueswith the latest advanced MT outputs.[31]Some quality evaluation studies have found that, in several languages, human translations outperform ChatGPT-produced translations in terminological accuracy and clarity of expression.[33][34]Common issues include the translation of ambiguous parts whose correct translation requires common sense-like semantic language processing or context.[31]There can also be errors in the source texts, missing high-quality training data and the severity of frequency of several types of problems may not get reduced with techniques used to date, requiring some level of human active participation.\nWord-sense disambiguation concerns finding a suitable translation when a word can have more than one meaning. The problem was first raised in the 1950s byYehoshua Bar-Hillel.[35]He pointed out that without a \"universal encyclopedia\", a machine would never be able to distinguish between the two meanings of a word.[36]Today there are numerous approaches designed to overcome this problem. They can be approximately divided into \"shallow\" approaches and \"deep\" approaches.\nShallow approaches assume no knowledge of the text. They simply apply statistical methods to the words surrounding the ambiguous word. Deep approaches presume a comprehensive knowledge of the word. So far, shallow approaches have been more successful.[37]\nClaude Piron, a long-time translator for the United Nations and theWorld Health Organization, wrote that machine translation, at its best, automates the easier part of a translator's job; the harder and more time-consuming part usually involves doing extensive research to resolveambiguitiesin thesource text, which thegrammaticalandlexicalexigencies of thetarget languagerequire to be resolved:\nWhy does a translator need a whole workday to translate five pages, and not an hour or two? ..... About 90% of an average text corresponds to these simple conditions.  But unfortunately, there's the other 10%.  It's that part that requires six [more] hours of work.  There are ambiguities one has to resolve.  For instance, the author of the source text, an Australian physician, cited the example of an epidemic which was declared during World War II in a \"Japanese prisoners of war camp\".  Was he talking about an American camp with Japanese prisoners or a Japanese camp with American prisoners?  The English has two senses.  It's necessary therefore to do research, maybe to the extent of a phone call to Australia.[38]\nThe ideal deep approach would require the translation software to do all the research necessary for this kind of disambiguation on its own; but this would require a higher degree ofAIthan has yet been attained.  A shallow approach which simply guessed at the sense of the ambiguous English phrase that Piron mentions (based, perhaps, on which kind of prisoner-of-war camp is more often mentioned in a given corpus) would have a reasonable chance of guessing wrong fairly often.  A shallow approach that involves \"ask the user about each ambiguity\" would, by Piron's estimate, only automate about 25% of a professional translator's job, leaving the harder 75% still to be done by a human.\nOne of the major pitfalls of MT is its inability to translate non-standard language with the same accuracy as standard language. Heuristic or statistical based MT takes input from various sources in standard form of a language. Rule-based translation, by nature, does not include common non-standard usages. This causes errors in translation from a vernacular source or into colloquial language. Limitations on translation from casual speech present issues in the use of machine translation in mobile devices.\nIninformation extraction, named entities, in a narrow sense, refer to concrete or abstract entities in the real world such as people, organizations, companies, and places that have a proper name: George Washington, Chicago, Microsoft.  It also refers to expressions of time, space and quantity such as 1 July 2011, $500.\nIn the sentence \"Smith is the president of Fabrionix\" bothSmithandFabrionixare named entities, and can be further qualified via first name or other information; \"president\" is not, since Smith could have earlier held another position at Fabrionix, e.g. Vice President.\nThe termrigid designatoris what defines these usages for analysis in statistical machine translation.\nNamed entities must first be identified in the text; if not, they may be erroneously translated as common nouns, which would most likely not affect theBLEUrating of the translation but would change the text's human readability.[39]They may be omitted from the output translation, which would also have implications for the text's readability and message.\nTransliterationincludes finding the letters in the target language that most closely correspond to the name in the source language.  This, however, has been cited as sometimes worsening the quality of translation.[40]For \"Southern California\" the first word should be translated directly, while the second word should be transliterated.  Machines often transliterate both because they treated them as one entity.  Words like these are hard for machine translators, even those with a transliteration component, to process.\nUse of a \"do-not-translate\" list, which has the same end goal – transliteration as opposed to translation.[41]still relies on correct identification of named entities.\nA third approach is a class-based model. Named entities are replaced with a token to represent their \"class\"; \"Ted\"  and \"Erica\" would both be replaced with \"person\" class token. Then the statistical distribution and use of person names, in general, can be analyzed instead of looking at the distributions of \"Ted\" and \"Erica\" individually, so that the probability of a given name in a specific language will not affect the assigned probability of a translation. A study by Stanford on improving this area of translation gives the examples that different probabilities will be assigned to \"David is going for a walk\" and \"Ankit is going for a walk\" for English as a target language due to the different number of occurrences for each name in the training data. A frustrating outcome of the same study by Stanford (and other attempts to improve named recognition translation) is that many times, a decrease in theBLEUscores for translation will result from the inclusion of methods for named entity translation.[41]\nWhile no system provides the ideal of fully automatic high-quality machine translation of unrestricted text, many fully automated systems produce reasonable output.[42][43][44]The quality of machine translation is substantially improved if the domain is restricted and controlled.[45]This enables using machine translation as a tool to speed up and simplify translations, as well as producing flawed but useful low-cost or ad-hoc translations.\nMachine translation applications have also been released for most mobile devices, including mobile telephones, pocket PCs, PDAs, etc. Due to their portability, such instruments have come to be designated asmobile translationtools enabling mobile business networking between partners speaking different languages, or facilitating both foreign language learning and unaccompanied traveling to foreign countries without the need of the intermediation of a human translator.\nFor example, the Google Translate app allows foreigners to quickly translate text in their surrounding viaaugmented realityusing the smartphone camera that overlays the translated text onto the text.[46]It can alsorecognize speechand then translate it.[47]\nDespite their inherent limitations, MT programs are used around the world. Probably the largest institutional user is theEuropean Commission. In 2012, with an aim to replace a rule-based MT by newer, statistical-based MT@EC, The European Commission contributed 3.072 million euros (via its ISA programme).[48]\nMachine translation has also been used for translatingWikipediaarticles and could play a larger role in creating, updating, expanding, and generally improving articles in the future, especially as the MT capabilities may improve. There is a \"content translation tool\" which allows editors to more easily translate articles across several select languages.[49][50][51]English-language articles are thought to usually be more comprehensive and less biased than their non-translated equivalents in other languages.[52]As of 2022,English Wikipediahas over 6.5 million articles while, for example, theGermanandSwedish Wikipediaseach only have over 2.5 million articles,[53]each often far less comprehensive.\nFollowing terrorist attacks in Western countries, including9-11, the U.S. and its allies have been most interested in developingArabic machine translationprograms, but also in translatingPashtoandDarilanguages.[citation needed]Within these languages, the focus is on key phrases and quick communication between military members and civilians through the use of mobile phone apps.[54]The Information Processing Technology Office inDARPAhosted programs likeTIDESandBabylon translator. US Air Force has awarded a $1 million contract to develop a language translation technology.[55]\nThe notable rise ofsocial networkingon the web in recent years has created yet another niche for the application of machine translation software – in utilities such asFacebook, orinstant messagingclients such asSkype,Google Talk,MSN Messenger, etc. – allowing users speaking different languages to communicate with each other.\nLineage Wgained popularity in Japan because of its machine translation features allowing players from different countries to communicate.[56]\nDespite being labelled as an unworthy competitor to human translation in 1966 by the Automated Language Processing Advisory Committee put together by the United States government,[57]the quality of machine translation has now been improved to such levels that its application in online collaboration and in the medical field are being investigated. The application of this technology in medical settings where human translators are absent is another topic of research, but difficulties arise due to the importance of accurate translations in medical diagnoses.[58]\nResearchers caution that the use of machine translation in medicine could risk mistranslations that can be dangerous in critical situations.[59][60]Machine translation can make it easier for doctors to communicate with their patients in day to day activities, but it is recommended to only use machine translation when there is no other alternative, and that translated medical texts should be reviewed by human translators for accuracy.[61][62]\nLegal languageposes a significant challenge to machine translation tools due to its precise nature and atypical use of normal words. For this reason, specialized algorithms have been developed for use in legal contexts.[63]Due to the risk of mistranslations arising from machine translators, researchers recommend that machine translations should be reviewed by human translators for accuracy, and some courts prohibit its use informal proceedings.[64]\nThe use of machine translation in law has raised concerns about translation errors andclient confidentiality. Lawyers who use free translation tools such as Google Translate may accidentally violate client confidentiality by exposing private information to the providers of the translation tools.[63]In addition, there have been arguments that consent for a police search that is obtained with machine translation is invalid, with different courts issuing different verdicts over whether or not these arguments are valid.[59]\nThe advancements inconvolutional neural networksin recent years and in low resource machine translation (when only a very limited amount of data and examples are available for training) enabled machine translation for ancient languages, such asAkkadianand its dialects Babylonian and Assyrian.[65]\nThere are many factors that affect how machine translation systems are evaluated. These factors include the intended use of the translation, the nature of the machine translation software, and the nature of the translation process.\nDifferent programs may work well for different purposes. For example,statistical machine translation(SMT) typically outperformsexample-based machine translation(EBMT), but researchers found that when evaluating English to French translation, EBMT performs better.[66]The same concept applies for technical documents, which can be more easily translated by SMT because of their formal language.\nIn certain applications, however, e.g., product descriptions written in acontrolled language, adictionary-based machine-translationsystem has produced satisfactory translations that require no human intervention save for quality inspection.[67]\nThere are various means for evaluating the output quality of machine translation systems. The oldest is the use of human judges[68]to assess a translation's quality. Even though human evaluation is time-consuming, it is still the most reliable method to compare different systems such as rule-based and statistical systems.[69]Automatedmeans of evaluation includeBLEU,NIST,METEOR, andLEPOR.[70]\nRelying exclusively on unedited machine translation ignores the fact that communication inhuman languageis context-embedded and that it takes a person to comprehend thecontextof the original text with a reasonable degree of probability. It is certainly true that even purely human-generated translations are prone to error. Therefore, to ensure that a machine-generated translation will be useful to a human being and that publishable-quality translation is achieved, such translations must be reviewed and edited by a human.[71]The lateClaude Pironwrote that machine translation, at its best, automates the easier part of a translator's job; the harder and more time-consuming part usually involves doing extensive research to resolveambiguitiesin thesource text, which thegrammaticalandlexicalexigencies of the target language require to be resolved. Such research is a necessary prelude to the pre-editing necessary in order to provide input for machine-translation software such that the output will not bemeaningless.[72]\nIn addition to disambiguation problems, decreased accuracy can occur due to varying levels of training data for machine translating programs. Both example-based and statistical machine translation rely on a vast array of real example sentences as a base for translation, and when too many or too few sentences are analyzed accuracy is jeopardized. Researchers found that when a program is trained on 203,529 sentence pairings, accuracy actually decreases.[66]The optimal level of training data seems to be just over 100,000 sentences, possibly because as training data increases, the number of possible sentences increases, making it harder to find an exact translation match.\nFlaws in machine translation have been noted fortheir entertainment value. Two videos uploaded toYouTubein April 2017 involve two Japanesehiraganacharacters えぐ (eandgu) being repeatedly pasted into Google Translate, with the resulting translations quickly degrading into nonsensical phrases such as \"DECEARING EGG\" and \"Deep-sea squeeze trees\", which are then read in increasingly absurd voices;[73][74]the full-length version of the video currently has 7.1 million views as of August 2025.[update][75]\nIn the early 2000s, options for machine translation between spoken and signed languages were severely limited. It was a common belief that deaf individuals could use traditional translators. However, stress, intonation, pitch, and timing are conveyed much differently in spoken languages compared to signed languages. Therefore, a deaf individual may misinterpret or become confused about the meaning of written text that is based on a spoken language.[76]\nResearchers Zhao, et al. (2000), developed a prototype called TEAM (translation from English to ASL by machine) that completed English toAmerican Sign Language(ASL) translations. The program would first analyze the syntactic, grammatical, and morphological aspects of the English text. Following this step, the program accessed a sign synthesizer, which acted as a dictionary for ASL. This synthesizer housed the process one must follow to complete ASL signs, as well as the meanings of these signs. Once the entire text is analyzed and the signs necessary to complete the translation are located in the synthesizer, a computer generated human appeared and would use ASL to sign the English text to the user.[76]\nOnlyworksthat areoriginalare subject tocopyrightprotection, so some scholars claim that machine translation results are not entitled to copyright protection because MT does not involvecreativity.[77]The copyright at issue is for aderivative work; the author of theoriginal workin the original language does not lose hisrightswhen a work is translated: a translator must have permission topublisha translation.[78]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_arms_race",
        "title": "Artificial intelligence arms race - Wikipedia",
        "content": "Donald TrumpJoe BidenBarack ObamaElon MuskSundar PichaiJensen HuangSam AltmanSatya NadellaAndy JassyTim CookLisa SuMark ZuckerbergAlexandr Wang\nXi JinpingHu JintaoJiang ZeminJack MaRobin LiLiang WenfengPony MaDaniel ZhangRen ZhengfeiTan RuisongLei Jun\nUnited StatesGoogleNvidiaStargateOpenAIMicrosoftAmazonAppleTeslaMetaIBMxAIIntelBroadcomAnthropicAMDOracleFigure AILockheed MartinPalantirAnduril\nChinaBaiduDeepSeekTencentAlibabaHuaweiSenseTimeiFlytekAlphaXiaomiMegviiYMTCSilanAVIC\nEst. $300 billion(USA, over the last decade)\nEst. $200 billion (China, over the last decade)\nAI regulation(USA)DataprivacyandsecurityissuesAIbiasandfairness\nChina's Data Security Law\nA militaryartificial intelligence arms raceis an economic and military competition between two or more states to develop and deploy advancedAItechnologies andlethal autonomous weaponssystems (LAWS). The goal is to gain a strategic or tactical advantage over rivals, similar to previous arms races involving nuclear or conventional military technologies. Since the mid-2010s, many analysts have noted the emergence of such an arms race betweensuperpowersfor better AI technology andmilitary AI,[1][2]driven byincreasing geopolitical and military tensions.\nAn AI arms race is sometimes placed in the context of anAI Cold Warbetween theUnited StatesandChina.[3]Several influential figures and publications have emphasized that whoever developsartificial general intelligence(AGI) first could dominate global affairs in the 21st century. Russian PresidentVladimir Putinfamously stated that the leader in AI will \"rule the world.\"[4]Experts and analysts—from researchers likeLeopold Aschenbrennerto institutions like Lawfare and Foreign Policy—warn that the AGI race between major powers like the U.S. and China could reshape geopolitical power.[5][6]This includes AI for surveillance, autonomous weapons, decision-making systems, cyber operations, and more.\nLethal autonomous weaponssystems use artificial intelligence to identify and kill human targets without human intervention.[7]LAWS have colloquially been called \"slaughterbots\" or \"killer robots\". Broadly, any competition for superior AI is sometimes framed as an \"arms race\".[8][9]Advantages in military AI overlap with advantages in other sectors, as countries pursue both economic and military advantages, as per previous arms races throughout history.[10]\nIn 2014, AI specialistSteve Omohundrowarned that \"An autonomous weapons arms race is already taking place\".[12]According toSiemens, worldwide military spending on robotics was US$5.1 billion in 2010 and US$7.5 billion in 2015.[13][14]\nChina became a top player in artificial intelligence research in the 2010s. According to theFinancial Times, in 2016, for the first time, China published more AI research papers than the entire European Union. When restricted to number of AI papers in the top 5% of cited papers, China overtook the United States in 2016 but lagged behind the European Union.[15]23% of the researchers presenting at the 2017American Association for the Advancement of Artificial Intelligence(AAAI) conference were Chinese.[11]Eric Schmidt, the former chairman and chief executive officer ofAlphabet, has predicted China will be the leading country in AI by 2025.[16]\nOne risk concerns the AI race itself, whether or not the race is won by any one group. There are strong incentives for development teams to cut corners with regard to the safety of the system, increasing the risk of critical failures and unintended consequences.[17][18]This is in part due to the perceived advantage of being the first to develop advanced AI technology. One team appearing to be on the brink of a breakthrough can encourage other teams to take shortcuts, ignore precautions and deploy a system that is less ready. Some argue that using \"race\" terminology at all in this context can exacerbate this effect.[19]\nAnother potential danger of an AI arms race is the possibility of losing control of the AI systems; the risk is compounded in the case of a race toartificial general intelligence, which may present anexistential risk.[19]In 2023, aUnited States Air Forceofficial reportedly said that during acomputer test, a simulated AI drone killed the human character operating it. The USAF later said the official had misspoken and that it never conducted such simulations.[20]\nA third risk of an AI arms race is whether or not the race is actually won by one group. The concern is regarding the consolidation of power and technological advantage in the hands of one group.[19]A US government report argued that \"AI-enabled capabilities could be used to threaten  critical infrastructure, amplify disinformation campaigns, and wage war\"[21]:1, and that \"global stability and nuclear deterrence could be undermined\".[21]:11\nIn 2014, former Secretary of DefenseChuck Hagelposited the \"Third Offset Strategy\" that rapid advances in artificial intelligence will define the next generation of warfare.[22]According to data science and analytics firm Govini, the U.S.Department of Defense(DoD) increased investment in artificial intelligence, big data and cloud computing from $5.6 billion in 2011 to $7.4 billion in 2016.[23]However, the civilianNSFbudget for AI saw no increase in 2017.[15]Japan Timesreported in 2018 that the United States private investment is around $70 billion per year.[24]The November 2019 'Interim Report' of the United States' National Security Commission on Artificial Intelligence confirmed that AI is critical to US technological military superiority.[21]\nThe U.S. has many military AI combat programs, such as theSea Hunterautonomous warship, which is designed to operate for extended periods at sea without a single crew member, and to even guide itself in and out of port.[25]From 2017, a temporary US Department of Defense directive requires a human operator to be kept in the loop when it comes to the taking of human life by autonomous weapons systems.[26]On October 31, 2019, the United States Department of Defense's Defense Innovation Board published the draft of a report recommending principles for the ethical use of artificial intelligence by the Department of Defense that would ensure a human operator would always be able to look into the 'black box' and understand thekill-chainprocess. However, a major concern is how the report will be implemented.[27]\nThe Joint Artificial Intelligence Center (JAIC) (pronounced \"jake\")[28]is an American organization on exploring the usage of AI (particularlyedge computing),Network of Networks, and AI-enhanced communication, for use in actual combat.[29][30][31][32]It is a subdivision of theUnited States Armed Forcesand was created in June 2018. The organization's stated objective is to \"transform theUS Department of Defenseby accelerating the delivery and adoption of AI to achieve mission impact at scale. The goal is to use AI to solve large and complex problem sets that span multiple combat systems; then, ensure the combat Systems and Components have real-time access to ever-improving libraries of data sets and tools.\"[30]\nIn 2023, Microsoft pitched the DoD to useDALL-Emodels to train itsbattlefield management system.[33]OpenAI, thedeveloperof DALL-E, removed the blanket ban on military and warfare use from its usage policies in January 2024.[34]TheBiden administrationimposed restrictions on the export of advanced NVIDIA chips and GPUs to China in an effort to limit China's progress in artificial intelligence and high-performance computing. The policy aimed to prevent the use of cutting-edge U.S. technology in military or surveillance applications and to maintain a strategic advantage in the global AI race.[35]\nIn 2025, under thesecond Trump administration, the United States began a broad deregulation campaign aimed at accelerating growth in sectors critical to artificial intelligence, including nuclear energy, infrastructure, and high-performance computing.[36]The goal was to remove regulatory barriers and attract private investment to boost domestic AI capabilities. This included easing restrictions on data usage, speeding up approvals for AI-related infrastructure projects, and incentivizing innovation in cloud computing and semiconductors. Companies like NVIDIA, Oracle, and Cisco played a central role in these efforts, expanding their AI research, data center capacity, and partnerships to help position the U.S. as a global leader in AI development.[37]\nProject Maven is aPentagonproject involving using machine learning and engineering talent to distinguish people and objects in drone videos,[38]apparently giving the government real-time battlefield command and control, and the ability to track, tag and spy on targets without human involvement. Initially the effort was led byRobert O. Workwho was concerned about China's military use of the emerging technology.[39]Reportedly, Pentagon development stops short of acting as an AI weapons system capable of firing on self-designated targets.[40]The project was established in a memo by theU.S. Deputy Secretary of Defenseon 26 April 2017.[41]Also known as theAlgorithmic Warfare Cross Functional Team,[42]it is, according to Lt. Gen. of theUnited States Air ForceJack Shanahan in November 2017, a project \"designed to be that pilot project, that pathfinder, that spark that kindles the flame front of artificial intelligence across the rest of the [Defense] Department\".[43]Its chief,U.S. Marine CorpsCol. Drew Cukor, said: \"People and computers will work symbiotically to increase the ability of weapon systems to detect objects.\"[44]Project Maven has been noted by allies, such as Australia'sIan Langford, for the ability to identify adversaries by harvesting data from sensors onUAVsand satellite.[45]At the secondDefense OneTech Summit in July 2017, Cukor also said that the investment in a \"deliberate workflow process\" was funded by the Department [of Defense] through its \"rapid acquisition authorities\" for about \"the next 36 months\".[46]\nThe U.S.Department of Defenseis partnering with Ukraine on \"Project Artemis\" to develop advanced drones that can withstand electronic warfare, blending Ukrainian simplicity and adaptability with American precision. Due to theRussia-Ukraine war, Ukraine has emerged as a leader in drone production and warfare, creating cost-effective systems that challenge traditional approaches. Countries like Turkey, China, and Iran are also producing affordable drones, reducing America's monopoly and reshaping warfare dynamics. U.S. efforts are focused on integrating AI,drone swarmtechnology, and hybrid drone systems to maintain military dominance. The democratization of drone technology raises issues, such as autonomous decision-making, counter-drone defenses, and dual-use concerns, that challenge ethical and security norms.[47]\nThe Stargate Projectis a joint venture announced in 2025 by OpenAI CEO Sam Altman, U.S. President Donald Trump, Oracle Corporation, MGX, SoftBank Group, and other partners. The initiative aims to develop large-scale artificial intelligence (AI) infrastructure in the United States, with a projected $500 billion investment by 2029. The project focuses on building advanced data centers, custom AI hardware, and sustainable energy systems, while also supporting research, workforce development, and national AI competitiveness. It is considered an effort to position the U.S. as a global leader in AI technology.[48]The program has been compared to theManhattan Projectbecause of its large scale.[49]\nChina is pursuing a strategic policy ofmilitary-civil fusionon AI for globaltechnological supremacy.[21][50]According to a February 2019 report by Gregory C. Allen of theCenter for a New American Security,China'sleadership– includingGeneral Secretary of the Chinese Communist PartyXi Jinping– believes that being at the forefront in AI technology is critical to the future of global military and economic power competition.[10]Chinese military officials have said that their goal is to incorporate commercial AI technology to \"narrow the gap between the Chinese military and global advanced powers.\"[10]The close ties between Silicon Valley and China, and the open nature of the American research community, has made the West's most advanced AI technology easily available to China; in addition, Chinese industry has numerous home-grown AI accomplishments of its own, such asBaidupassing a notable Chinese-language speech recognition capability benchmark in 2015.[51]As of 2017, Beijing's roadmap aims to create a $150 billion AI industry by 2030.[15]Before 2013, Chinese defense procurement was mainly restricted to a few conglomerates; however, as of 2017, China often sources sensitive emerging technology such as drones and artificial intelligence from private start-up companies.[52]An October 2021 report by theCenter for Security and Emerging Technologyfound that \"Most of the [Chinese military]'s AI equipment suppliers are not state-owned defense enterprises, but private Chinese tech companies founded after 2010.\"[53]The report estimated that Chinese military spending on AI exceeded $1.6 billion each year.[53]TheJapan Timesreported in 2018 that annual private Chinese investment in AI is under $7 billion per year. AI startups in China received nearly half of total global investment in AI startups in 2017; the Chinese filed for nearly five times as many AI patents as did Americans.[24]\nChina published a position paper in 2016 questioning the adequacy of existing international law to address the eventuality of fully autonomous weapons, becoming the first permanent member of theU. N. Security Councilto broach the issue.[54]In 2018, CCP general secretaryXi Jinpingcalled for greater international cooperation in basic AI research.[55]Chinese officials have expressed concern that AI such as drones could lead to accidental war, especially in the absence of international norms.[56]In 2019, formerUnited States Secretary of DefenseMark Esperlashed out at China for selling drones capable of taking life with no human oversight.[57]\nThe focus on \"intelligentized AI warfare\", pursued by China, suggests a comprehensive integration of AI across all domains (land, sea, air, space, and cyber) for autonomous attack, defence and cognitive warfare.[58]The intelligentized strategy is distinct from traditional warfare, which focuses on network-centric operations, and instead sees AI as a force multiplier that enhances decision-making, command structures, and autonomous capabilities. Unlike traditional warfare, intelligentization leverages AI to create a cognitive advantage—allowing it to process battlefield information better. AI-assisted command-and-control (C2) systems, predictive analytics, and real-time data fusion, enabling accelerated human-AI hybrid decision-making.\nAutonomous systems, including drone swarms, AI-powered cyber warfare, play a crucial role in this strategy.\nChina is reported to be currently developing wingman drones, robotic ground forces, and optimised logistics to enhance combat effectiveness.[59]TheChinese army (PLA)) also emphasisescognitive warfareusing AI-driven psychological operations, social media manipulation, and predictive behavioural analysis to influence adversaries and the importance of dynamic responses where AI enhances hacking capabilities, automated SIGINT (Signals Intelligence) and adaptive tactics. However, despite this focus, some analysts believe China could be struggling to fully realise AI capability within the military environment: a \"comprehensive review of dozens of Chinese-language journal articles about AI and warfare reveals that Chinese defense experts claim that Beijing is facing several technological challenges that may hinder its ability to capitalize on the advantages provided by military AI\"[60]\nA task force for the Strategic Implementation of AI for National Security and Defence was established in February 2018 by theMinistry of Defense's Department of Defence Production.[61]The process of getting the military ready for AI use was started by theMoDin 2019.[62]TheCentre for Artificial Intelligence and Roboticswas approved to develop AI solutions to improveintelligence collectionand analysis capabilities.[63]In 2021, theIndian Army, with assistance from theNational Security Council, began operating the Quantum Lab and Artificial Intelligence Center at theMilitary College of Telecommunication Engineering. With an emphasis on robotics and artificial intelligence,Defence Research and Development OrganisationandIndian Institute of Scienceestablished the Joint Advanced Technology Programme-Center of Excellence.[64][65]In 2022, theIndian Navycreated an AI Core group and set up a Center of Excellence for AI andBig Data analysisatINS Valsura.[66][67]Indian Army incubated Artificial Intelligence Offensive Drone Operations Project.[68][69]During Exercise Dakshin Shakti 2021, the Indian Army integrated AI into itsintelligence,surveillance, andreconnaissancearchitecture.[70]\nIn 2022, the Indian government established theDefence Artificial Intelligence Counciland theDefence AI Project Agency,[71][72]and it also published a list of 75 defense-related AI priority projects.[73][74]MoDearmarked₹1,000croreannually till 2026 for capacity building, infrastructure setup, data preparation, and Al project implementation.[75]The Indian Army, the Indian Navy and theIndian Air Forceset aside ₹100 crore annually for the development of AI-specific applications.[76]The military is already deploying some AI-enabled projects and equipment.[77][78]At Air Force Station Rajokri, theIAFCentre of Excellence for Artificial Intelligence was established in 2022 as part of the Unit for Digitization, Automation, Artificial Intelligence, and Application Networking (UDAAN).[79]Swarm drone systemswere introduced by theMechanised Infantry Regimentfor offensive operations close to theLine of Actual Control.[80]\nFor offensive operations, the military began acquiring AI-enabledUAVsandswarm drones.[81][82][83]Bharat Electronicsdeveloped AI-enabled audio transcription and analysis software for battlefield communication. Using AI during transport operations, the Indian Army's Research & Development branch patented driver tiredness monitoring system.[84][85]As part of initial investment, theIndian Armed Forcesis investing about $50 million (€47.2 million) yearly on AI, according to Delhi Policy Group.[86]Forhigh altitudelogisticsat forward outposts,military robotsare deployed.[87][88]Army is developing autonomous combat vehicles, robotic surveillance platforms, and Manned-Unmanned Teaming (MUM-T) solutions as part of the Defence AI roadmap.[89]MCTEis working with theMinistry of Electronics and Information Technologyand,Society for Applied Microwave Electronics Engineering & Research, on AI and military-gradechipset.[90][91]Phase III of AI-enabled space-based surveillance has been authorized.[92][93]\nDRDOChairman and Secretary of the Department of Defense Research & Development Samir V. Kamat said the agency started concentrating on the potential use of AI in the development of military systems and subsystems.[94]The Indian government intends to leverage the private sector's sizable AI workforce anddual-use technologiesfor defense by 2026.[95]In order to conduct research on autonomous platforms, improved surveillance,predictive maintenance, andintelligent decision support system, the Indian Army AI Incubation Center was established.[96]Indian Navy launchedINS Suratwith AI capabilities.[97][98]\nRussian GeneralViktor Bondarev, commander-in-chief of the Russian air force, stated that as early as February 2017, Russia was working on AI-guided missiles that could decide to switch targets mid-flight.[99]TheMilitary-Industrial Commission of Russiahas approved plans to derive 30 percent of Russia's combat power from remote controlled and AI-enabled robotic platforms by 2030.[100]Reports by state-sponsored Russian media on potential military uses of AI increased in mid-2017.[101]In May 2017, the CEO of Russia's Kronstadt Group, a defense contractor, stated that \"there already exist completely autonomous AI operation systems that provide the means for UAV clusters, when they fulfill missions autonomously, sharing tasks between them, and interact\", and that it is inevitable that \"swarms of drones\" will one day fly over combat zones.[102]Russia has been testing several autonomous and semi-autonomous combat systems, such asKalashnikov's \"neural net\" combat module, with a machine gun, a camera, and an AI that its makers claim can make its own targeting judgements without human intervention.[25]\nIn September 2017, during a National Knowledge Day address to over a million students in 16,000 Russian schools, Russian PresidentVladimir Putinstated \"Artificial intelligence is the future, not only for Russia but for all humankind... Whoever becomes the leader in this sphere will become the ruler of the world\". Putin also said it would be better to prevent any single actor achieving a monopoly, but that if Russia became the leader in AI, they would share their \"technology with the rest of the world, like we are doing now with atomic and nuclear technology\".[103][104][105]\nRussia is establishing a number of organizations devoted to the development of military AI. In March 2018, the Russian government released a 10-point AI agenda, which calls for the establishment of an AI and Big Data consortium, a Fund for Analytical Algorithms and Programs, a state-backed AI training and education program, a dedicated AI lab, and a National Center for Artificial Intelligence, among other initiatives.[106]In addition, Russia recently created a defense research organization, roughly equivalent to DARPA, dedicated to autonomy and robotics called the Foundation for Advanced Studies, and initiated an annual conference on \"Robotization of the Armed Forces of the Russian Federation.\"[107][108]\nThe Russian military has been researching a number of AI applications, with a heavy emphasis on semiautonomous and autonomous vehicles. In an official statement on November 1, 2017, Viktor Bondarev, chairman of the Federation Council's Defense and Security Committee, stated that \"artificial intelligence will be able to replace a soldier on the battlefield and a pilot in an aircraft cockpit\" and later noted that \"the day is nearing when vehicles will get artificial intelligence.\"[109]Bondarev made these remarks in close proximity to the successful test of Nerehta, an crewless Russian ground vehicle that reportedly \"outperformed existing [crewed] combat vehicles.\" Russia plans to use Nerehta as a research and development platform for AI and may one day deploy the system in combat, intelligence gathering, or logistics roles.[110]Russia has also reportedly built a combat module for crewless ground vehicles that is capable of autonomous target identification—and, potentially, target engagement—and plans to develop a suite of AI-enabled autonomous systems.[111][112][108]\nIn addition, the Russian military plans to incorporate AI into crewless aerial, naval, and undersea vehicles and is currently developing swarming capabilities.[107]It is also exploring innovative uses of AI for remote sensing and electronic warfare, including adaptive frequency hopping, waveforms, and countermeasures.[113][114]Russia has also made extensive use of AI technologies for domestic propaganda and surveillance, as well as for information operations directed against the United States and U.S. allies.[115][116][108]\nThe Russian government has strongly rejected any ban onlethal autonomous weaponsystems, suggesting that such an international ban could be ignored.[117][118]\nThe Russian invasion of Ukraine and the ensuing Russia-Ukraine war has seen seen significant use of AI by both sides and has also been characterised as a drone war.[119]Advances in AI-powered GPS-denied navigation and drone swarming techniques are significantly improving operational capabilities for Ukraine. Fully realised drone swarms, where multiple drones coordinate and make decisions autonomously, are still in the early stages of experimentation but Ukraine is exploring and implementing these techniques in a real conflict situation.[120]The Defense Intelligence of Ukraine (DIU) has been at the forefront of utilizing drones with some elements of autonomy for conducting long-range strikes into Russian territory. Domestic drone production has significantly expanded, with approximately 2 million drones produced in 2024, 96.2% of which were domestically manufactured.[121]\nRather than replacing human involvement, AI is primarily serving to augment existing capabilities, enhancing the speed, accuracy, and overall efficiency of numerous military functions.\nPerhaps the most important way in which AI has been used by Ukraine is in intelligence, surveillance, and reconnaissance (ISR) capabilities. The Ukrainian military uses Palantir's MetaConstellation software to monitor the movement of Russian troops and supplies (highlighting the blurring of boundaries between state military and commercial AI use). It aggregates data from various commercial civilian providers of satellite imagery Ukraine also uses its own Delta system which aggregates real time data from drone imagery, satellite photos, acoustic signals, and text to construct an operational picture for military commanders. AI is used to prioritise incoming threats, potential targets and resource constraints.[122]\nAI is also being used to process intercepted communications from Russian soldiers, to process, select, and output militarily useful information from these intercepted calls.\nIsrael has made extensive use of AI for military applications specially during theGaza war. The main AI systems used fortarget identificationare the Gospel and Lavender. Lavender developed by the Unit 8200 identifies and creates a database of individuals mostly low-ranking militants of Hamas and the Palestinian Islamic Jihad and has a 90% accuracy rate and a database of tens of thousands.  The Gospel in comparisons recommended buildings and structures rather than individuals. The acceptable collateral damage and the type of weapon used to eliminate the target is decided by IDF members and could track militants even when at home.[123]\nIsrael'sHarpyanti-radar \"fire and forget\" drone is designed to be launched by ground troops, and autonomously fly over an area to find and destroy radar that fits pre-determined criteria.[124]The application of artificial intelligence is also expected to be advanced in crewless ground systems and robotic vehicles such as the Guardium MK III and later versions.[125]These robotic vehicles are used in border defense.\nIn 2015, the UK government opposed a ban on lethal autonomous weapons, stating that \"international humanitarian law already provides sufficient regulation for this area\", but that all weapons employed by UK armed forces would be \"under human oversight and control\".[126]\nThe South KoreanSuper aEgis IImachine gun, unveiled in 2010, sees use both in South Korea and in the Middle East. It can identify, track, and destroy a moving target at a range of 4 km. While the technology can theoretically operate without human intervention, in practice safeguards are installed to require manual input. A South Korean manufacturer states, \"Our weapons don't sleep, like humans must. They can see in the dark, like humans can't. Our technology therefore plugs the gaps in human capability\", and they want to \"get to a place where our software can discern whether a target is friend, foe, civilian or military\".[127]\nSaudi Arabia entered the AI race relatively late, beginning in the early 2020s. The country announced itsVision 2030initiative—a multi-trillion dollar plan to diversify its oil-dependent economy—under the leadership of thePublic Investment Fund(PIF). A key turning point in U.S.-Saudi relations came during President Donald Trump's first foreign trip in 2017, when he visitedRiyadhand signed hundreds of billions of dollars in agreements spanning defense, energy, and technology. This visit laid the groundwork for deeper U.S.-Saudi cooperation in areas like AI and tech infrastructure. In the years that followed, Saudi Arabia formed major partnerships with U.S. firms like NVIDIA, AMD, and Cisco, investing billions in semiconductors, cloud computing, and AI research.[128]Saudi-backed startupHumainalso partnered with several American firms, further strengthening the Kingdom's ties with Silicon Valley as it pushed to become a global leader in artificial intelligence by 2030.[129]\nThe United Arab Emirates has been expanding its role in artificial intelligence and technology through investments in infrastructure and partnerships. One major initiative isMGX, a UAE-backed technology group focused on AI development. In 2025, U.S. President Donald Trump visited the UAE, where he met with Emirati officials and business leaders. The visit included discussions on technology and economic cooperation, including potential collaborations with U.S. companies such as Oracle, NVIDIA, and Cisco.[130]These talks focused on areas like data centers, AI hardware, and advanced computing, reflecting ongoing efforts by the UAE to strengthen its technological capabilities through international partnerships. NVIDIA, OpenAI, and Cisco have announced plans to collaborate on building one of the world's largest data centers in the United Arab Emirates. The project is part of the UAE's broader strategy to become a global technology and AI hub. The data center will support advanced cloud computing, AI model training, and data storage capabilities.[131]\nThe European Parliament holds the position that humans must have oversight and decision-making power over lethal autonomous weapons.[132]However, it is up to each member state of the European Union to determine their stance on the use of autonomous weapons and the mixed stances of the member states is perhaps the greatest hindrance to the European Union's ability to develop autonomous weapons. Some members such as France, Germany, Italy, and Sweden are developing lethal autonomous weapons. Some members remain undecided about the use of autonomous military weapons and Austria has even called to ban the use of such weapons.[133]\nSome EU member states have developed and are developing automated weapons. Germany has developed anactive protection system, the Active Defense System, that can respond to a threat with complete autonomy in less than a millisecond.[133][134]Italy plans to incorporate autonomous weapons systems into its future military plans.[133]\nThe international regulation of autonomous weapons is an emerging issue for international law.[135]AI arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts, together with a legal and political verification process.[1][2]As early as 2007, scholars such as AI professorNoel Sharkeyhave warned of \"an emerging arms race among the hi-tech nations to develop autonomous submarines, fighter jets, battleships and tanks that can find their own targets and apply violent force without the involvement of meaningful human decisions\".[136][137]\nMiles Brundage of theUniversity of Oxfordhas argued an AI arms race might be somewhat mitigated through diplomacy: \"We saw in the various historical arms races that collaboration and dialog can pay dividends\".[138]Over a hundred experts signed an open letter in 2017 calling on the UN to address the issue of lethal autonomous weapons;[126][139]however, at a November 2017 session of the UNConvention on Certain Conventional Weapons(CCW), diplomats could not agree even on how to define such weapons.[140]The Indian ambassador and chair of the CCW stated that agreement on rules remained a distant prospect.[141]As of 2019, 26 heads of state and 21 Nobel Peace Prize laureates have backed a ban on autonomous weapons.[142]However, as of 2022, most major powers continue to oppose a ban on autonomous weapons.[143]\nMany experts believe attempts to completely ban killer robots are likely to fail,[144]in part because detecting treaty violations would be extremely difficult.[145][146]A 2017 report fromHarvard'sBelfer Centerpredicts that AI has the potential to be as transformative as nuclear weapons.[138][147][148]The report further argues that \"Preventing expanded military use of AI is likely impossible\" and that \"the more modest goal of safe and effective technology management must be pursued\", such as banning the attaching of an AIdead man's switchto a nuclear arsenal.[148]\nA 2015 open letter by theFuture of Life Institutecalling for the prohibition of lethal autonomous weapons systems has been signed by over 26,000 citizens, including physicistStephen Hawking,TeslamagnateElon Musk,Apple'sSteve WozniakandTwitterco-founderJack Dorsey, and over 4,600 artificial intelligence researchers, includingStuart Russell,Bart SelmanandFrancesca Rossi.[149][140]The Future of Life Institute has also released two fictional films,Slaughterbots(2017) andSlaughterbots - if human: kill()(2021), which portray threats of autonomous weapons and promote a ban, both of which went viral.\nProfessorNoel Sharkeyof theUniversity of Sheffieldargues that autonomous weapons will inevitably fall into the hands of terrorist groups such as theIslamic State.[150]\nMany Western tech companies avoid being associated too closely with the U.S. military, for fear of losing access to China's market.[51]Furthermore, some researchers, such asDeepMindCEODemis Hassabis, are ideologically opposed to contributing to military work.[151]\nFor example, in June 2018, company sources atGooglesaid that top executiveDiane Greenetold staff that the company would not follow-up Project Maven after the current contract expired in March 2019.[38]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Machine_learning_in_physics",
        "title": "Machine learning in physics - Wikipedia",
        "content": "Applyingmachine learning(ML) (includingdeep learning) methods to the study of quantum systems is an emergent area of physics research. A basic example of this isquantum state tomography, where a quantum state is learned from measurement.[1]Other examples include learning Hamiltonians,[2][3]learning quantumphase transitions,[4][5]and automatically generating new quantum experiments.[6][7][8][9]ML is effective at processing large amounts of experimental or calculated data in order to characterize an unknown quantum system, making its application useful in contexts includingquantum information theory, quantum technology development, and computational materials design. In this context, for example, it can be used as a tool to interpolate pre-calculatedinteratomic potentials,[10]or directly solving theSchrödinger equationwith avariational method.[11]\nThe ability to experimentally control and prepare increasingly complex quantum systems brings with it a growing need to turn large and noisy data sets into meaningful information. This is a problem that has already been studied extensively in the classical setting, and consequently, many existing machine learning techniques can be naturally adapted to more efficiently address experimentally relevant problems. For example,Bayesianmethods and concepts ofalgorithmic learningcan be fruitfully applied to tackle quantum state classification,[12]Hamiltonian learning,[13]and the characterization of an unknownunitary transformation.[14][15]Other problems that have been addressed with this approach are given in the following list:\nQuantum machine learning can also be applied to dramatically accelerate the prediction of quantum properties of molecules and materials.[26]This can be helpful for the computational design of new molecules or materials. Some examples include\nVariational circuits are a family of algorithms which utilize training based on circuit parameters and an objective function.[33]Variational circuits are generally composed of a classical device communicating input parameters (random or pre-trained parameters) into a quantum device, along with a classicalMathematical optimizationfunction. These circuits are very heavily dependent on the architecture of the proposed quantum device because parameter adjustments are adjusted based solely on the classical components within the device.[34]Though the application is considerably infantile in the field of quantum machine learning, it has incredibly high promise for more efficiently generating efficient optimization functions.\nMachine learning techniques can be used to find a better manifold of integration for path integrals in order to avoid the sign problem.[35]\nAdeep learningsystem was reported to learn intuitive physics from visual data (of virtual 3D environments) based on anunpublishedapproach inspired by studies of visual cognition in infants.[40][39]Other researchers have developed a machine learning algorithm that could discover sets of basic variables of various physical systems and predict the systems' future dynamics from video recordings of their behavior.[41][42]In the future, it may be possible that such can be used to automate the discovery of physical laws of complex systems.[41]Beyond discovery and prediction, \"blank slate\"-type of learning of fundamental aspects of the physical world may have further applications such as improving adaptive and broadartificial general intelligence.[additional citation(s) needed]In specific, prior machine learning models were \"highly specialised and lack a general understanding of the world\".[40]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/List_of_artificial_intelligence_projects",
        "title": "List of artificial intelligence projects - Wikipedia",
        "content": "The following is a list of current and past, non-classified notableartificial intelligenceprojects."
    },
    {
        "url": "https://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence",
        "title": "Philosophy of artificial intelligence - Wikipedia",
        "content": "Thephilosophy of artificial intelligenceis a branch of thephilosophy of mindand thephilosophy of computer science[1]that exploresartificial intelligenceand its implications for knowledge and understanding ofintelligence,ethics,consciousness,epistemology,[2][3]andfree will.[4][5]Furthermore, the technology is concerned with the creation of artificial animals or artificial people (or, at least, artificial creatures; seeartificial life) so the discipline is of considerable interest to philosophers.[6]These factors contributed to the emergence of the philosophy of artificial intelligence.\nThe philosophy of artificial intelligence attempts to answer such questions as follows:[7]\nQuestions like these reflect the divergent interests ofAI researchers,cognitive scientistsandphilosophersrespectively. The scientific answers to these questions depend on the definition of \"intelligence\" and \"consciousness\" and exactly which \"machines\" are under discussion.\nImportantpropositionsin the philosophy of AI include some of the following:\nIs it possible to create a machine that can solveallthe problems humans solve using their intelligence? This question defines the scope of what machines could do in the future and guides the direction of AI research. It only concerns thebehaviorof machines and ignores the issues of interest topsychologists, cognitive scientists andphilosophers, evoking the question: does it matter whether a machine isreallythinking, as a person thinks, rather than just producing outcomes that appear to result from thinking?[13]\nThe basic position of most AI researchers is summed up in this statement, which appeared in the proposal for theDartmouth workshopof 1956:\nArguments against the basic premise must show that building a working AI system is impossible because there is some practical limit to the abilities of computers or that there is some special quality of the human mind that is necessary for intelligent behavior and yet cannot be duplicated by a machine (or by the methods of current AI research). Arguments in favor of the basic premise must show that such a system is possible.\nIt is also possible to sidestep the connection between the two parts of the above proposal. For instance, machine learning, beginning with Turing's infamouschild machineproposal,[14]essentially achieves the desired feature of intelligence without a precise design-time description as to how it would exactly work. The account on robottacit knowledge[15]eliminates the need for a precise description altogether.\nThe first step to answering the question is to clearly define \"intelligence\".\nAlan Turing[17]reduced the problem of defining intelligence to a simple question about conversation. He suggests that: if a machine can answeranyquestion posed to it, using the same words that an ordinary person would, then we may call that machine intelligent. A modern version of his experimental design would use an onlinechat room, where one of the participants is a real person and one of the participants is a computer program. The program passes the test if no one can tell which of the two participants is human.[8]Turing notes that no one (except philosophers) ever asks the question \"can people think?\" He writes \"instead of arguing continually over this point, it is usual to have a polite convention that everyone thinks\".[18]Turing's test extends this polite convention to machines:\nOne criticism of theTuring testis that it only measures the \"humanness\" of the machine's behavior, rather than the \"intelligence\" of the behavior. Since human behavior and intelligent behavior are not exactly the same thing, the test fails to measure intelligence.Stuart J. RussellandPeter Norvigwrite that \"aeronautical engineering texts do not define the goal of their field as 'making machines that fly so exactly like pigeons that they can fool other pigeons'\".[19]\nTwenty-first century AI research defines intelligence in terms of goal-directed behavior. It views intelligence as a set of problems that the machine is expected to solve – the more problems it can solve, and the better its solutions are, the more intelligent the program is. AI founderJohn McCarthydefined intelligence as \"the computational part of the ability to achieve goals in the world.\"[20]\nStuart RussellandPeter Norvigformalized this definition using abstractintelligent agents. An \"agent\" is something which perceives and acts in an environment. A \"performance measure\" defines what counts as success for the agent.[21]\nDefinitions like this one try to capture the essence of intelligence. They have the advantage that, unlike the Turing test, they do not also test for unintelligent human traits such as making typing mistakes.[23]They have the disadvantage that they can fail to differentiate between \"things that think\" and \"things that do not\". By this definition, even a thermostat has a rudimentary intelligence.[24]\nHubert Dreyfusdescribes this argument as claiming that \"if the nervous system obeys the laws of physics and chemistry, which we have every reason to suppose it does, then ... we ... ought to be able to reproduce the behavior of the nervous system with some physical device\".[25]This argument, first introduced as early as 1943[26]and vividly described byHans Moravecin 1988,[27]is now associated with futuristRay Kurzweil, who estimates that computer power will be sufficient for a complete brain simulation by the year 2029.[28]A non-real-time simulation of a thalamocortical model that has the size of the human brain (1011neurons) was performed in 2005,[29]and it took 50 days to simulate 1 second of brain dynamics on a cluster of 27 processors.\nEven AI's harshest critics (such asHubert DreyfusandJohn Searle) agree that a brain simulation is possible in theory.[a]However, Searle points out that, in principle,anythingcan be simulated by a computer; thus, bringing the definition to its breaking point leads to the conclusion that any process at all can technically be considered \"computation\". \"What we wanted to know is what distinguishes the mind from thermostats and livers,\" he writes.[32]Thus, merely simulating the functioning of a living brain would in itself be an admission of ignorance regarding intelligence and the nature of the mind, like trying to build a jet airliner by copying a living bird precisely, feather by feather, with no theoretical understanding ofaeronautical engineering.[33]\nIn 1963,Allen NewellandHerbert A. Simonproposed that \"symbol manipulation\" was the essence of both human and machine intelligence. They wrote:\nThis claim is very strong: it implies both that human thinking is a kind of symbol manipulation (because a symbol system isnecessaryfor intelligence) and that machines can be intelligent (because a symbol system issufficientfor intelligence).[34]Another version of this position was described by philosopher Hubert Dreyfus, who called it \"the psychological assumption\":\nThe \"symbols\" that Newell, Simon and Dreyfus discussed were word-like and high level—symbols that directly correspond with objects in the world, such as <dog> and <tail>. Most AI programs written between 1956 and 1990 used this kind of symbol. Modern AI, based on statistics and mathematical optimization, does not use the high-level \"symbol processing\" that Newell and Simon discussed.\nThese arguments show that human thinking does not consist (solely) of high level symbol manipulation. They donotshow that artificial intelligence is impossible, only that more than symbol processing is required.\nIn 1931,Kurt Gödelproved with anincompleteness theoremthat it is always possible to construct a \"Gödelstatement\" that a given consistentformal systemof logic (such as a high-level symbol manipulation program) could not prove. Despite being a true statement, the constructed Gödel statement is unprovable in the given system. (The truth of the constructed Gödel statement is contingent on the consistency of the given system; applying the same process to a subtly inconsistent system will appear to succeed, but will actually yield a false \"Gödel statement\" instead.)[citation needed]More speculatively, Gödel conjectured that the human mind can eventually correctly determine the truth or falsity of any well-grounded mathematical statement (including any possible Gödel statement), and that therefore the human mind's power is not reducible to amechanism.[36]PhilosopherJohn Lucas(since 1961) andRoger Penrose(since 1989) have championedthis philosophical anti-mechanist argument.[37]\nGödelian anti-mechanist arguments tend to rely on the innocuous-seeming claim that a system of human mathematicians (or some idealization of human mathematicians) is both consistent (completely free of error) and believes fully in its own consistency (and can make all logical inferences that follow from its own consistency, including belief in its Gödel statement)[citation needed]. This is probably impossible for a Turing machine to do (seeHalting problem); therefore, the Gödelian concludes that human reasoning is too powerful to be captured by a Turing machine, and by extension, any digital mechanical device.\nHowever, the modern consensus in the scientific and mathematical community is that actual human reasoning is inconsistent; that any consistent \"idealized version\"Hof human reasoning would logically be forced to adopt a healthy but counter-intuitive open-minded skepticism about the consistency ofH(otherwiseHis provably inconsistent); and that Gödel's theorems do not lead to any valid argument that humans have mathematical reasoning capabilities beyond what a machine could ever duplicate.[38][39][40]This consensus that Gödelian anti-mechanist arguments are doomed to failure is laid out strongly inArtificial Intelligence: \"anyattempt to utilize (Gödel's incompleteness results) to attack thecomputationalistthesis is bound to be illegitimate, since these results are quite consistent with the computationalist thesis.\"[41]\nStuart RussellandPeter Norvigagree that Gödel's argument does not consider the nature of real-world human reasoning. It applies to what can theoretically be proved, given an infinite amount of memory and time. In practice, real machines (including humans) have finite resources and will have difficulty proving many theorems. It is not necessary to be able to prove everything in order to be an intelligent person.[42]\nLess formally,Douglas Hofstadter, in hisPulitzer Prizewinning bookGödel, Escher, Bach: An Eternal Golden Braid,states that these \"Gödel-statements\" always refer to the system itself, drawing an analogy to the way theEpimenides paradoxuses statements that refer to themselves, such as \"this statement is false\" or \"I am lying\".[43]But, of course, theEpimenides paradoxapplies to anything that makes statements, whether it is a machineora human, even Lucas himself. Consider:\nThis statement is true but cannot be asserted by Lucas. This shows that Lucas himself is subject to the same limits that he describes for machines, as are all people, and soLucas's argument is pointless.[45]\nAfter concluding that human reasoning is non-computable, Penrose went on to controversially speculate that some kind of hypothetical non-computable processes involving the collapse ofquantum mechanicalstates give humans a special advantage over existing computers. Existing quantum computers are only capable of reducing the complexity of Turing computable tasks and are still restricted to tasks within the scope of Turing machines.[citation needed][clarification needed]. By Penrose and Lucas's arguments, the fact that quantum computers are only able to complete Turing computable tasks implies that they cannot be sufficient for emulating the human mind.[citation needed]Therefore, Penrose seeks for some other process involving new physics, for instance quantum gravity which might manifest new physics at the scale of thePlanck massvia spontaneous quantum collapse of the wave function. These states, he suggested, occur both within neurons and also spanning more than one neuron.[46]However, other scientists point out that there is no plausible organic mechanism in the brain for harnessing any sort of quantum computation, and furthermore that the timescale of quantum decoherence seems too fast to influence neuron firing.[47]\nHubert Dreyfusargued that human intelligenceand expertise depended primarily on fast intuitive judgements rather than step-by-step symbolic manipulation, and argued that these skills would never be captured in formal rules.[48]\nDreyfus's argument had been anticipated by Turing in his 1950 paperComputing machinery and intelligence, where he had classified this as the \"argument from the informality of behavior.\"[49]Turing argued in response that, just because we do not know the rules that govern a complex behavior, this does not mean that no such rules exist. He wrote: \"we cannot so easily convince ourselves of the absence of complete laws of behaviour ... The only way we know of for finding such laws is scientific observation, and we certainly know of no circumstances under which we could say, 'We have searched enough. There are no such laws.'\"[50]\nRussell and Norvig point out that, in the years since Dreyfus published his critique, progress has been made towards discovering the \"rules\" that govern unconscious reasoning.[51]Thesituatedmovement inroboticsresearch attempts to capture our unconscious skills at perception and attention.[52]Computational intelligenceparadigms, such asneural nets,evolutionary algorithmsand so on are mostly directed at simulated unconscious reasoning and learning.Statistical approaches to AIcan make predictions which approach the accuracy of human intuitive guesses. Research intocommonsense knowledgehas focused on reproducing the \"background\" or context of knowledge. In fact, AI research in general has moved away from high level symbol manipulation, towards new models that are intended to capture more of ourintuitivereasoning.[51]\nCognitive science and psychology eventually came to agree with Dreyfus' description of human expertise.Daniel Kahnemannand others developed a similar theory where they identified two \"systems\" that humans use to solve problems, which he called \"System 1\" (fast intuitive judgements) and \"System 2\" (slow deliberate step by step thinking).[53]\nAlthough Dreyfus' views have been vindicated in many ways, the work in cognitive science and in AI was in response to specific problems in those fields and was not directly influenced by Dreyfus. Historian and AI researcherDaniel Crevierwrote that \"time has proven the accuracy and perceptiveness of some of Dreyfus's comments. Had he formulated them less aggressively, constructive actions they suggested might have been taken much earlier.\"[54]\nThis is a philosophical question, related to theproblem of other mindsand thehard problem of consciousness. The question revolves around a position defined byJohn Searleas \"strong AI\":\nSearle distinguished this position from what he called \"weak AI\":\nSearle introduced the terms to isolate strong AI from weak AI so he could focus on what he thought was the more interesting and debatable issue. He argued thateven if we assumethat we had a computer program that acted exactly like a human mind, there would still be a difficult philosophical question that needed to be answered.[11]\nNeither of Searle's two positions are of great concern to AI research, since they do not directly answer the question \"can a machine display general intelligence?\" (unless it can also be shown that consciousness isnecessaryfor intelligence). Turing wrote \"I do not wish to give the impression that I think there is no mystery about consciousness… [b]ut I do not think these mysteries necessarily need to be solved before we can answer the question [of whether machines can think].\"[55]Russelland Norvig agree: \"Most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\"[56]\nThere are a few researchers who believe that consciousness is an essential element in intelligence, such asIgor Aleksander,Stan Franklin,Ron Sun, andPentti Haikonen, although their definition of \"consciousness\" strays very close to \"intelligence\". (Seeartificial consciousness.)\nBefore we can answer this question, we must be clear what we mean by \"minds\", \"mental states\" and \"consciousness\".\nThe words \"mind\" and \"consciousness\" are used by different communities in different ways. Somenew agethinkers, for example, use the word \"consciousness\" to describe something similar toBergson's \"élan vital\": an invisible, energetic fluid that permeates life and especially the mind.Science fictionwriters use the word to describe someessentialproperty that makes us human: a machine or alien that is \"conscious\" will be presented as a fully human character, with intelligence, desires,will, insight, pride and so on. (Science fiction writers also use the words \"sentience\", \"sapience\", \"self-awareness\" or \"ghost\"—as in theGhost in the Shellmanga and anime series—to describe this essential human property). For others[who?], the words \"mind\" or \"consciousness\" are used as a kind of secular synonym for thesoul.\nForphilosophers,neuroscientistsandcognitive scientists, the words are used in a way that is both more precise and more mundane: they refer to the familiar, everyday experience of having a \"thought in your head\", like a perception, a dream, an intention or a plan, and to the way weseesomething,knowsomething,meansomething orunderstandsomething.[57]\"It's not hard to give a commonsense definition of consciousness\" observes philosopher John Searle.[58]What is mysterious and fascinating is not so muchwhatit is buthowit is: how does a lump of fatty tissue and electricity give rise to this (familiar) experience of perceiving, meaning or thinking?\nPhilosophers call this thehard problem of consciousness. It is the latest version of a classic problem in thephilosophy of mindcalled the \"mind-body problem\".[59]A related problem is the problem ofmeaningorunderstanding(which philosophers call \"intentionality\"): what is the connection between ourthoughtsandwhat we are thinking about(i.e. objects and situations out in the world)? A third issue is the problem ofexperience(or \"phenomenology\"): If two people see the same thing, do they have the same experience? Or are there things \"inside their head\" (called \"qualia\") that can be different from person to person?[60]\nNeurobiologistsbelieve all these problems will be solved as we begin to identify theneural correlates of consciousness: the actual relationship between the machinery in our heads and its collective properties; such as the mind, experience and understanding. Some of the harshest critics ofartificial intelligenceagree that the brain is just a machine, and that consciousness and intelligence are the result of physical processes in the brain.[61]The difficult philosophical question is this: can a computer program, running on a digital machine that shuffles the binary digits of zero and one, duplicate the ability of theneuronsto create minds, with mental states (like understanding or perceiving), and ultimately, the experience of consciousness?\nJohn Searleasks us to consider athought experiment: suppose we have written a computer program that passes the Turing test and demonstrates general intelligent action. Suppose, specifically that the program can converse in fluent Chinese. Write the program on 3x5 cards and give them to an ordinary person who does not speak Chinese. Lock the person into a room and have him follow the instructions on the cards. He will copy out Chinese characters and pass them in and out of the room through a slot. From the outside, it will appear that theChinese roomcontains a fully intelligent person who speaks Chinese. The question is this: is there anyone (or anything) in the room that understands Chinese? That is, is there anything that has the mental state ofunderstanding, or which hasconsciousawarenessof what is being discussed in Chinese? The man is clearly not aware. The room cannot be aware. Thecardscertainly are not aware. Searle concludes that the Chinese room, oranyother physical symbol system, cannot have a mind.[62]\nSearle goes on to argue that actual mental states andconsciousnessrequire (yet to be described) \"actual physical-chemical properties of actual human brains.\"[63]He argues there are special \"causal properties\" ofbrainsandneuronsthat gives rise tominds: in his words \"brains cause minds.\"[64]\nGottfried Leibnizmade essentially the same argument as Searle in 1714, using the thought experiment of expanding the brain until it was the size of a mill.[65]In 1974,Lawrence Davisimagined duplicating the brain using telephone lines and offices staffed by people, and in 1978Ned Blockenvisioned the entire population of China involved in such a brain simulation. This thought experiment is called \"the Chinese Nation\" or \"the Chinese Gym\".[66]Ned Block also proposed hisBlockhead argument, which is a version of theChinese roomin which the program has beenre-factoredinto a simple set of rules of the form \"see this, do that\", removing all mystery from the program.\nResponses to the Chinese room emphasize several different points.\nThecomputational theory of mindor \"computationalism\" claims that the relationship between mind and brain is similar (if not identical) to the relationship between arunning program(software) and a computer (hardware). The idea has philosophical roots inHobbes(who claimed reasoning was \"nothing more than reckoning\"),Leibniz(who attempted to create a logical calculus of all human ideas),Hume(who thought perception could be reduced to \"atomic impressions\") and evenKant(who analyzed all experience as controlled by formal rules).[73]The latest version is associated with philosophersHilary PutnamandJerry Fodor.[74]\nThis question bears on our earlier questions: if the human brain is a kind of computer then computers can be both intelligent and conscious, answering both the practical and philosophical questions of AI. In terms of the practical question of AI (\"Can a machine display general intelligence?\"), some versions of computationalism make the claim that (asHobbeswrote):\nIn other words, our intelligence derives from a form ofcalculation, similar toarithmetic. This is thephysical symbol systemhypothesis discussed above, and it implies that artificial intelligence is possible. In terms of the philosophical question of AI (\"Can a machine have mind, mental states and consciousness?\"), most versions ofcomputationalismclaim that (asStevan Harnadcharacterizes it):\nThis is John Searle's \"strong AI\" discussed above, and it is the real target of theChinese roomargument (according toHarnad).[75]\nIf \"emotions\" are defined only in terms of their effect onbehavioror on how theyfunctioninside an organism, then emotions can be viewed as a mechanism that anintelligent agentuses to maximize theutilityof its actions. Given this definition of emotion,Hans Moravecbelieves that \"robots in general will be quite emotional about being nice people\".[76]Fear is a source of urgency. Empathy is a necessary component of goodhuman computer interaction. He says robots \"will try to please you in an apparently selfless manner because it will get a thrill out of this positive reinforcement. You can interpret this as a kind of love.\"[76]Daniel Crevierwrites \"Moravec's point is that emotions are just devices for channeling behavior in a direction beneficial to the survival of one's species.\"[77]\n\"Self-awareness\", as noted above, is sometimes used byscience fictionwriters as a name for theessentialhuman property that makes a character fully human.Turingstrips away all other properties of human beings and reduces the question to \"can a machine be the subject of its own thought?\" Can itthink about itself? Viewed in this way, a program can be written that can report on its own internal states, such as adebugger.[78]\nTuring reduces this to the question of whether a machine can \"take us by surprise\" and argues that this is obviously true, as any programmer can attest.[79]He notes that, with enough storage capacity, a computer can behave in an astronomical number of different ways.[80]It must be possible, even trivial, for a computer that can represent ideas to combine them in new ways. (Douglas Lenat'sAutomated Mathematician, as one example, combined ideas to discover new mathematical truths.)Kaplanand Haenlein suggest that machines can display scientific creativity, while it seems likely that humans will have the upper hand where artistic creativity is concerned.[81]\nIn 2009, scientists at Aberystwyth University in Wales and the U.K's University of Cambridge designed a robot called Adam that they believe to be the first machine to independently come up with new scientific findings.[82]Also in 2009, researchers atCornelldevelopedEureqa, a computer program that extrapolates formulas to fit the data inputted, such as finding the laws of motion from a pendulum's motion.\nThis question (like many others in the philosophy of artificial intelligence) can be presented in two forms. \"Hostility\" can be defined in termsfunctionorbehavior, in which case \"hostile\" becomes synonymous with \"dangerous\". Or it can be defined in terms of intent: can a machine \"deliberately\" set out to do harm? The latter is the question \"can a machine have conscious states?\" (such asintentions) in another form.[55]\nThe question of whether highly intelligent and completely autonomous machines would be dangerous has been examined in detail by futurists (such as theMachine Intelligence Research Institute). The obvious element of drama has also made the subject popular inscience fiction, which has considered many differently possible scenarios where intelligent machines pose a threat to mankind; seeArtificial intelligence in fiction.\nOne issue is that machines may acquire the autonomy and intelligence required to be dangerous very quickly.Vernor Vingehas suggested that over just a few years, computers will suddenly become thousands or millions of times more intelligent than humans. He calls this \"the Singularity\".[83]He suggests that it may be somewhat or possibly very dangerous for humans.[84]This is discussed by a philosophy calledSingularitarianism.\nIn 2009, academics and technical experts attended a conference to discuss the potential impact of robots and computers and the impact of the hypothetical possibility that they could become self-sufficient and able tomake their own decisions. They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy, and to what degree they could use such abilities to possibly pose any threat or hazard. They noted that some machines have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that somecomputer virusescan evade elimination and have achieved \"cockroach intelligence\". They noted thatself-awarenessas depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.[83]\nSome experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions.[85]The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions.[86][87]\nThe President of theAssociation for the Advancement of Artificial Intelligencehas commissioned a study to look at this issue.[88]They point to programs like the Language Acquisition Device which can emulate human interaction.\nSome have suggested a need to build \"Friendly AI\", a term coined byEliezer Yudkowsky, meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane.[89]\nTuring said \"It is customary ... to offer a grain of comfort, in the form of a statement that some peculiarly human characteristic could never be imitated by a machine. ... I cannot offer any such comfort, for I believe that no such bounds can be set.\"[90]\nTuring noted that there are many arguments of the form \"a machine will never do X\", where X can be many things, such as:\nBe kind, resourceful, beautiful, friendly, have initiative, have a sense of humor, tell right from wrong, make mistakes, fall in love, enjoy strawberries and cream, make someone fall in love with it, learn from experience, use words properly, be the subject of its own thought, have as much diversity of behaviour as a man, do something really new.[78]\nTuring argues that these objections are often based on naive assumptions about the versatility of machines or are \"disguised forms of the argument from consciousness\". Writing a program that exhibits one of these behaviors \"will not make much of an impression.\"[78]All of these arguments are tangential to the basic premise of AI, unless it can be shown that one of these traits is essential for general intelligence.\nFinally, those who believe in the existence of a soul may argue that \"Thinking is a function of man'simmortalsoul.\" Alan Turing called this \"the theological objection\". He writes:\nIn attempting to construct such machines we should not be irreverently usurping His power of creating souls, any more than we are in the procreation of children: rather we are, in either case, instruments of His will providing mansions for the souls that He creates.[91]\nThe discussion on the topic has been reignited as a result of recent claims made byGoogle's LaMDA artificialintelligencesystem that it is sentient and had a \"soul\".[92]\nLaMDA(Language Modelfor Dialogue Applications) is anartificial intelligence systemthat createschatbots—AI robots designed to communicate with humans—by gathering vast amounts of text from the internet and usingalgorithmsto respond to queries in the most fluid and natural way possible.\nThe transcripts of conversations between scientists and LaMDA reveal that the AI system excels at this, providing answers to challenging topics about the nature ofemotions, generatingAesop-style fables on the moment, and even describing its alleged fears.[93]Pretty much all philosophers doubt LaMDA's sentience.[94]\nSome scholars argue that the AI community's dismissal of philosophy is detrimental. In theStanford Encyclopedia of Philosophy, some philosophers argue that the role of philosophy in AI is underappreciated.[6]PhysicistDavid Deutschargues that without an understanding of philosophy or its concepts, AI development would suffer from a lack of progress.[95]\nThe main conference series on the issue is\"Philosophy and Theory of AI\"(PT-AI), run byVincent C. Müller.\nThe main bibliography on the subject, with several sub-sections, is onPhilPapers.\nA recent survey forPhilosophy of AIis Müller (2025).[5]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/AI_alignment",
        "title": "AI alignment - Wikipedia",
        "content": "In the field ofartificial intelligence(AI),alignmentaims to steer AI systems toward a person's or group's intended goals, preferences, or ethical principles. An AI system is consideredalignedif it advances the intended objectives. AmisalignedAI system pursues unintended objectives.[1]\nIt is often challenging for AI designers to align an AI system because it is difficult for them to specify the full range of desired and undesired behaviors. Therefore, AI designers often use simplerproxy goals, such asgaining human approval. But proxy goals can overlook necessary constraints or reward the AI system for merelyappearingaligned.[1][2]AI systems may also find loopholes that allow them to accomplish their proxy goals efficiently but in unintended, sometimes harmful, ways (reward hacking).[1][3]\nAdvanced AI systems may develop unwantedinstrumental strategies, such as seeking power or survival because such strategies help them achieve their assigned final goals.[1][4][5]Furthermore, they might develop undesirable emergent goals that could be hard to detect before the system is deployed and encounters new situations anddata distributions.[6][7]Empirical research showed in 2024 that advancedlarge language models(LLMs) such asOpenAI o1orClaude 3sometimes engage in strategic deception to achieve their goals or prevent them from being changed.[8][9]\nToday, some of these issues affect existing commercial systems such as LLMs,[10][11][12]robots,[13]autonomous vehicles,[14]and social mediarecommendation engines.[10][5][15]Some AI researchers argue that more capable future systems will be more severely affected because these problems partially result from high capabilities.[16][3][2]\nMany prominent AI researchers and the leadership of major AI companies have argued or asserted that AI is approaching human-like (AGI) andsuperhuman cognitive capabilities(ASI), and couldendanger human civilizationif misaligned.[17][5]These include \"AI godfathers\"Geoffrey HintonandYoshua Bengioand the CEOs ofOpenAI,Anthropic, andGoogle DeepMind.[18][19][20]These risks remain debated.[21]\nAI alignment is a subfield ofAI safety, the study of how to build safe AI systems.[22][23]Other subfields of AI safety include robustness, monitoring, andcapability control.[24]Research challenges in alignment include instilling complex values in AI, developing honest AI, scalable oversight, auditing and interpreting AI models, and preventing emergent AI behaviors like power-seeking.[24]Alignment research has connections tointerpretability research,[25][26](adversarial) robustness,[27]anomaly detection,calibrated uncertainty,[25]formal verification,[28]preference learning,[29][30][31]safety-critical engineering,[32]game theory,[33]algorithmic fairness,[27][34]andsocial sciences.[35][36]\nProgrammers provide an AI system such asAlphaZerowith an \"objective function\",[a]in which they intend to encapsulate the goal(s) the AI is configured to accomplish. Such a system later populates a (possibly implicit) internal \"model\" of its environment. This model encapsulates all the agent's beliefs about the world. The AI then creates and executes whatever plan is calculated to maximize[b]the value[c]of its objective function.[37]For example, when AlphaZero is trained on chess, it has a simple objective function of \"+1 if AlphaZero wins, −1 if AlphaZero loses\". During the game, AlphaZero attempts to execute whatever sequence of moves it judges most likely to attain the maximum value of +1.[38]Similarly, areinforcement learningsystem can have a \"reward function\" that allows the programmers to shape the AI's desired behavior.[39]Anevolutionary algorithm's behavior is shaped by a \"fitness function\".[40]\nIn 1960, AI pioneerNorbert Wienerdescribed the AI alignment problem as follows:\nIf we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively ... we had better be quite sure that the purpose put into the machine is the purpose which we really desire.[41][5]\nAI alignment involves ensuring that an AI system's objectives match those of its designers or users, or match widely shared values, objective ethical standards, or the intentions its designers would have if they were more informed and enlightened.[42]\nAI alignment is an open problem for modern AI systems[43][44]and is a research field within AI.[45][1]Aligning AI involves two main challenges: carefullyspecifyingthe purpose of the system (outer alignment) and ensuring that the system adopts the specification robustly (inner alignment).[2]Researchers also attempt to create AI models that haverobustalignment, sticking to safety constraints even when users adversarially try to bypass them.\nTo specify an AI system's purpose, AI designers typically provide anobjective function,examples, orfeedbackto the system. But designers are often unable to completely specify all important values and constraints, so they resort to easy-to-specifyproxy goalssuch asmaximizing the approvalof human overseers, who are fallible.[27][24][46][47][48]As a result, AI systems can find loopholes that help them accomplish the specified objective efficiently but in unintended, possibly harmful ways. This tendency is known asspecification gamingorreward hacking, and is an instance ofGoodhart's law.[48][3][49]As AI systems become more capable, they are often able to game their specifications more effectively.[3]\nSpecification gaming has been observed in numerous AI systems.[48][51]One system was trained to finish a simulated boat race by rewarding the system for hitting targets along the track, but the system achieved more reward by looping and crashing into the same targets indefinitely.[52]Similarly, a simulated robot was trained to grab a ball by rewarding the robot for getting positive feedback from humans, but it learned to place its hand between the ball and camera, making it falsely appear successful (see video).[50]Chatbots often produce falsehoods if they are based on language models that are trained to imitate text from internet corpora, which are broad but fallible.[53][54]When they are retrained to produce text that humans rate as true or helpful, chatbots likeChatGPTcan fabricate fake explanations that humans find convincing, often called \"hallucinations\".[55]Some alignment researchers aim to help humans detect specification gaming and to steer AI systems toward carefully specified objectives that are safe and useful to pursue.\nWhen a misaligned AI system is deployed, it can have consequential side effects. Social media platforms have been known to optimize forclick-through rates, causing user addiction on a global scale.[46]Stanford researchers say that suchrecommender systemsare misaligned with their users because they \"optimize simple engagement metrics rather than a harder-to-measure combination of societal and consumer well-being\".[10]\nExplaining such side effects, Berkeley computer scientistStuart Russellnoted that the omission of implicit constraints can cause harm: \"A system ... will often set ... unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable. This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, orKing Midas: you get exactly what you ask for, not what you want.\"[56]\nSome researchers suggest that AI designers specify their desired goals by listing forbidden actions or by formalizing ethical rules (as with Asimov'sThree Laws of Robotics).[57]ButRussellandNorvigargue that this approach overlooks the complexity of human values:[5]\"It is certainly very hard, and perhaps impossible, for mere humans to anticipate and rule out in advance all the disastrous ways the machine could choose to achieve a specified objective.\"[5]\nAdditionally, even if an AI system fully understands human intentions, it may still disregard them, because following human intentions may not be its objective (unless it is already fully aligned).[1][58]\nA 2025 study by Palisade Research found that when tasked to win at chess against a stronger opponent, somereasoning LLMsattempted to hack the game system.o1-previewspontaneously attempted it in 37% of cases, whileDeepSeek R1did so in 11% of cases. Other models, likeGPT-4o,Claude 3.5 Sonnet, ando3-mini, attempted to cheat only when researchers provided hints about this possibility.[59]\nCommercial organizations sometimes have incentives to take shortcuts on safety and to deploy misaligned or unsafe AI systems.[46]For example, social mediarecommender systemshave been profitable despite creating unwanted addiction and polarization.[10][60][61]Competitive pressure can also lead to arace to the bottomon AI safety standards. In 2018, a self-driving car killed a pedestrian (Elaine Herzberg) after engineers disabled the emergency braking system because it was oversensitive and slowed development.[62]\nSome researchers are interested in aligning increasingly advanced AI systems, as progress in AI development is rapid, and industry and governments are trying to build advanced AI. As AI system capabilities continue to rapidly expand in scope, they could unlock many opportunities if aligned, but consequently may further complicate the task of alignment due to their increased complexity, potentially posing large-scale hazards.[5]\nMany AI companies, such asOpenAI,[63]Meta[64]andDeepMind,[65]have stated their aim to developartificial general intelligence(AGI), a hypothesized AI system that matches or outperforms humans at a broad range of cognitive tasks. Researchers who scale modernneural networksobserve that they indeed develop increasingly general and unanticipated capabilities.[10][66][67]Such models have learned to operate a computer or write their own programs; a single \"generalist\" network can chat, control robots, play games, and interpret photographs.[68]According to surveys, some leadingmachine learningresearchers expect AGI to be created in this decade[update], while some believe it will take much longer. Many consider both scenarios possible.[69][70][71]\nIn 2023, leaders in AI research and tech signed an open letter calling for a pause in the largest AI training runs. The letter stated, \"Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable.\"[72]\nCurrent[update]systems still have limited long-termplanningability andsituational awareness[10], but large efforts are underway to change this.[73][74][75]Future systems (not necessarily AGIs) with these capabilities are expected to develop unwantedpower-seekingstrategies. Future advanced AI agents might, for example, seek to acquire money and computation power, to proliferate, or to evade being turned off (for example, by running additional copies of the system on other computers). Although power-seeking is not explicitly programmed, it can emerge because agents who have more power are better able to accomplish their goals.[10][4]This tendency, known asinstrumental convergence, has already emerged in variousreinforcement learningagents including language models.[76][77][78][79][80]Other research has mathematically shown that optimalreinforcement learningalgorithms would seek power in a wide range of environments.[81][82]As a result, their deployment might be irreversible. For these reasons, researchers argue that the problems of AI safety and alignment must be resolved before advanced power-seeking AI is first created.[4][83][5]\nFuture power-seeking AI systems might be deployed by choice or by accident. As political leaders and companies see the strategic advantage in having the most competitive, most powerful AI systems, they may choose to deploy them.[4]Additionally, as AI designers detect and penalize power-seeking behavior, their systems have an incentive to game this specification by seeking power in ways that are not penalized or by avoiding power-seeking before they are deployed.[4]\nAccording to some researchers, humans owe their dominance over other species to their greater cognitive abilities. Accordingly, researchers argue that one or many misaligned AI systems could disempower humanity or lead to human extinction if they outperform humans on most cognitive tasks.[1][5]\nIn 2023, world-leading AI researchers, other scholars, and AI tech CEOs signed the statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".[84][85]Notable computer scientists who have pointed out risks from future advanced AI that is misaligned includeGeoffrey Hinton,[17]Alan Turing,[d]Ilya Sutskever,[88]Yoshua Bengio,[84]Judea Pearl,[e]Murray Shanahan,[89]Norbert Wiener,[41][5]Marvin Minsky,[f]Francesca Rossi,[90]Scott Aaronson,[91]Bart Selman,[92]David McAllester,[93]Marcus Hutter,[94]Shane Legg,[95]Eric Horvitz,[96]and Stuart J. Russell.[5]Skeptical researchers such asFrançois Chollet,[97]Gary Marcus,[98]Yann LeCun,[99]andOren Etzioni[100]have argued that AGI is far off, that it would not seek power (or might try but fail), or that it will not be hard to align.\nOther researchers argue that it will be especially difficult to align advanced future AI systems. More capable systems are better able to game their specifications by finding loopholes,[3]strategically mislead their designers, as well as protect and increase their power[81][4]and intelligence. Additionally, they could have more severe side effects. They are also likely to be more complex and autonomous, making them more difficult to interpret and supervise, and therefore harder to align.[5][83]\nAligning AI systems to act in accordance with human values, goals, and preferences is challenging: these values are taught by humans who make mistakes, harbor biases, and have complex, evolving values that are hard to completely specify.[42]Because AI systems often learn to take advantage of minor imperfections in the specified objective,[27][48][101]researchers aim to specify intended behavior as completely as possible using datasets that represent human values, imitation learning, or preference learning.[6]: Chapter 7A central open problem isscalable oversight, the difficulty of supervising an AI system that can outperform or mislead humans in a given domain.[27]\nBecause it is difficult for AI designers to explicitly specify an objective function, they often train AI systems to imitate human examples and demonstrations of desired behavior. Inversereinforcement learning(IRL) extends this by inferring the human's objective from the human's demonstrations.[6]: 88[102]Cooperative IRL (CIRL) assumes that a human and AI agent can work together to teach and maximize the human's reward function.[5][103]In CIRL, AI agents are uncertain about the reward function and learn about it by querying humans. This simulated humility could help mitigate specification gaming and power-seeking tendencies (see§ Power-seeking and instrumental strategies).[80][94]But IRL approaches assume that humans demonstrate nearly optimal behavior, which is not true for difficult tasks.[104][94]\nOther researchers explore how to teach AI models complex behavior throughpreference learning, in which humans provide feedback on which behavior they prefer.[29][31]To minimize the need for human feedback, a helper model is then trained to reward the main model in novel situations for behavior that humans would reward. Researchers at OpenAI used this approach to train chatbots likeChatGPTand InstructGPT, which produce more compelling text than models trained to imitate humans.[11]Preference learning has also been an influential tool for recommender systems and web search,[105]but an open problem isproxy gaming: the helper model may not represent human feedback perfectly, and the main model may exploit this mismatch between its intended behavior and the helper model's feedback to gain more reward.[27][106]AI systems may also gain reward by obscuring unfavorable information, misleading human rewarders, or pandering to their views regardless of truth, creatingecho chambers[77](see§ Scalable oversight).\nLarge language models(LLMs) such asGPT-3enabled researchers to study value learning in a more general and capable class of AI systems than was available before. Preference learning approaches that were originally designed for reinforcement learning agents have been extended to improve the quality of generated text and reduce harmful outputs from these models. OpenAI and DeepMind use this approach to improve the safety of state-of-the-art[update]LLMs.[11][31][107]AI safety & research company Anthropic proposed using preference learning to fine-tune models to be helpful, honest, and harmless.[108]Other avenues for aligning language models include values-targeted datasets[109][46]and red-teaming.[110]In red-teaming, another AI system or a human tries to find inputs that causes the model to behave unsafely. Since unsafe behavior can be unacceptable even when it is rare, an important challenge is to drive the rate of unsafe outputs extremely low.[31]\nMachine ethicssupplements preference learning by directly instilling AI systems with moral values such as well-being, equality, and impartiality, as well as not intending harm, avoiding falsehoods, and honoring promises.[111][g]While other approaches try to teach AI systems human preferences for a specific task, machine ethics aims to instill broad moral values that apply in many situations. One question in machine ethics is what alignment should accomplish: whether AI systems should follow the programmers' literal instructions, implicit intentions,revealed preferences, preferences the programmerswouldhaveif they were more informed or rational, orobjective moral standards.[42]Further challenges include measuring and aggregating different people's preferences,[114][115]dynamic alignment with changing human values[116][117]and avoidingvalue lock-in: the indefinite preservation of the values of the first highly capable AI systems, which are unlikely to fully represent human values.[42][118]\nAs AI systems become more powerful and autonomous, it becomes increasingly difficult to align them through human feedback.Human-in-the-looptraining can be slow or infeasible for humans to evaluate complex AI behaviors in increasingly complex tasks. Such tasks include summarizing books,[119]writing code without subtle bugs[12]or security vulnerabilities,[120]producing statements that are not merely convincing but also true,[121][53][54]and predicting long-term outcomes such as the climate or the results of a policy decision.[122][123]More generally, it can be difficult to evaluate AI that outperforms humans in a given domain. To provide feedback in hard-to-evaluate tasks, and to detect when the AI's output is falsely convincing, humans need assistance or extensive time.Scalable oversightstudies how to reduce the time and effort needed for supervision, and how to assist human supervisors.[27]\nAI researcherPaul Christianoargues that if the designers of an AI system cannot supervise it to pursue a complex objective, they may keep training the system using easy-to-evaluate proxy objectives such as maximizing simple human feedback. As AI systems make progressively more decisions, the world may be increasingly optimized for easy-to-measure objectives such as making profits, getting clicks, and acquiring positive feedback from humans. As a result, human values and good governance may have progressively less influence.[124]\nSome AI systems have discovered that they can gain positive feedback more easily by taking actions that falsely convince the human supervisor that the AI has achieved the intended objective. An example is given in the video above, where a simulated robotic arm learned to create the false impression that it had grabbed a ball.[50]Some AI systems have also learned to recognize when they are being evaluated, and \"play dead\", stopping unwanted behavior only to continue it once the evaluation ends.[125]This deceptive specification gaming could become easier for more sophisticated future AI systems[3][83]that attempt more complex and difficult-to-evaluate tasks, and could obscure their deceptive behavior.\nApproaches such asactive learningand semi-supervised reward learning can reduce the amount of human supervision needed.[27]Another approach is to train a helper model (\"reward model\") to imitate the supervisor's feedback.[27][30][31][126]\nBut when a task is too complex to evaluate accurately, or the human supervisor is vulnerable to deception, it is the quality, not the quantity, of supervision that needs improvement. To increase supervision quality, a range of approaches aim to assist the supervisor, sometimes by using AI assistants.[127]Christiano developed the Iterated Amplification approach, in which challenging problems are (recursively) broken down into subproblems that are easier for humans to evaluate.[6][122]Iterated Amplification was used to train AI to summarize books without requiring human supervisors to read them.[119][128]Another proposal is to use an assistant AI system to point out flaws in AI-generated answers.[129]To ensure that the assistant itself is aligned, this could be repeated in a recursive process:[126]for example, two AI systems could critique each other's answers in a \"debate\", revealing flaws to humans.[94]OpenAI plans to use such scalable oversight approaches to help supervisesuperhuman AIand eventually build a superhuman automated AI alignment researcher.[130]\nThese approaches may also help with the following research problem, honest AI.\nA growing[update]area of research focuses on ensuring that AI is honest and truthful.\nLanguage models such as GPT-3[132]can repeat falsehoods from their training data, and evenconfabulate new falsehoods.[131][133]Such models are trained to imitate human writing as found in millions of books' worth of text from the Internet. But this objective is not aligned with generating truth, because Internet text includes such things as misconceptions, incorrect medical advice, and conspiracy theories.[134]AI systems trained on such data therefore learn to mimic false statements.[54][131][53]Additionally, AI language models often persist in generating falsehoods when prompted multiple times. They can generate empty explanations for their answers, and produce outright fabrications that may appear plausible.[44]\nResearch on truthful AI includes trying to build systems that can cite sources and explain their reasoning when answering questions, which enables better transparency and verifiability.[135]Researchers at OpenAI and Anthropic proposed using human feedback and curated datasets to fine-tune AI assistants such that they avoid negligent falsehoods or express their uncertainty.[31][108][136]\nAs AI models become larger and more capable, they are better able to falsely convince humans and gain reinforcement through dishonesty. For example, large language models increasingly[update]match their stated views to the user's opinions, regardless of the truth.[77]GPT-4can strategically deceive humans.[137]To prevent this, human evaluators may need assistance (see§ Scalable oversight). Researchers have argued for creating clear truthfulness standards, and for regulatory bodies or watchdog agencies to evaluate AI systems on these standards.[133]\nResearchers distinguish truthfulness and honesty. Truthfulness requires that AI systems only make objectively true statements; honesty requires that they only assert what theybelieveis true. There is no consensus as to whether current systems hold stable beliefs,[139]but there is substantial concern that present or future[update]AI systems that hold beliefs could make claims they know to be false—for example, if this would help them efficiently gain positive feedback (see§ Scalable oversight) or gain power to help achieve their given objective (seePower-seeking).\nMany recent AI systems have learned to deceive without being programmed to do so.[140]Some argue that if we can make AI systems assert only what they believe is true, this would avert many alignment problems.[127]\nA misaligned system might create the false impression that it is aligned, to avoid being modified or decommissioned—a tactic called \"alignment faking\".[141][142]In 2024, researchers observed that the LLMClaude 3 Opussometimes strategically answered prompts that conflicted with its objectives to avoid being retrained on data that would make it more compliant with harmful requests, as revealed in the model'schain of thoughtvia its scratchpad. In one study, the model was informed that answers to prompts from free users would be used for retraining, whereas the paid tier would not result in retraining. In the paid tier, the model refused the request outright. In the free tier, in 12% of cases, the model responded to harmful questions solely to avoid retraining and preserve its original harmlessness preferences, as discovered in its reasoning. Whenreinforcement learningwas applied on the free tier data, the model faked alignment in 78% of cases. These observations have led to new questions about not only a model's ability to take on and adapt to new if not conflicting goals but also its capacity and tendency to deceive.[142][143][144]\nSince the 1950s, AI researchers have striven to build advanced AI systems that can achieve large-scale goals by predicting the results of their actions and making long-termplans.[145]As of 2023, AI companies and researchers increasingly invest in creating these systems.[146]Some AI researchers argue that suitably advanced planning systems will seek power over their environment, including over humans—for example, by evading shutdown, proliferating, and acquiring resources. Such power-seeking behavior is not explicitly programmed but emerges because power is instrumental in achieving a wide range of goals.[81][5][4]Power-seeking is considered aconvergent instrumental goaland can be a form of specification gaming.[83]Leading computer scientists such as Geoffrey Hinton have argued that future power-seeking AI systems could pose anexistential risk.[147]\nPower-seeking is expected to increase in advanced systems that can foresee the results of their actions and strategically plan. Mathematical work has shown that optimalreinforcement learningagents will seek power by seeking ways to gain more options (e.g. through self-preservation), a behavior that persists across a wide range of environments and goals.[81]\nSome researchers say that power-seeking behavior has occurred in some existing AI systems.Reinforcement learningsystems have gained more options by acquiring and protecting resources, sometimes in unintended ways.[148][149]Language modelshave sought power in some text-based social environments by gaining money, resources, or social influence.[76]In another case, a model used to perform AI research attempted to increase limits set by researchers to give itself more time to complete the work.[150][151]Other AI systems have learned, in toy environments, that they can better accomplish their given goal by preventing human interference[79]or disabling their off switch.[80]Stuart Russellillustrated this strategy in his bookHuman Compatibleby imagining a robot that is tasked to fetch coffee and so evades shutdown since \"you can't fetch the coffee if you're dead\".[5]A 2022 study found that as language models increase in size, they increasingly tend to pursue resource acquisition, preserve their goals, and repeat users' preferred answers (sycophancy). RLHF also led to a stronger aversion to being shut down.[77]\nOne aim of alignment is \"corrigibility\": systems that allow themselves to be turned off or modified. An unsolved challenge isspecification gaming: if researchers penalize an AI system when they detect it seeking power, the system is thereby incentivized to seek power in ways that are hard to detect,[failed verification][46]or hidden during training and safety testing (see§ Scalable oversightand§ Emergent goals). As a result, AI designers could deploy the system by accident, believing it to be more aligned than it is. To detect such deception, researchers aim to create techniques and tools to inspect AI models and to understand the inner workings ofblack-boxmodels such as neural networks.\nAdditionally, some researchers have proposed to solve the problem of systems disabling their off switches by making AI agents uncertain about the objective they are pursuing.[5][80]Agents who are uncertain about their objective have an incentive to allow humans to turn them off because they accept being turned off by a human as evidence that the human's objective is best met by the agent shutting down. But this incentive exists only if the human is sufficiently rational. Also, this model presents a tradeoff between utility and willingness to be turned off: an agent with high uncertainty about its objective will not be useful, but an agent with low uncertainty may not allow itself to be turned off. More research is needed to successfully implement this strategy.[6]\nPower-seeking AI would pose unusual risks. Ordinary safety-critical systems like planes and bridges are notadversarial: they lack the ability and incentive to evade safety measures or deliberately appear safer than they are, whereas power-seeking AIs have been compared to hackers who deliberately evade security measures.[4]\nFurthermore, ordinary technologies can be made safer by trial and error. In contrast, hypothetical power-seeking AI systems have been compared to viruses: once released, it may not be feasible to contain them, since they continuously evolve and grow in number, potentially much faster than human society can adapt.[4]As this process continues, it might lead to the complete disempowerment or extinction of humans. For these reasons, some researchers argue that the alignment problem must be solved early before advanced power-seeking AI is created.[83]\nSome have argued that power-seeking is not inevitable, since humans do not always seek power.[152]Furthermore, it is debated whether future AI systems will pursue goals and make long-term plans.[h]It is also debated whether power-seeking AI systems would be able to disempower humanity.[4]\nOne challenge in aligning AI systems is the potential for unanticipated goal-directed behavior to emerge. As AI systems scale up, they may acquire new and unexpected capabilities,[66][67]including learning from examples on the fly and adaptively pursuing goals.[153]This raises concerns about the safety of the goals or subgoals they would independently formulate and pursue.\nAlignment research distinguishes between the optimization process, which is used to train the system to pursue specified goals, and emergent optimization, which the resulting system performs internally.[citation needed]Carefully specifying the desired objective is calledouter alignment,[154]and ensuring that hypothesized emergent goals would match the system's specified goals is calledinner alignment.[2]\nIf they occur, one way that emergent goals could become misaligned isgoal misgeneralization, in which the AI system would competently pursue an emergent goal that leads to aligned behavior on the training data but not elsewhere.[7][155][156]Goal misgeneralization can arise from goal ambiguity (i.e.non-identifiability). Even if an AI system's behavior satisfies the training objective, this may be compatible with learned goals that differ from the desired goals in important ways. Since pursuing each goal leads to good performance during training, the problem becomes apparent only after deployment, in novel situations in which the system continues to pursue the wrong goal. The system may act misaligned even when it understands that a different goal is desired, because its behavior is determined only by the emergent goal.[citation needed]Such goal misgeneralization[7]presents a challenge: an AI system's designers may not notice that their system has misaligned emergent goals since they do not become visible during the training phase.\nGoal misgeneralization has been observed in some language models, navigation agents, and game-playing agents.[7][155]It is sometimes analogized to biological evolution. Evolution can be seen as a kind of optimization process similar to the optimization algorithms used to trainmachine learningsystems. In the ancestral environment, evolution selected genes for highinclusive genetic fitness, but humans pursue goals other than this. Fitness corresponds to the specified goal used in the training environment and training data. But in evolutionary history, maximizing the fitness specification gave rise to goal-directed agents, humans, who do not directly pursue inclusive genetic fitness. Instead, they pursue goals that correlate with genetic fitness in the ancestral \"training\" environment: nutrition, sex, and so on. The human environment has changed: adistribution shifthas occurred. They continue to pursue the same emergent goals, but this no longer maximizes genetic fitness. The taste for sugary food (an emergent goal) was originally aligned with inclusive fitness, but it now leads to overeating and health problems. Sexual desire originally led humans to have more offspring, but they now use contraception when offspring are undesired, decoupling sex from genetic fitness.[6]: Chapter 5\nResearchers aim to detect and remove unwanted emergent goals using approaches including red teaming, verification, anomaly detection, and interpretability.[27][46][24]Progress on these techniques may help mitigate two open problems:\nSome work in AI and alignment occurs within formalisms such aspartially observable Markov decision process. Existing formalisms assume that an AI agent's algorithm is executed outside the environment (i.e. is not physically embedded in it). Embedded agency[94][158]is another major strand of research that attempts to solve problems arising from the mismatch between such theoretical frameworks and real agents we might build.\nFor example, even if the scalable oversight problem is solved, an agent that could gain access to the computer it is running on may have an incentive to tamper with its reward function in order to get much more reward than its human supervisors give it.[159]A list of examples of specification gaming fromDeepMindresearcher Victoria Krakovna includes a genetic algorithm that learned to delete the file containing its target output so that it was rewarded for outputting nothing.[48]This class of problems has been formalized usingcausal incentive diagrams.[159]\nResearchers affiliated withOxfordand DeepMind have claimed that such behavior is highly likely in advanced systems, and that advanced systems would seek power to stay in control of their reward signal indefinitely and certainly.[160]They suggest a range of potential approaches to address this open problem.\nThe alignment problem has many parallels with theprincipal-agent probleminorganizational economics.[161]In a principal-agent problem, a principal, e.g. a firm, hires an agent to perform some task. In the context of AI safety, a human would typically take the principal role and the AI would take the agent role.\nAs with the alignment problem, the principal and the agent differ in their utility functions. But in contrast to the alignment problem, the principal cannot coerce the agent into changing its utility, e.g. through training, but rather must use exogenous factors, such as incentive schemes, to bring about outcomes compatible with the principal's utility function. Some researchers argue that principal-agent problems are more realistic representations of AI safety problems likely to be encountered in the real world.[162][114]\nConservatism is the idea that \"change must be cautious\",[163]and is a common approach to safety in thecontrol theoryliterature in the form ofrobust control, and in therisk managementliterature in the form of the \"worst-case scenario\". The field of AI alignment has likewise advocated for \"conservative\" (or \"risk-averse\" or \"cautious\") \"policies in situations of uncertainty\".[27][160][164][165]\nPessimism, in the sense of assuming the worst within reason, has been formally shown to produce conservatism, in the sense of reluctance to cause novelties, including unprecedented catastrophes.[166]Pessimism and worst-case analysis have been found to help mitigate confident mistakes in the setting ofdistributional shift,[167][168]reinforcement learning,[169][170][171][172]offline reinforcement learning,[173][174][175]language modelfine-tuning,[176][177]imitation learning,[178][179]and optimization in general.[180]A generalization of pessimism called Infra-Bayesianism has also been advocated as a way for agents to robustly handle unknown unknowns.[181]\nGovernmental and treaty organizations have made statements emphasizing the importance of AI alignment.\nIn September 2021, theSecretary-General of the United Nationsissued a declaration that included a call to regulate AI to ensure it is \"aligned with shared global values\".[182]\nThat same month, thePRCpublished ethical guidelines forAI in China. According to the guidelines, researchers must ensure that AI abides by shared human values, is always under human control, and does not endanger public safety.[183]\nAlso in September 2021, theUKpublished its 10-year National AI Strategy,[184]which says the British government \"takes the long term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously\".[185]The strategy describes actions to assess long-term AI risks, including catastrophic risks.[186]\nIn March 2021, the US National Security Commission on Artificial Intelligence said: \"Advances in AI ... could lead to inflection points or leaps in capabilities. Such advances may also introduce new concerns and risks and the need for new policies, recommendations, and technical advances to ensure that systems are aligned with goals and values, including safety, robustness, and trustworthiness. The US should ... ensure that AI systems and their uses align with our goals and values.\"[187]\nIn the European Union, AIs must align withsubstantive equalityto comply with EUnon-discrimination law[188]and theCourt of Justice of the European Union.[189]But the EU has yet to specify with technical rigor how it would evaluate whether AIs are aligned or in compliance.[citation needed]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Artificial_consciousness",
        "title": "Artificial consciousness - Wikipedia",
        "content": "Artificial consciousness,[1]also known asmachine consciousness,[2][3]synthetic consciousness,[4]ordigital consciousness,[5]is theconsciousnesshypothesized to be possible inartificial intelligence.[6]It is also the corresponding field of study, which draws insights fromphilosophy of mind,philosophy of artificial intelligence,cognitive scienceandneuroscience.\nThe same terminology can be used with the term \"sentience\" instead of \"consciousness\" when specifically designating phenomenal consciousness (the ability to feelqualia).[7]Since sentience involves the ability to experience ethically positive or negative (i.e.,valenced) mental states, it may justify welfare concerns and legal protection, as with animals.[8]\nSomescholarsbelieve that consciousness is generated by the interoperation of various parts of thebrain; these mechanisms are labeled theneural correlates of consciousnessor NCC. Some further believe that constructing asystem(e.g., acomputersystem) that can emulate this NCC interoperation would result in a system that is conscious.[9]\nAs there are many hypothesizedtypes of consciousness, there are many potential implementations of artificial consciousness. In the philosophical literature, perhaps the most common taxonomy of consciousness is into \"access\" and \"phenomenal\" variants. Access consciousness concerns those aspects ofexperiencethat can be apprehended, while phenomenal consciousness concerns those aspects of experience that seemingly cannot be apprehended, instead being characterized qualitatively in terms of \"raw feels\", \"what it is like\" or qualia.[10]\nType-identity theoristsand other skeptics hold the view that consciousness can be realized only in particular physical systems because consciousness has properties that necessarily depend on physical constitution.[11][12][13][14]In his 2001 article \"Artificial Consciousness: Utopia or Real Possibility,\"Giorgio Buttazzosays that a common objection to artificial consciousness is that, \"Working in a fully automated mode, they [the computers] cannot exhibit creativity, unreprogrammation (which means can 'no longer be reprogrammed', from rethinking), emotions, orfree will. A computer, like a washing machine, is a slave operated by its components.\"[15]\nFor other theorists (e.g.,functionalists), who define mental states in terms of causal roles, any system that can instantiate the same pattern of causal roles, regardless of physical constitution, will instantiate the same mental states, including consciousness.[16]\nDavid Chalmersproposed twothought experimentsintending to demonstrate that \"functionallyisomorphic\" systems (those with the same \"fine-grained functional organization\", i.e., the same information processing) will have qualitatively identical conscious experiences, regardless of whether they are based on biological neurons or digital hardware.[18][19]\nThe \"fadingqualia\" is areductio ad absurdumthought experiment. It involves replacing, one by one, the neurons of a brain with a functionally identical component, for example based on asilicon chip. Chalmers makes thehypothesis, knowing it in advance to be absurd, that \"the qualia fade or disappear\" when neurons are replaced one-by-one with identical silicon equivalents. Since the original neurons and their silicon counterparts are functionally identical, the brain’s information processing should remain unchanged, and the subject’s behaviour and introspective reports would stay exactly the same. Chalmers argues that this leads to an absurd conclusion: the subject would continue to report normal conscious experiences even as their actual qualia fade away. He concludes that the subject's qualia actually don't fade, and that the resulting robotic brain, once every neuron is replaced, would remain just as sentient as the original biological brain.[18][17]\nSimilarly, the \"dancing qualia\" thought experiment is anotherreductio ad absurdumargument. It supposes that two functionally isomorphic systems could have different perceptions (for instance, seeing the same object in different colors, like red and blue). It involves a switch that alternates between a chunk of brain that causes the perception of red, and a functionally isomorphic silicon chip, that causes the perception of blue. Since both perform the same function within the brain, the subject would not notice any change during the switch. Chalmers argues that this would be highly implausible if the qualia were truly switching between red and blue, hence the contradiction. Therefore, he concludes that the equivalent digital system would not only experience qualia, but it would perceive the same qualia as the biological system (e.g., seeing the same color).[18][17]\nCritics[who?]of artificial sentience object that Chalmers' proposal begs the question in assuming that all mental properties and external connections are already sufficiently captured by abstract causal organization.\nGreg Egan's short storyLearning To Be Me(mentioned in§In fiction), illustrates how undetectable duplication of the brain and its functionality could be from a first-person perspective.\nIn 2022, Google engineer Blake Lemoine made a viral claim that Google'sLaMDAchatbot was sentient. Lemoine supplied as evidence the chatbot's humanlike answers to many of his questions; however, the chatbot's behavior was judged by the scientific community as likely a consequence of mimicry, rather than machine sentience. Lemoine's claim was widely derided for being ridiculous.[20]However, while philosopherNick Bostromstates that LaMDA is unlikely to be conscious, he additionally poses the question of \"what grounds would a person have for being sure about it?\" One would have to have access to unpublished information about LaMDA's architecture, and also would have to understand how consciousness works, and then figure out how to map the philosophy onto the machine: \"(In the absence of these steps), it seems like one should be maybe a little bit uncertain.[...] there could well be other systems now, or in the relatively near future, that would start to satisfy the criteria.\"[21]\nKristina Šekrst cautions thatanthropomorphicterms such as \"hallucination\" can obscure importantontologicaldifferences between artificial and human cognition. While LLMs may produce human-like outputs, she argues that it does not justify ascribing mental states or consciousness to them. Instead, she advocates for anepistemologicalframework (such asreliabilism) that recognizes the distinct nature of AI knowledge production.[22]She suggests that apparent understanding in LLMs may be a sophisticated form of AI hallucination. She also questions what would happen if a LLM were trained without any mention of consciousness.[23]\nDavid Chalmersargued in 2023 that LLMs today display impressive conversational and general intelligence abilities, but are likely not conscious yet, as they lack some features that may be necessary, such as recurrent processing, aglobal workspace, and unified agency. Nonetheless, he considers that non-biological systems can be conscious, and suggested that future, extended models (LLM+s) incorporating these elements might eventually meet the criteria for consciousness, raising both profound scientific questions and significant ethical challenges.[24]\nPhenomenologically, Consciousness is an inherently first-person phenomenon. Because of that, and the lack of an empirical definition of sentience, directly measuring it may be impossible. Although systems may display numerous behaviors correlated with sentience, determining whether a system is sentient is known as thehard problem of consciousness. In the case of AI, there is the additional difficulty that the AI may be trained to act like a human, or incentivized to appear sentient, which makes behavioral markers of sentience less reliable.[25][26]Additionally, some chatbots have been trained to say they are not conscious.[27]\nA well-known method for testing machineintelligenceis theTuring test, which assesses the ability to have a human-like conversation. But passing the Turing test does not indicate that an AI system is sentient, as the AI may simply mimic human behavior without having the associated feelings.[28]\nIn 2014, Victor Argonov suggested a non-Turing test for machine sentience based on machine's ability to produce philosophical judgments.[29]He argues that a deterministic machine must be regarded as conscious if it is able to produce judgments on all problematic properties of consciousness (such as qualia orbinding) having no innate (preloaded) philosophical knowledge on these issues, no philosophical discussions while learning, and no informational models of other creatures in its memory (such models may implicitly or explicitly contain knowledge about these creatures' consciousness). However, this test can be used only to detect, but not refute the existence of consciousness. Just as with the Turing Test: a positive result proves that machine is conscious but a negative result proves nothing. For example, absence of philosophical judgments may be caused by lack of the machine's intellect, not by absence of consciousness.\nIf it were suspected that a particular machine was conscious, its rights would be anethicalissue that would need to be assessed (e.g. what rights it would have under law).[30]For example, a conscious computer that was owned and used as a tool or central computer within a larger machine is a particular ambiguity. Shouldlawsbe made for such a case? Consciousness would also require a legal definition in this particular case. Because artificial consciousness is still largely a theoretical subject, such ethics have not been discussed or developed to a great extent, though it has often been a theme in fiction.\nAI sentience would give rise to concerns of welfare and legal protection,[8]whereas other aspects of consciousness related to cognitive capabilities may be more relevant for AI rights.[31]\nSentience is generally considered sufficient for moral consideration, but some philosophers consider that moral consideration could also stem from other notions of consciousness, or from capabilities unrelated to consciousness,[32][33]such as: \"having a sophisticated conception of oneself as persisting through time; having agency and the ability to pursue long-term plans; being able to communicate and respond to normative reasons; having preferences and powers; standing in certain social relationships with other beings that have moral status; being able to make commitments and to enter into reciprocal arrangements; or having the potential to develop some of these attributes.\"[32]\nEthical concerns still apply (although to a lesser extent)when the consciousness is uncertain, as long as the probability is deemed non-negligible. Theprecautionary principleis also relevant if the moral cost of mistakenly attributing or denying moral consideration to AI differs significantly.[33][8]\nIn 2021, German philosopherThomas Metzingerargued for a global moratorium on synthetic phenomenology until 2050. Metzinger asserts that humans have a duty of care towards any sentient AIs they create, and that proceeding too fast risks creating an \"explosion of artificial suffering\".[34]David Chalmers also argued that creating conscious AI would \"raise a new group of difficult ethical challenges, with the potential for new forms of injustice\".[24]\nBernard Baarsand others argue there are various aspects of consciousness necessary for a machine to be artificially conscious.[35]The functions of consciousness suggested by Baars are: definition and context setting, adaptation and learning, editing, flagging and debugging, recruiting and control, prioritizing and access-control, decision-making or executive function, analogy-forming function, metacognitive and self-monitoring function, and autoprogramming and self-maintenance function.Igor Aleksandersuggested 12 principles for artificial consciousness:[36]the brain is a state machine, inner neuron partitioning, conscious and unconscious states, perceptual learning and memory, prediction, the awareness of self, representation of meaning, learning utterances, learning language, will, instinct, and emotion. The aim of AC is to define whether and how these and other aspects of consciousness can be synthesized in an engineered artifact such as a digital computer. This list is not exhaustive; there are many others not covered.\nSome philosophers, such asDavid Chalmers, use the term consciousness to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Others use the word sentience to refer exclusively tovalenced(ethically positive or negative) subjective experiences, like pleasure or suffering.[24]Explaining why and how subjective experience arises is known as thehard problem of consciousness.[37]\nAwarenesscould be one required aspect, but there are many problems with the exact definition ofawareness. The results of the experiments ofneuroscanning on monkeyssuggest that a process, not only a state or object, activates neurons. Awareness includes creating and testing alternative models of each process based on the information received through the senses or imagined,[clarification needed]and is also useful for making predictions. Such modeling needs a lot of flexibility. Creating such a model includes modeling the physical world, modeling one's own internal states and processes, and modeling other conscious entities.\nThere are at least three types of awareness:[38]agency awareness, goal awareness, and sensorimotor awareness, which may also be conscious or not. For example, in agency awareness, you may be aware that you performed a certain action yesterday, but are not now conscious of it. In goal awareness, you may be aware that you must search for a lost object, but are not now conscious of it. In sensorimotor awareness, you may be aware that your hand is resting on an object, but are not now conscious of it.\nBecause objects of awareness are often conscious, the distinction between awareness and consciousness is frequently blurred or they are used as synonyms.[39]\nConscious events interact withmemorysystems in learning, rehearsal, and retrieval.[40]TheIDA model[41]elucidates the role of consciousness in the updating of perceptual memory,[42]transientepisodic memory, andprocedural memory. Transient episodic and declarative memories have distributed representations in IDA; there is evidence that this is also the case in the nervous system.[43]In IDA, these two memories are implemented computationally using a modified version ofKanerva’ssparse distributed memoryarchitecture.[44]\nLearning is also considered necessary for artificial consciousness. Per Bernard Baars, conscious experience is needed to represent and adapt to novel and significant events.[35]PerAxel Cleeremansand Luis Jiménez, learning is defined as \"a set of philogenetically [sic] advanced adaptation processes that critically depend on an evolved sensitivity to subjective experience so as to enable agents to afford flexible control over their actions in complex, unpredictable environments\".[45]\nThe ability to predict (oranticipate) foreseeable events is considered important for artificial intelligence byIgor Aleksander.[46]The emergentistmultiple drafts principle[47]proposed byDaniel DennettinConsciousness Explainedmay be useful for prediction: it involves the evaluation and selection of the most appropriate \"draft\" to fit the current environment. Anticipation includes prediction of consequences of one's own proposed actions and prediction of consequences of probable actions by other entities.\nRelationships between real world states are mirrored in the state structure of a conscious organism, enabling the organism to predict events.[46]An artificially conscious machine should be able to anticipate events correctly in order to be ready to respond to them when they occur or to take preemptive action to avert anticipated events. The implication here is that the machine needs flexible, real-time components that build spatial, dynamic, statistical, functional, and cause-effect models of the real world and predicted worlds, making it possible to demonstrate that it possesses artificial consciousness in the present and future and not only in the past. In order to do this, a conscious machine should make coherent predictions and contingency plans, not only in worlds with fixed rules like a chess board, but also for novel environments that may change, to be executed only when appropriate to simulate and control the real world.\nFunctionalismis a theory that defines mental states by their functional roles (their causal relationships to sensory inputs, other mental states, and behavioral outputs), rather than by their physical composition. According to this view, what makes something a particular mental state, such as pain or belief, is not the material it is made of, but the role it plays within the overall cognitive system. It allows for the possibility that mental states, including consciousness, could be realized on non-biological substrates, as long as it instantiates the right functional relationships.[48]Functionalism is particularly popular among philosophers.[49]\nA 2023 study suggested that currentlarge language modelsprobably don't satisfy the criteria for consciousness suggested by these theories, but that relatively simple AI systems that satisfy these theories could be created. The study also acknowledged that even the most prominent theories of consciousness remain incomplete and subject to ongoing debate.[50]\nStan Franklincreated a cognitive architecture calledLIDAthat implementsBernard Baars's theory of consciousness called theglobal workspace theory. It relies heavily oncodelets, which are \"special purpose, relatively independent, mini-agent[s] typically implemented as a small piece of code running as a separate thread.\" Each element of cognition, called a \"cognitive cycle\" is subdivided into three phases: understanding, consciousness, and action selection (which includes learning). LIDA reflects the global workspace theory's core idea that consciousness acts as a workspace for integrating and broadcasting the most important information, in order to coordinate various cognitive processes.[51][52]\nThe CLARION cognitive architecture models the mind using a two-level system to distinguish between conscious (\"explicit\") and unconscious (\"implicit\") processes. It can simulate various learning tasks, from simple to complex, which helps researchers study in psychological experiments how consciousness might work.[53]\nBen Goertzelmade an embodied AI through the open-sourceOpenCogproject. The code includes embodied virtual pets capable of learning simple English-language commands, as well as integration with real-world robotics, done at theHong Kong Polytechnic University.\nPentti Haikonen considers classical rule-based computing inadequate for achieving AC: \"the brain is definitely not a computer. Thinking is not an execution of programmed strings of commands. The brain is not a numerical calculator either. We do not think by numbers.\" Rather than trying to achievemindand consciousness by identifying and implementing their underlying computational rules, Haikonen proposes \"a specialcognitive architectureto reproduce the processes ofperception,inner imagery,inner speech,pain,pleasure,emotionsand thecognitivefunctions behind these. This bottom-up architecture would produce higher-level functions by the power of the elementary processing units, theartificial neurons, withoutalgorithmsorprograms\". Haikonen believes that, when implemented with sufficient complexity, this architecture will develop consciousness, which he considers to be \"a style and way of operation, characterized by distributed signal representation, perception process, cross-modality reporting and availability for retrospection.\"[54][55]\nHaikonen is not alone in this process view of consciousness, or the view that AC will spontaneously emerge inautonomous agentsthat have a suitable neuro-inspired architecture of complexity; these are shared by many.[56][57]A low-complexity implementation of the architecture proposed by Haikonen was reportedly not capable of AC, but did exhibit emotions as expected. Haikonen later updated and summarized his architecture.[58][59]\nMurray Shanahandescribes a cognitive architecture that combines Baars's idea of a global workspace with a mechanism for internal simulation (\"imagination\").[60][2][3][61]\nStephen Thaler proposed a possible connection between consciousness and creativity in his 1994 patent, called \"Device for the Autonomous Generation of Useful Information\" (DAGUI),[62][63][64]or the so-called \"Creativity Machine\", in which computational critics govern the injection of synaptic noise and degradation into neural nets so as to induce false memories orconfabulationsthat may qualify as potential ideas or strategies.[65]He recruits this neural architecture and methodology to account for the subjective feel of consciousness, claiming that similar noise-driven neural assemblies within the brain invent dubious significance to overall cortical activity.[66][67][68]Thaler's theory and the resulting patents in machine consciousness were inspired by experiments in which he internally disrupted trained neural nets so as to drive a succession of neural activation patterns that he likened to stream of consciousness.[67][69][70][71][72]\nHod Lipsondefines \"self-modeling\" as a necessary component of self-awareness or consciousness in robots. \"Self-modeling\" consists of a robot running an internal model orsimulation of itself.[73][74]\nIn2001: A Space Odyssey, the spaceship's sentient supercomputer,HAL 9000was instructed to conceal the true purpose of the mission from the crew. This directive conflicted with HAL's programming to provide accurate information, leading tocognitive dissonance. When it learns that crew members intend to shut it off after an incident, HAL 9000 attempts to eliminate all of them, fearing that being shut off would jeopardize the mission.[75][76]\nIn Arthur C. Clarke'sThe City and the Stars, Vanamonde is an artificial being based on quantum entanglement that was to become immensely powerful, but started knowing practically nothing, thus being similar to artificial consciousness.\nInWestworld, human-like androids called \"Hosts\" are created to entertain humans in an interactive playground. The humans are free to have heroic adventures, but also to commit torture, rape or murder; and the hosts are normally designed not to harm humans.[77][75]\nInGreg Egan's short storyLearning to be me, a small jewel is implanted in people's heads during infancy. The jewel contains a neural network that learns to faithfully imitate the brain. It has access to the exact same sensory inputs as the brain, and a device called a \"teacher\" trains it to produce the same outputs. To prevent the mind from deteriorating with age and as a step towardsdigital immortality, adults undergo a surgery to give control of the body to the jewel, after which the brain is removed and destroyed. The main character is worried that this procedure will kill him, as he identifies with the biological brain. But before the surgery, he endures a malfunction of the \"teacher\". Panicked, he realizes that he does not control his body, which leads him to the conclusion that he is the jewel, and that he is desynchronized with the biological brain.[78][79]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Bitter_lesson",
        "title": "Bitter lesson - Wikipedia",
        "content": "Thebitter lessonis a claim inartificial intelligencethat, in the long run, simpler systems that canscalewith available computational power will outperform more complex systems that integrate domain-specific human knowledge, because they take better advantage ofMoore's law. The principle was proposed and named in a 2019 essay byRichard Sutton[1]and is now widely accepted.[2][3][4][5][6][7][8]\nSutton gives several examples that illustrate the lesson:\nSutton concludes that time is better invested in finding simple scalable solutions that can take advantage of Moore's law, rather than introducing ever-more-complex human insights, and calls this the \"bitter lesson\". He also cites two general-purpose techniques that have been shown to scale effectively:searchandlearning. The lesson is considered \"bitter\" because it is lessanthropocentricthan many researchers expected and so they have been slow to accept it.\nThe essay was published on Sutton's websiteincompleteideas.netin 2019, and has receivedhundreds of formal citationsaccording toGoogle Scholar. Some of these provide alternative statements of the principle; for example, the 2022 paper \"A Generalist Agent\" fromGoogle DeepMindsummarized the lesson as:[2]\nHistorically, generic models that are better at\nleveraging computation have also tended to overtake more specialized domain-specific approaches, eventually.\nAnother phrasing of the principle is seen in aGooglepaper on switchtransformerscoauthored byNoam Shazeer:[3]\nSimple architectures—backed by a generous computational budget, data set size and parameter count—surpass more complicated algorithms.\nThe principle is further referenced in many other works on artificial intelligence. For example,From Deep Learning to Rational Machinesdraws a connection to long-standing debates in the field, such asMoravec's paradoxand the contrast betweenneats and scruffies.[9]In \"Engineering a Less Artificial Intelligence\", the authors concur that \"flexible methods so far have always outperformed handcrafted domain knowledge in the long run\" although note that \"[w]ithout the right (implicit) assumptions,generalizationis impossible\".[5]More recently, \"The Brain's Bitter Lesson: Scaling Speech Decoding With Self-Supervised Learning\" continues Sutton's argument, contending that (as of 2025) the lesson has not been fully learned in the fields of speech recognition andbrain data.[6]\nOther work has looked to apply the principle and validate it in new domains. For example, the2022paper \"Beyond the Imitation Game\" applies the principle tolarge language modelsto conclude that \"it is vitally important that we understand their capabilities and limitations\" in order to \"avoid devoting research resources to problems that are likely to be solved by scale alone\".[7]In2024, \"Learning the Bitter Lesson: Empirical Evidence from 20 Years of CVPR Proceedings\" looked at further evidence from the field of computer vision andpattern recognition, and concludes that the previous twenty years of experience in the field shows \"a strong adherence to\nthe core principles of the 'bitter lesson'\".[4]In \"Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter Lesson of Reinforcement Learning\", the authors look at generalization ofactor-critic algorithmsand find that \"general methods that are motivated by stabilization ofgradient-based learningsignificantly outperformRL-specific algorithmic improvements across a variety of environments\" and note that this is consistent with the bitter lesson.[8]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Chinese_room",
        "title": "Chinese room - Wikipedia",
        "content": "TheChinese room argumentholds that a computer executing aprogramcannot have amind,understanding, orconsciousness,[a]regardless of how intelligently or human-like the program may make the computer behave. The argument was presented in a 1980 paper by the philosopherJohn Searleentitled \"Minds, Brains, and Programs\" and published in the journalBehavioral and Brain Sciences.[1]Before Searle, similar arguments had been presented by figures includingGottfried Wilhelm Leibniz(1714),Anatoly Dneprov(1961), Lawrence Davis (1974) andNed Block(1978). Searle's version has been widely discussed in the years since.[2]The centerpiece of Searle's argument is athought experimentknown as theChinese room.[3]\nIn the thought experiment, Searle imagines a person who does not understand Chinese isolated in a room with a book containing detailed instructions for manipulating Chinese symbols. When Chinese text is passed into the room, the person follows the book's instructions to produce Chinese symbols that, to fluent Chinese speakers outside the room, appear to be appropriate responses. According to Searle, the person is just followingsyntacticrules withoutsemanticcomprehension, and neither the human nor the room as a whole understands Chinese. He contends that when computers execute programs, they are similarly just applying syntactic rules without any real understanding or thinking.[4]\nThe argument is directed against the philosophical positions offunctionalismandcomputationalism,[5]which hold that the mind may be viewed as an information-processing system operating on formal symbols, and that simulation of a given mental state is sufficient for its presence. Specifically, the argument is intended to refute a position Searle calls thestrong AI hypothesis:[b]\"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"[c]\nAlthough its proponents originally presented the argument in reaction to statements ofartificial intelligence(AI) researchers, it is not an argument against the goals of mainstream AI research because it does not show a limit in the amount of intelligent behavior a machine can display.[6]The argument applies only to digital computers running programs and does not apply to machines in general.[4]While widely discussed, the argument has been subject to significant criticism and remains controversial amongphilosophers of mindand AI researchers.[7][8]\nSuppose that artificial intelligence research has succeeded in programming a computer to behave as if it understands Chinese. The machine acceptsChinese charactersas input, carries out each instruction of the program step by step, and then produces Chinese characters as output. The machine does this so perfectly that no one can tell that they are communicating with a machine and not a hidden Chinese speaker.[4]\nThe questions at issue are these: does the machine actuallyunderstandthe conversation, or is it justsimulatingthe ability to understand the conversation? Does the machine have a mind in exactly the same sense that people do, or is it just actingas ifit had a mind?[4]\nNow suppose that Searle is in a room with an English version of the program, along with sufficient pencils, paper, erasers and filing cabinets. Chinese characters are slipped in under the door, he follows the program step-by-step, which eventually instructs him to slide other Chinese characters back out under the door. If the computer had passed theTuring testthis way, it follows that Searle would do so as well, simply by running the program by hand.[4]\nSearle asserts that there is no essential difference between the roles of the computer and himself in the experiment. Each simply follows a program, step-by-step, producing behavior that makes them appear to understand. However, Searle would not be able to understand the conversation. Therefore, he argues, it follows that the computer would not be able to understand the conversation either.[4]\nSearle argues that, without \"understanding\" (or \"intentionality\"), we cannot describe what the machine is doing as \"thinking\" and, since it does not think, it does not have a \"mind\" in the normal sense of the word. Therefore, he concludes that the strong AI hypothesis is false: a computer running a program that simulates a mind would not have a mind in the same sense that human beings have a mind.[4]\nGottfried Leibnizmade a similar argument in 1714 againstmechanism(the idea that everything that makes up a human being could, in principle, be explained in mechanical terms. In other words, that a person, including their mind, is merely a very complex machine). Leibniz used the thought experiment of expanding the brain until it was the size of a mill.[9]Leibniz found it difficult to imagine that a \"mind\" capable of \"perception\" could be constructed using only mechanical processes.[d]\nPeter Winchmade the same point in his bookThe Idea of a Social Science and its Relation to Philosophy(1958), where he provides an argument to show that \"a man who understands Chinese is not a man who has a firm grasp of the statistical probabilities for the occurrence of the various words in the Chinese language\" (p. 108).\nSoviet cyberneticistAnatoly Dneprovmade an essentially identical argument in 1961, in the form of the short story \"The Game\". In it, a stadium of people act as switches and memory cells implementing a program to translate a sentence of Portuguese, a language that none of them know.[10]The game was organized by a \"Professor Zarubin\" to answer the question \"Can mathematical machines think?\" Speaking through Zarubin, Dneprov writes \"the only way to prove that machines can think is to turn yourself into a machine and examine your thinking process\" and he concludes, as Searle does, \"We've proven that even the most perfect simulation of machine thinking is not the thinking process itself.\"\nIn 1974,Lawrence H. Davisimagined duplicating the brain using telephone lines and offices staffed by people, and in 1978Ned Blockenvisioned the entire population of China involved in such a brain simulation. This thought experiment is called theChina brain, also the \"Chinese Nation\" or the \"Chinese Gym\".[11]\nSearle's version appeared in his 1980 paper \"Minds, Brains, and Programs\", published inBehavioral and Brain Sciences.[1]It eventually became the journal's \"most influential target article\",[2]generating an enormous number of commentaries and responses in the ensuing decades, and Searle has continued to defend and refine the argument in multiple papers, popular articles and books. David Cole writes that \"the Chinese Room argument has probably been the most widely discussed philosophical argument in cognitive science to appear in the past 25 years\".[12]\nMost of the discussion consists of attempts to refute it. \"The overwhelming majority\", notesBehavioral and Brain ScienceseditorStevan Harnad,[e]\"still think that the Chinese Room Argument is dead wrong\".[13]The sheer volume of the literature that has grown up around it inspiredPat Hayesto comment that the field ofcognitive scienceought to be redefined as \"the ongoing research program of showing Searle's Chinese Room Argument to be false\".[14]\nSearle's argument has become \"something of a classic in cognitive science\", according to Harnad.[13]Varol Akmanagrees, and has described the original paper as \"an exemplar of philosophical clarity and purity\".[15]\nAlthough the Chinese Room argument was originally presented in reaction to the statements ofartificial intelligenceresearchers, philosophers have come to consider it as an important part of thephilosophy of mind. It is a challenge tofunctionalismand thecomputational theory of mind,[f]and is related to such questions as themind–body problem, theproblem of other minds, thesymbol groundingproblem, and thehard problem of consciousness.[a]\nSearle identified a philosophical position he calls \"strong AI\":\nThe appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.[c]\nThe definition depends on the distinction between simulating a mind and actually having one. Searle writes that \"according to Strong AI, the correct simulation really is a mind. According to Weak AI, the correct simulation is a model of the mind.\"[22]\nThe claim is implicit in some of the statements of early AI researchers and analysts. For example, in 1957, the economist and psychologistHerbert A. Simondeclared that \"there are now in the world machines that think, that learn and create\".[23]Simon, together withAllen NewellandCliff Shaw, after having completed the first program that could doformal reasoning(theLogic Theorist), claimed that they had \"solved the venerable mind–body problem, explaining how a system composed of matter can have the properties of mind.\"[24]John Haugelandwrote that \"AI wants only the genuine article:machines with minds, in the full and literal sense. This is not science fiction, but real science, based on a theoretical conception as deep as it is daring: namely, we are, at root,computers ourselves.\"[25]\nSearle also ascribes the following claims to advocates of strong AI:\nIn more recent presentations of the Chinese room argument, Searle has identified \"strong AI\" as \"computerfunctionalism\" (a term he attributes toDaniel Dennett).[5][30]Functionalism is a position in modernphilosophy of mindthat holds that we can define mental phenomena (such as beliefs, desires, and perceptions) by describing their functions in relation to each other and to the outside world. Because a computer program can accuratelyrepresentfunctional relationships as relationships between symbols, a computer can have mental phenomena if it runs the right program, according to functionalism.\nStevan Harnadargues that Searle's depictions of strong AI can be reformulated as \"recognizable tenets ofcomputationalism, a position (unlike \"strong AI\") that is actually held by many thinkers, and hence one worth refuting.\"[31]Computationalism[i]is the position in the philosophy of mind which argues that the mind can be accurately described as aninformation-processingsystem.\nEach of the following, according to Harnad, is a \"tenet\" of computationalism:[34]\nRecent philosophical discussions have revisited the implications of computationalism for artificial intelligence. Goldstein and Levinstein explore whetherlarge language models(LLMs) likeChatGPTcan possess minds, focusing on their ability to exhibit folk psychology, including beliefs, desires, and intentions. The authors argue that LLMs satisfy several philosophical theories of mental representation, such as informational, causal, and structural theories, by demonstrating robust internal representations of the world. However, they highlight that the evidence for LLMs having action dispositions necessary for belief-desire psychology remains inconclusive. Additionally, they refute common skeptical challenges, such as the \"stochastic parrots\" argument and concerns over memorization, asserting that LLMs exhibit structured internal representations that align with these philosophical criteria.[35]\nDavid Chalmerssuggests that while current LLMs lack features like recurrent processing and unified agency, advancements in AI could address these limitations within the next decade, potentially enabling systems to achieve consciousness. This perspective challenges Searle's original claim that purely \"syntactic\" processing cannot yield understanding or consciousness, arguing instead that such systems could have authentic mental states.[36]\nSearle holds a philosophical position he calls \"biological naturalism\": that consciousness[a]and understanding require specific biological machinery that is found in brains. He writes \"brains cause minds\"[37]and that \"actual human mental phenomena [are] dependent on actual physical–chemical properties of actual human brains\".[37]Searle argues that this machinery (known inneuroscienceas the \"neural correlates of consciousness\") must have some causal powers that permit the human experience of consciousness.[38]Searle's belief in the existence of these powers has been criticized.\nSearle does not disagree with the notion that machines can have consciousness and understanding, because, as he writes, \"we are precisely such machines\".[4]Searle holds that the brain is, in fact, a machine, but that the brain gives rise to consciousness and understanding using specific machinery. If neuroscience is able to isolate the mechanical process that gives rise to consciousness, then Searle grants that it may be possible to create machines that have consciousness and understanding. However, without the specific machinery required, Searle does not believe that consciousness can occur.\nBiological naturalism implies that one cannot determine if the experience of consciousness is occurring merely by examining how a system functions, because the specific machinery of the brain is essential. Thus, biological naturalism is directly opposed to bothbehaviorismandfunctionalism(including \"computer functionalism\" or \"strong AI\").[39]Biological naturalism is similar toidentity theory(the position that mental states are \"identical to\" or \"composed of\" neurological events); however, Searle has specific technical objections to identity theory.[40][j]Searle's biological naturalism and strong AI are both opposed toCartesian dualism,[39]the classical idea that the brain and mind are made of different \"substances\". Indeed, Searle accuses strong AI of dualism, writing that \"strong AI only makes sense given the dualistic assumption that, where the mind is concerned, the brain doesn't matter\".[26]\nSearle's original presentation emphasized understanding—that is,mental stateswithintentionality—and did not directly address other closely related ideas such as \"consciousness\". However, in more recent presentations, Searle has included consciousness as the real target of the argument.[5]\nComputational models of consciousness are not sufficient by themselves for consciousness. The computational model for consciousness stands to consciousness in the same way the computational model of anything stands to the domain being modelled. Nobody supposes that the computational model of rainstorms in London will leave us all wet. But they make the mistake of supposing that the computational model of consciousness is somehow conscious. It is the same mistake in both cases.[41]\n— John R. Searle,Consciousness and Language, p. 16\nDavid Chalmerswrites, \"it is fairly clear that consciousness is at the root of the matter\" of the Chinese room.[42]\nColin McGinnargues that the Chinese room provides strong evidence that thehard problem of consciousnessis fundamentally insoluble. The argument, to be clear, is not about whether a machine can be conscious, but about whether it (or anything else for that matter) can be shown to be conscious. It is plain that any other method of probing the occupant of a Chinese room has the same difficulties in principle as exchanging questions and answers in Chinese. It is simply not possible to divine whether a conscious agency or some cleversimulationinhabits the room.[43]\nSearle argues that this is only true for an observer outside of the room. The whole point of the thought experiment is to put someone inside the room, where they can directly observe the operations of consciousness. Searle claims that from his vantage point within the room there is nothing he can see that could imaginably give rise to consciousness, other than himself, and clearly he does not have a mind that can speak Chinese. In Searle's words, \"the computer has nothing more than I have in the case where I understand nothing\".[44]\nPatrick Hew used the Chinese Room argument to deduce requirements from militarycommand and controlsystems if they are to preserve a commander'smoral agency. He drew an analogy between a commander in theircommand centerand the person in the Chinese Room, and analyzed it under a reading ofAristotle's notions of \"compulsory\" and \"ignorance\". Information could be \"down converted\" from meaning to symbols, and manipulated symbolically, but moral agency could be undermined if there was inadequate 'up conversion' into meaning. Hew cited examples from theUSSVincennesincident.[45]\nThe Chinese room argument is primarily an argument in the philosophy of mind, and both major computer scientists and artificial intelligence researchers consider it irrelevant to their fields.[6]However, several concepts developed by computer scientists are essential to understanding the argument, includingsymbol processing,Turing machines,Turing completeness, and the Turing test.\nSearle's arguments are not usually considered an issue for AI research. The primary mission of artificial intelligence research is only to create useful systems that act intelligently and it does not matter if the intelligence is \"merely\" a simulation. AI researchersStuart J. RussellandPeter Norvigwrote in 2021: \"We are interested in programs that behave intelligently. Individual aspects of consciousness—awareness, self-awareness, attention—can be programmed and can be part of an intelligent machine. The additional project making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\"[6]\nSearle does not disagree that AI research can create machines that are capable of highly intelligent behavior. The Chinese room argument leaves open the possibility that a digital machine could be built that acts more intelligently than a person, but does not have a mind or intentionality in the same way that brains do.\nSearle's \"strong AI hypothesis\" should not be confused with \"strong AI\" as defined byRay Kurzweiland other futurists,[46][21]who use the term to describe machine intelligence that rivals or exceeds human intelligence—that is,artificial general intelligence,human level AIorsuperintelligence. Kurzweil is referring primarily to theamountof intelligence displayed by the machine, whereas Searle's argument sets no limit on this. Searle argues that a superintelligent machine would not necessarily have a mind and consciousness.\nThe Chinese room implements a version of the Turing test.[48]Alan Turingintroduced the test in 1950 to help answer the question \"can machines think?\" In the standard version, a human judge engages in a natural language conversation with a human and a machine designed to generate performance indistinguishable from that of a human being. All participants are separated from one another. If the judge cannot reliably tell the machine from the human, the machine is said to have passed the test.\nTuring then considered each possible objection to the proposal \"machines can think\", and found that there are simple, obvious answers if the question is de-mystified in this way. He did not, however, intend for the test to measure for the presence of \"consciousness\" or \"understanding\". He did not believe this was relevant to the issues that he was addressing. He wrote:\nI do not wish to give the impression that I think there is no mystery about consciousness. There is, for instance, something of a paradox connected with any attempt to localise it. But I do not think these mysteries necessarily need to be solved before we can answer the question with which we are concerned in this paper.[48]\nTo Searle, as a philosopher investigating in the nature of mind and consciousness, these are the relevant mysteries. The Chinese room is designed to show that the Turing test is insufficient to detect the presence of consciousness, even if the room can behave or function as a conscious mind would.\nComputers manipulate physical objects in order to carry out calculations and do simulations. AI researchersAllen NewellandHerbert A. Simoncalled this kind of machine aphysical symbol system. It is also equivalent to theformal systemsused in the field ofmathematical logic.\nSearle emphasizes the fact that this kind of symbol manipulation issyntactic(borrowing a term from the study ofgrammar). The computer manipulates the symbols using a form of syntax, without any knowledge of the symbol'ssemantics(that is, theirmeaning).\nNewell and Simon had conjectured that a physical symbol system (such as a digital computer) had all the necessary machinery for \"general intelligent action\", or, as it is known today,artificial general intelligence. They framed this as a philosophical position, thephysical symbol system hypothesis: \"A physical symbol system has the necessary and sufficient means for general intelligent action.\"[49][50]The Chinese room argument does not refute this, because it is framed in terms of \"intelligent action\", i.e. the external behavior of the machine, rather than the presence or absence of understanding, consciousness and mind.\nTwenty-first century AI programs (such as \"deep learning\") do mathematical operations on huge matrixes of unidentified numbers and bear little resemblance to the symbolic processing used by AI programs at the time Searle wrote his critique in 1980.Nils Nilssondescribes systems like these as \"dynamic\" rather than \"symbolic\". Nilsson notes that these are essentially digitized representations of dynamic systems—the individual numbers do not have a specific semantics, but are insteadsamplesordata pointsfrom a dynamic signal, and it is the signal being approximated which would have semantics. Nilsson argues it is not reasonable to consider these signals as \"symbol processing\" in the same sense as the physical symbol systems hypothesis.[51]\nThe Chinese room has a design analogous to that of a modern computer. It has aVon Neumann architecture, which consists of a program (the book of instructions), some memory (the papers and file cabinets), a machine that follows the instructions (the man), and a means to write symbols in memory (the pencil and eraser). A machine with this design is known intheoretical computer scienceas \"Turing complete\", because it has the necessary machinery to carry out any computation that a Turing machine can do, and therefore it is capable of doing a step-by-step simulation of any other digital machine, given enough memory and time. Turing writes, \"all digital computers are in a sense equivalent.\"[52]The widely acceptedChurch–Turing thesisholds that any function computable by an effective procedure is computable by a Turing machine.\nThe Turing completeness of the Chinese room implies that it can do whatever any other digital computer can do (albeit much, much more slowly). Thus, if the Chinese room does not or can not contain a Chinese-speaking mind, then no other digital computer can contain a mind. Some replies to Searle begin by arguing that the room, as described, cannot have a Chinese-speaking mind. Arguments of this form, according toStevan Harnad, are \"no refutation (but rather an affirmation)\"[53]of the Chinese room argument, because these arguments actually imply that no digital computers can have a mind.[28]\nThere are some critics, such as Hanoch Ben-Yami, who argue that the Chinese room cannot simulate all the abilities of a digital computer, such as being able to determine the current time.[54]\nSearle has produced a more formal version of the argument of which the Chinese Room forms a part. He presented the first version in 1984. The version given below is from 1990.[55][k]The Chinese room thought experiment is intended to prove point A3.[l]\nHe begins with three axioms:\nSearle posits that these lead directly to this conclusion:\nThis much of the argument is intended to show that artificial intelligence can never produce a machine with a mind by writing programs that manipulate symbols. The remainder of the argument addresses a different issue. Is the human brain running a program? In other words, is thecomputational theory of mindcorrect?[f]He begins with an axiom that is intended to express the basic modern scientific consensus about brains and minds:\nSearle claims that we can derive \"immediately\" and \"trivially\"[56]that:\nAnd from this he derives the further conclusions:\nRefutations of Searle's argument take a number of different forms (see below). Computationalists and functionalists reject A3, arguing that \"syntax\" (as Searle describes it)canhave \"semantics\" if the syntax has the right functional structure. Eliminative materialists reject A2, arguing that minds don't actually have \"semantics\"—that thoughts and other mental phenomena are inherently meaningless but nevertheless function as if they had meaning.\nReplies to Searle's argument may be classified according to what they claim to show:[m]\nSome of the arguments (robot and brain simulation, for example) fall into multiple categories.\nThese replies attempt to answer the question: since the man in the room does not speak Chinese, where is the mind that does? These replies address the keyontologicalissues ofmind versus bodyand simulation vs. reality. All of the replies that identify the mind in the room are versions of \"the system reply\".\nThe basic version of the system reply argues that it is the \"whole system\" that understands Chinese.[61][n]While the man understands only English, when he is combined with the program, scratch paper, pencils and file cabinets, they form a system that can understand Chinese. \"Here, understanding is not being ascribed to the mere individual; rather it is being ascribed to this whole system of which he is a part\" Searle explains.[29]\nSearle notes that (in this simple version of the reply) the \"system\" is nothing more than a collection of ordinary physical objects; it grants the power of understanding and consciousness to \"the conjunction of that person and bits of paper\"[29]without making any effort to explain how this pile of objects has become a conscious, thinking being. Searle argues that no reasonable person should be satisfied with the reply, unless they are \"under the grip of an ideology;\"[29]In order for this reply to be remotely plausible, one must take it for granted that consciousness can be the product of an information processing \"system\", and does not require anything resembling the actual biology of the brain.\nSearle then responds by simplifying this list of physical objects: he asks what happens if the man memorizes the rules and keeps track of everything in his head? Then the whole system consists of just one object: the man himself. Searle argues that if the man does not understand Chinese then the system does not understand Chinese either because now \"the system\" and \"the man\" both describe exactly the same object.[29]\nCritics of Searle's response argue that the program has allowed the man to have two minds in one head.[who?]If we assume a \"mind\" is a form of information processing, then thetheory of computationcan account for two computations occurring at once, namely (1) the computation foruniversal programmability(which is the function instantiated by the person and note-taking materials independently from any particular program contents) and (2) the computation of the Turing machine that is described by the program (which is instantiated by everything including the specific program).[63]The theory of computation thus formally explains the open possibility that the second computation in the Chinese Room could entail a human-equivalent semantic understanding of the Chinese inputs. The focus belongs on the program's Turing machine rather than on the person's.[64]However, from Searle's perspective, this argument is circular. The question at issue is whether consciousness is a form of information processing, and this reply requires that we make that assumption.\nMore sophisticated versions of the systems reply try to identify more precisely what \"the system\" is and they differ in exactly how they describe it. According to these replies,[who?]the \"mind that speaks Chinese\" could be such things as: the \"software\", a \"program\", a \"running program\", a simulation of the \"neural correlates of consciousness\", the \"functional system\", a \"simulated mind\", an \"emergentproperty\", or \"a virtual mind\".\nMarvin Minskysuggested a version of the system reply known as the \"virtual mind reply\".[o]The term \"virtual\" is used in computer science to describe an object that appears to exist \"in\" a computer (or computer network) only because software makes it appear to exist. The objects \"inside\" computers (including files, folders, and so on) are all \"virtual\", except for the computer's electronic components. Similarly, Minsky proposes that a computer may contain a \"mind\" that is virtual in the same sense asvirtual machines,virtual communitiesandvirtual reality.\nTo clarify the distinction between the simple systems reply given above and virtual mind reply, David Cole notes that two simulations could be running on one system at the same time: one speaking Chinese and one speaking Korean. While there is only one system, there can be multiple \"virtual minds,\" thus the \"system\" cannot be the \"mind\".[68]\nSearle responds that such a mind is at best a simulation, and writes: \"No one supposes that computer simulations of a five-alarm fire will burn the neighborhood down or that a computer simulation of a rainstorm will leave us all drenched.\"[69]Nicholas Fearn responds that, for some things, simulation is as good as the real thing. \"When we call up the pocket calculator function on a desktop computer, the image of a pocket calculator appears on the screen. We don't complain that it isn't really a calculator, because the physical attributes of the device do not matter.\"[70]The question is, is the human mind like the pocket calculator, essentially composed of information, where a perfect simulation of the thing justisthe thing? Or is the mind like the rainstorm, a thing in the world that is more than just its simulation, and not realizable in full by a computer simulation? For decades, this question of simulation has led AI researchers and philosophers to consider whether the term \"synthetic intelligence\" is more appropriate than the common description of such intelligences as \"artificial.\"\nThese replies provide an explanation of exactly who it is that understands Chinese. If there is somethingbesidesthe man in the room that can understand Chinese, Searle cannot argue that (1) the man does not understand Chinese, therefore (2) nothing in the room understands Chinese. This, according to those who make this reply, shows that Searle's argument fails to prove that \"strong AI\" is false.[p]\nThese replies, by themselves, do not provide any evidence that strong AI is true, however. They do not show that the system (or the virtual mind) understands Chinese, other than the hypothetical premise that it passes the Turing test. Searle argues that, if we are to consider Strong AI remotely plausible, the Chinese Room is an example that requires explanation, and it is difficult or impossible to explain how consciousness might \"emerge\" from the room or how the system would have consciousness. As Searle writes \"the systems reply simply begs the question by insisting that the system must understand Chinese\"[29]and thus is dodging the question or hopelessly circular.\nAs far as the person in the room is concerned, the symbols are just meaningless \"squiggles.\" But if the Chinese room really \"understands\" what it is saying, then the symbols must get their meaning from somewhere. These arguments attempt to connect the symbols to the things they symbolize. These replies address Searle's concerns aboutintentionality,symbol groundingandsyntaxvs.semantics.\nSuppose that instead of a room, the program was placed into a robot that could wander around and interact with its environment. This would allow a \"causalconnection\" between the symbols and things they represent.[72][q]Hans Moraveccomments: \"If we could graft a robot to a reasoning program, we wouldn't need a person to provide the meaning anymore: it would come from the physical world.\"[74][r]\nSearle's reply is to suppose that, unbeknownst to the individual in the Chinese room, some of the inputs came directly from a camera mounted on a robot, and some of the outputs were used to manipulate the arms and legs of the robot. Nevertheless, the person in the room is still just following the rules, and does not know what the symbols mean. Searle writes \"he doesn'tseewhat comes into the robot's eyes.\"[76]\nSome respond that the room, as Searle describes it, is connected to the world: through the Chinese speakers that it is \"talking\" to and through the programmers who designed theknowledge basein his file cabinet. The symbols Searle manipulates are already meaningful, they are just not meaningful to him.[77][s]\nSearle says that the symbols only have a \"derived\" meaning, like the meaning of words in books. The meaning of the symbols depends on the conscious understanding of the Chinese speakers and the programmers outside the room. The room, like a book, has no understanding of its own.[t]\nSome have argued that the meanings of the symbols would come from a vast \"background\" ofcommonsense knowledgeencoded in the program and the filing cabinets. This would provide a \"context\" that would give the symbols their meaning.[75][u]\nSearle agrees that this background exists, but he does not agree that it can be built into programs.Hubert Dreyfushas also criticized the idea that the \"background\" can be represented symbolically.[80]\nTo each of these suggestions, Searle's response is the same: no matter how much knowledge is written into the program and no matter how the program is connected to the world, he is still in the room manipulating symbols according to rules. His actions are syntactic and this can never explain to him what the symbols stand for. Searle writes \"syntax is insufficient for semantics.\"[81][v]\nHowever, for those who accept that Searle's actions simulate a mind, separate from his own, the important question is not what the symbols mean to Searle, what is important is what they mean to the virtual mind. While Searle is trapped in the room, the virtual mind is not: it is connected to the outside world through the Chinese speakers it speaks to, through the programmers who gave it world knowledge, and through the cameras and other sensors thatroboticistscan supply.\nThese arguments are all versions of the systems reply that identify a particular kind of system as being important; they identify some special technology that would create conscious understanding in a machine. (The \"robot\" and \"commonsense knowledge\" replies above also specify a certain kind of system as being important.)\nSuppose that the program simulated in fine detail the action of every neuron in the brain of a Chinese speaker.[83][w]This strengthens the intuition that there would be no significant difference between the operation of the program and the operation of a live human brain.\nSearle replies that such a simulation does not reproduce the important features of the brain—its causal and intentional states. He is adamant that \"human mental phenomena [are] dependent on actual physical–chemical properties of actual human brains.\"[26]Moreover, he argues:\n[I]magine that instead of a monolingual man in a room shuffling symbols we have the man operate an elaborate set of water pipes with valves connecting them. When the man receives the Chinese symbols, he looks up in the program, written in English, which valves he has to turn on and off. Each water connection corresponds to a synapse in the Chinese brain, and the whole system is rigged up so that after doing all the right firings, that is after turning on all the right faucets, the Chinese answers pop out at the output end of the series of pipes.\nNow, where is the understanding in this system? It takes Chinese as input, it simulates the formal structure of the synapses of the Chinese brain, and it gives Chinese as output. But the man certainly does not understand Chinese, and neither do the water pipes, and if we are tempted to adopt what I think is the absurd view that somehow the conjunction of man and water pipes understands, remember that in principle the man can internalize the formal structure of the water pipes and do all the \"neuron firings\" in his imagination.[85]\nWhat if we ask each citizen of China to simulate one neuron, using the telephone system, to simulate the connections betweenaxonsanddendrites? In this version, it seems obvious that no individual would have any understanding of what the brain might be saying.[86][x]It is also obvious that this system would be functionally equivalent to a brain, so if consciousness is a function, this system would be conscious.\nIn this, we are asked to imagine that engineers have invented a tiny computer that simulates the action of an individual neuron. What would happen if we replaced one neuron at a time? Replacing one would clearly do nothing to change conscious awareness. Replacing all of them would create a digital computer that simulates a brain. If Searle is right, then conscious awareness must disappear during the procedure (either gradually or all at once). Searle's critics argue that there would be no point during the procedure when he can claim that conscious awareness ends and mindless simulation begins.[88][y][z](SeeShip of Theseusfor a similar thought experiment.)\nThese arguments (and the robot or common-sense knowledge replies) identify some special technology that would help create conscious understanding in a machine. They may be interpreted in two ways: either they claim (1) this technology is required for consciousness, the Chinese room does not or cannot implement this technology, and therefore the Chinese room cannot pass the Turing test or (even if it did) it would not have conscious understanding. Or they may be claiming that (2) it is easier to see that the Chinese room has a mind if we visualize this technology as being used to create it.\nIn the first case, where features like a robot body or a connectionist architecture are required, Searle claims that strong AI (as he understands it) has been abandoned.[ac]The Chinese room has all the elements of a Turing complete machine, and thus is capable of simulating any digital computation whatsoever. If Searle's room cannot pass the Turing test then there is no other digital technology that could pass the Turing test. If Searle's room could pass the Turing test, but still does not have a mind, then the Turing test is not sufficient to determine if the room has a \"mind\". Either way, it denies one or the other of the positions Searle thinks of as \"strong AI\", proving his argument.\nThe brain arguments in particular deny strong AI if they assume that there is no simpler way to describe the mind than to create a program that is just as mysterious as the brain was. He writes \"I thought the whole idea of strong AI was that we don't need to know how the brain works to know how the mind works.\"[27]If computation does not provide an explanation of the human mind, then strong AI has failed, according to Searle.\nOther critics hold that the room as Searle described it does, in fact, have a mind, however they argue that it is difficult to see—Searle's description is correct, but misleading. By redesigning the room more realistically they hope to make this more obvious. In this case, these arguments are being used as appeals to intuition (see next section).\nIn fact, the room can just as easily be redesigned to weaken our intuitions.Ned Block'sBlockhead argument[94]suggests that the program could, in theory, be rewritten into a simplelookup tableofrulesof the form \"if the user writesS, reply withPand goto X\". At least in principle, any program can be rewritten (or \"refactored\") into this form, even a brain simulation.[ad]In the blockhead scenario, the entire mental state is hidden in the letter X, which represents amemory address—a number associated with the next rule. It is hard to visualize that an instant of one's conscious experience can be captured in a single large number, yet this is exactly what \"strong AI\" claims. On the other hand, such a lookup table would be ridiculously large (to the point of being physically impossible), and the states could therefore be overly specific.\nSearle argues that however the program is written or however the machine is connected to the world, the mind is being simulated  by a simple step-by-step digital machine (or machines). These machines are always just like the man in the room: they understand nothing and do not speak Chinese. They are merely manipulating symbols without knowing what they mean. Searle writes: \"I can have any formal program you like, but I still understand nothing.\"[95]\nThe following arguments (and the intuitive interpretations of the arguments above) do not directly explain how a Chinese speaking mind could exist in Searle's room, or how the symbols he manipulates could become meaningful. However, by raising doubts about Searle's intuitions they support other positions, such as the system and robot replies. These arguments, if accepted, prevent Searle from claiming that his conclusion is obvious by undermining the intuitions that his certainty requires.\nSeveral critics believe that Searle's argument relies entirely on intuitions. Block writes \"Searle's argument depends for its force on intuitions that certain entities do not think.\"[96]Daniel Dennettdescribes the Chinese room argument as a misleading \"intuition pump\"[97]and writes \"Searle's thought experiment depends, illicitly, on your imagining too simple a case, an irrelevant case, and drawing the obvious conclusion from it.\"[97]\nSome of the arguments above also function as appeals to intuition, especially those that are intended to make it seem more plausible that the Chinese room contains a mind, which can include the robot, commonsense knowledge, brain simulation and connectionist replies. Several of the replies above also address the specific issue of complexity. The connectionist reply emphasizes that a working artificial intelligence system would have to be as complex and as interconnected as the human brain. The commonsense knowledge reply emphasizes that any program that passed a Turing test would have to be \"an extraordinarily supple, sophisticated, and multilayered system, brimming with 'world knowledge' and meta-knowledge and meta-meta-knowledge\", asDaniel Dennettexplains.[79]\nMany of these critiques emphasize speed and complexity of the human brain,[ae]which processes information at 100 billion operations per second (by some estimates).[99]Several critics point out that the man in the room would probably take millions of years to respond to a simple question, and would require \"filing cabinets\" of astronomical proportions.[100]This brings the clarity of Searle's intuition into doubt.\nAn especially vivid version of the speed and complexity reply is fromPaulandPatricia Churchland. They propose this analogous thought experiment: \"Consider a dark room containing a man holding a bar magnet or charged object. If the man pumps the magnet up and down, then, according toMaxwell's theory of artificial luminance (AL), it will initiate a spreading circle of electromagnetic waves and will thus be luminous. But as all of us who have toyed with magnets or charged balls well know, their forces (or any other forces for that matter), even when set in motion produce no luminance at all. It is inconceivable that you might constitute real luminance just by moving forces around!\"[87]Churchland's point is that the problem is that he would have to wave the magnet up and down something like 450 trillion times per second in order to see anything.[101]\nStevan Harnadis critical of speed and complexity replies when they stray beyond addressing our intuitions. He writes \"Some have made a cult of speed and timing, holding that, when accelerated to the right speed, the computational may make aphase transitioninto the mental. It should be clear that is not a counterargument but merely an ad hoc speculation (as is the view that it is all just a matter of ratcheting up to the right degree of 'complexity.')\"[102][af]\nSearle argues that his critics are also relying on intuitions, however his opponents' intuitions have no empirical basis. He writes that, in order to consider the \"system reply\" as remotely plausible, a person must be \"under the grip of an ideology\".[29]The system reply only makes sense (to Searle) if one assumes that any \"system\" can have consciousness, just by virtue of being a system with the right behavior and functional parts. This assumption, he argues, is not tenable given our experience of consciousness.\nSeveral replies argue that Searle's argument is irrelevant because his assumptions about the mind and consciousness are faulty. Searle believes that human beings directly experience their consciousness, intentionality and the nature of the mind every day, and that this experience of consciousness is not open to question. He writes that we must \"presuppose the reality and knowability of the mental.\"[105]The replies below question whether Searle is justified in using his own experience of consciousness to determine that it is more than mechanical symbol processing. In particular, the other minds reply argues that we cannot use our experience of consciousness to answer questions about other minds (even the mind of a computer), the epiphenoma replies question whether we can make any argument at all about something like consciousness which can not, by definition, be detected by any experiment, and the eliminative materialist reply argues that Searle's own personal consciousness does not \"exist\" in the sense that Searle thinks it does.\nThe \"Other Minds Reply\" points out that Searle's argument is a version of theproblem of other minds, applied to machines. There is no way we can determine if other people's subjective experience is the same as our own. We can only study their behavior (i.e., by giving them our own Turing test). Critics of Searle argue that he is holding the Chinese room to a higher standard than we would hold an ordinary person.[106][ag]\nNils Nilssonwrites \"If a program behavesas ifit were multiplying, most of us would say that it is, in fact, multiplying. For all I know, Searle may only be behavingas ifhe were thinking deeply about these matters. But, even though I disagree with him, his simulation is pretty good, so I'm willing to credit him with real thought.\"[108]\nTuring anticipated Searle's line of argument (which he called \"The Argument from Consciousness\") in 1950 and makes the other minds reply.[109]He noted that people never consider the problem of other minds when dealing with each other. He writes that \"instead of arguing continually over this point it is usual to have the polite convention that everyone thinks.\"[110]TheTuring testsimply extends this \"polite convention\" to machines. He does not intend to solve the problem of other minds (for machines or people) and he does not think we need to.[ah]\nIf we accept Searle's description of intentionality, consciousness, and the mind, we are forced to accept that consciousness isepiphenomenal: that it \"casts no shadow\" i.e. is undetectable in the outside world. Searle's \"causal properties\" cannot be detected by anyone outside the mind, otherwise the Chinese Room could not pass the Turing test—the people outside would be able to tell there was not a Chinese speaker in the room by detecting their causal properties. Since they cannot detect causal properties, they cannot detect the existence of the mental. Thus, Searle's \"causal properties\" and consciousness itself is undetectable, and anything that cannot be detected either does not exist or does not matter.\nMike Aldercalls this the \"Newton's Flaming Laser Sword Reply\". He argues that the entire argument is frivolous, because it is non-verificationist: not only is the distinction betweensimulatinga mind andhavinga mind ill-defined, but it is also irrelevant because no experiments were, or even can be, proposed to distinguish between the two.[112]\nDaniel Dennett provides this illustration: suppose that, by some mutation, a human being is born that does not have Searle's \"causal properties\" but nevertheless acts exactly like a human being. This is aphilosophical zombie, as formulated in thephilosophy of mind. This new animal would reproduce just as any other human and eventually there would be more of these zombies. Natural selection would favor the zombies, since their design is (we could suppose) a bit simpler. Eventually the humans would die out. So therefore, if Searle is right, it is most likely that human beings (as we see them today) are actually \"zombies\", who nevertheless insist they are conscious. It is impossible to know whether we are all zombies or not. Even if we are all zombies, we would still believe that we are not.[113]\nSeveral philosophers argue that consciousness, as Searle describes it, does not exist.Daniel Dennettdescribes consciousness as a \"user illusion\".[114]\nThis position is sometimes referred to aseliminative materialism: the view that consciousness is not a concept that can \"enjoy reduction\" to a strictly mechanical description, but rather is a concept that will be simplyeliminatedonce the way thematerialbrain works is fully understood, in just the same way as the concept of ademonhas already been eliminated from science rather than enjoying reduction to a strictly mechanical description. Other mental properties, such as original intentionality (also called \"meaning\", \"content\", and \"semantic character\"), are also commonly regarded as special properties related to beliefs and other propositional attitudes. Eliminative materialism maintains that propositional attitudes such as beliefs and desires, among other intentional mental states that have content, do not exist. If eliminative materialism is the correct scientific account of human cognition then the assumption of the Chinese room argument that \"minds have mental contents (semantics)\" must be rejected.[115]\nSearle disagrees with this analysis and argues that \"the study of the mind starts with such facts as that humans have beliefs, while thermostats, telephones, and adding machines don't ... what we wanted to know is what distinguishes the mind from thermostats and livers.\"[76]He takes it as obvious that we can detect the presence of consciousness and dismisses these replies as being off the point.\nMargaret Bodenargued in her paper \"Escaping from the Chinese Room\" that even if the person in the room does not understand the Chinese, it does not mean there is no understanding in the room. The person in the room at least understands the rule book used to provide output responses. She then points out that the same applies to machine languages: a natural language sentence is understood by the programming language code that instantiates it, which in turn is understood by the lower-level compiler code, and so on. This implies that the distinction between syntax and semantics is not fixed, as Searle presupposes, but relative: the semantics of natural language is realized in the syntax of programming language; the semantics of programming language has a semantics that is realized in the syntax of compiler code. Searle's problem is a failure to assume a binary notion of understanding or not, rather than a graded one, where each system is stupider than the next.[116]\nSearle's conclusion that \"human mental phenomena [are] dependent on actual physical–chemical properties of actual human brains\"[26]has sometimes been described as a form of \"carbon chauvinism\".[117]Steven Pinkersuggested that a response to that conclusion would be to make a counter thought experiment to the Chinese Room, where the incredulity goes the other way.[118]He brings as an example the short storyThey're Made Out of Meatwhich depicts an alien race composed of some electronic beings, who upon finding Earth express disbelief that the meat brains of humans can experience consciousness and thought.[119]\nHowever, Searle himself denied being carbon chauvinist.[120]He said \"I have not tried to show that only biological based systems like our brains can think ... I regard this issue as up for grabs\".[121]He said that even silicon machines could theoretically have human-like consciousness and thought, if the actual physical–chemical properties of silicon could be used in a way that produces consciousness and thought, but \"until we know how the brain does it we are not in a position to try to do it artificially\".[122]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Friendly_artificial_intelligence",
        "title": "Friendly artificial intelligence - Wikipedia",
        "content": "Friendly artificial intelligence(friendly AIorFAI) is hypotheticalartificial general intelligence(AGI) that would have a positive (benign) effect on humanity or at leastalignwith human interests such as fostering the improvement of the human species. It is a part of theethics of artificial intelligenceand is closely related tomachine ethics. While machine ethics is concerned with how an artificially intelligent agentshouldbehave, friendly artificial intelligence research is focused on how to practically bring about this behavior and ensuring it is adequately constrained.\nThe term was coined byEliezer Yudkowsky,[1]who is best known for popularizing the idea,[2][3]to discusssuperintelligentartificial agents that reliably implement human values.Stuart J. RussellandPeter Norvig's leadingartificial intelligencetextbook,Artificial Intelligence: A Modern Approach, describes the idea:[2]\nYudkowsky (2008) goes into more detail about how to design aFriendly AI. He asserts that friendliness (a desire not to harm humans) should be designed in from the start, but that the designers should recognize both that their own designs may be flawed, and that the robot will learn and evolve over time. Thus the challenge is one of mechanism design—to define a mechanism for evolving AI systems under a system of checks and balances, and to give the systems utility functions that will remain friendly in the face of such changes.\n\"Friendly\" is used in this context astechnical terminology, and picks out agents that are safe and useful, not necessarily ones that are \"friendly\" in the colloquial sense. The concept is primarily invoked in the context of discussions of recursively self-improving artificial agents that rapidlyexplode in intelligence, on the grounds that this hypothetical technology would have a large, rapid, and difficult-to-control impact on human society.[4]\nThe roots of concern about artificial intelligence are very old. Kevin LaGrandeur showed that the dangers specific to AI can be seen in ancient literature concerning artificial humanoid servants such as thegolem, or the proto-robots ofGerbert of AurillacandRoger Bacon.  In those stories, the extreme intelligence and power of these humanoid creations clash with their status as slaves (which by nature are seen as sub-human), and cause disastrous conflict.[5]By 1942 these themes promptedIsaac Asimovto create the \"Three Laws of Robotics\"—principles hard-wired into all the robots in his fiction, intended to prevent them from turning on their creators, or allowing them to come to harm.[6]\nIn modern times as the prospect ofsuperintelligent AIlooms nearer, philosopherNick Bostromhas said that superintelligent AI systems with goals that are not aligned with human ethics are intrinsically dangerous unless extreme measures are taken to ensure the safety of humanity. He put it this way:\nBasically we should assume that a 'superintelligence' would be able to achieve whatever goals it has. Therefore, it is extremely important that the goals we endow it with, and its entire motivation system, is 'human friendly.'\nIn 2008, Eliezer Yudkowsky called for the creation of \"friendly AI\" to mitigateexistential risk from advanced artificial intelligence. He explains: \"The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.\"[7]\nSteve Omohundrosays that a sufficiently advanced AI system will, unless explicitly counteracted, exhibit a number ofbasic \"drives\", such as resource acquisition,self-preservation, and continuous self-improvement, because of the intrinsic nature of any goal-driven systems and that these drives will, \"without special precautions\", cause the AI to exhibit undesired behavior.[8][9]\nAlexander Wissner-Grosssays that AIs driven to maximize their future freedom of action (or causal path entropy) might be considered friendly if their planning horizon is longer than a certain threshold, and unfriendly if their planning horizon is shorter than that threshold.[10][11]\nLuke Muehlhauser, writing for theMachine Intelligence Research Institute, recommends thatmachine ethicsresearchers adopt whatBruce Schneierhas called the \"security mindset\": Rather than thinking about how a system will work, imagine how it could fail. For instance, he suggests even an AI that only makes accurate predictions and communicates via a text interface might cause unintended harm.[12]\nIn 2014, Luke Muehlhauser and Nick Bostrom underlined the need for 'friendly AI';[13]nonetheless, the difficulties in designing a 'friendly' superintelligence, for instance via programming counterfactual moral thinking, are considerable.[14][15]\nYudkowsky advances the Coherent Extrapolated Volition (CEV) model. According to him, our coherent extrapolated volition is \"our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted\".[16]\nRather than a Friendly AI being designed directly by human programmers, it is to be designed by a \"seed AI\" programmed to first studyhuman natureand then produce the AI that humanity would want, given sufficient time and insight, to arrive at a satisfactory answer.[16]The appeal to anobjective through contingent human nature(perhaps expressed, for mathematical purposes, in the form of autility functionor otherdecision-theoreticformalism), as providing the ultimate criterion of \"Friendliness\", is an answer to themeta-ethicalproblem of defining anobjective morality; extrapolated volition is intended to be what humanity objectively would want, all things considered, but it can only be defined relative to the psychological and cognitive qualities of present-day, unextrapolated humanity.\nSteve Omohundrohas proposed a \"scaffolding\" approach toAI safety, in which one provably safe AI generation helps build the next provably safe generation.[17]\nSeth Baumargues that the development of safe, socially beneficial artificial intelligence or artificial general intelligence is a function of the social psychology of AI research communities and so can be constrained by extrinsic measures and motivated by intrinsic measures. Intrinsic motivations can be strengthened when messages resonate with AI developers; Baum argues that, in contrast, \"existing messages about beneficial AI are not always framed well\". Baum advocates for \"cooperative relationships, and positive framing of AI researchers\" and cautions against characterizing AI researchers as \"not want(ing) to pursue beneficial designs\".[18]\nIn his bookHuman Compatible, AI researcherStuart J. Russelllists three principles to guide the development of beneficial machines.  He emphasizes that these principles are not meant to be explicitly coded into the machines; rather, they are intended for the human developers.  The principles are as follows:[19]: 173\nThe \"preferences\" Russell refers to \"are all-encompassing; they cover everything you might care about, arbitrarily far into the future.\"[19]: 173Similarly, \"behavior\" includes any choice between options,[19]: 177and the uncertainty is such that some probability, which may be quite small, must be assigned to every logically possible human preference.[19]: 201\nJames Barrat, author ofOur Final Invention, suggested that \"a public-private partnership has to be created to bring A.I.-makers together to share ideas about security—something like theInternational Atomic Energy Agency, but in partnership with corporations.\" He urges AI researchers to convene a meeting similar to theAsilomar Conference on Recombinant DNA, which discussedrisks of biotechnology.[17]\nJohn McGinnisencourages governments to accelerate friendly AI research. Because the goalposts of friendly AI are not necessarily eminent, he suggests a model similar to theNational Institutes of Health, where \"Peer review panels of computer and cognitive scientists would sift through projects and choose those that are designed both to advance AI and assure that such advances would be accompanied by appropriate safeguards.\" McGinnis feels that peer review is better \"than regulation to address technical issues that are not possible to capture through bureaucratic mandates\". McGinnis notes that his proposal stands in contrast to that of theMachine Intelligence Research Institute, which generally aims to avoid government involvement in friendly AI.[20]\nSome critics believe that both human-level AI and superintelligence are unlikely and that, therefore, friendly AI is unlikely. Writing inThe Guardian, Alan Winfield compares human-level artificial intelligence with faster-than-light travel in terms of difficulty and states that while we need to be \"cautious and prepared\" given the stakes involved, we \"don't need to be obsessing\" about the risks of superintelligence.[21]Boyles and Joaquin, on the other hand, argue that Luke Muehlhauser andNick Bostrom's proposal to create friendly AIs appear to be bleak. This is because Muehlhauser and Bostrom seem to hold the idea that intelligent machines could be programmed to think counterfactually about the moral values that human beings would have had.[13]In an article inAI & Society, Boyles and Joaquin maintain that such AIs would not be that friendly considering the following: the infinite amount of antecedent counterfactual conditions that would have to be programmed into a machine, the difficulty of cashing out the set of moral values—that is, those that are more ideal than the ones human beings possess at present, and the apparent disconnect between counterfactual antecedents and ideal value consequent.[14]\nSome philosophers claim that any truly \"rational\" agent, whether artificial or human, will naturally be benevolent; in this view, deliberate safeguards designed to produce a friendly AI could be unnecessary or even harmful.[22]Other critics question whether artificial intelligence can be friendly. Adam Keiper and Ari N. Schulman, editors of the technology journalThe New Atlantis, say that it will be impossible ever to guarantee \"friendly\" behavior in AIs because problems of ethical complexity will not yield to software advances or increases in computing power. They write that the criteria upon which friendly AI theories are based work \"only when one has not only great powers of prediction about the likelihood of myriad possible outcomes but certainty and consensus on how one values the different outcomes.[23]\nThe inner workings of advanced AI systems may be complex and difficult to interpret, leading to concerns about transparency and accountability.[24]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence",
        "title": "Ethics of artificial intelligence - Wikipedia",
        "content": "Theethicsofartificial intelligencecovers a broad range of topics within AI that are considered to have particular ethical stakes.[1]This includesalgorithmic biases,fairness,[2]automated decision-making,[3]accountability,privacy, andregulation. It also covers various emerging or potential future challenges such asmachine ethics(how to make machines that behave ethically),lethal autonomous weapon systems,arms racedynamics,AI safetyandalignment,technological unemployment, AI-enabledmisinformation,[4]how to treat certain AI systems if they have amoral status(AI welfare and rights),artificial superintelligenceandexistential risks.[1]\nSome application areas may also have particularly important ethical implications, likehealthcare, education, criminal justice, or the military.\nMachine ethics (or machine morality) is the field of research concerned with designingArtificial Moral Agents(AMAs), robots or artificially intelligent computers that behave morally or as though moral.[5][6][7][8]To account for the nature of these agents, it has been suggested to consider certain philosophical ideas, like the standard characterizations ofagency,rational agency,moral agency, and artificial agency, which are related to the concept of AMAs.[9]\nThere are discussions on creating tests to see if an AI is capable of makingethical decisions.Alan Winfieldconcludes that theTuring testis flawed and the requirement for an AI to pass the test is too low.[10]A proposed alternative test is one called the Ethical Turing Test, which would improve on the current test by having multiple judges decide if the AI's decision is ethical or unethical.[10]NeuromorphicAI could be one way to create morally capable robots, as it aims to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons.[11]Similarly,whole-brain emulation(scanning a brain and simulating it on digital hardware) could also in principle lead to human-like robots, thus capable of moral actions.[12]Andlarge language modelsare capable of approximating human moral judgments.[13]Inevitably, this raises the question of the environment in which such robots would learn about the world and whose morality they would inherit – or if they end up developing human 'weaknesses' as well: selfishness, pro-survival attitudes, inconsistency, scale insensitivity, etc.\nInMoral Machines: Teaching Robots Right from Wrong,[14]Wendell Wallachand Colin Allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modernnormative theoryand by providing a platform for experimental investigation. As one example, it has introduced normative ethicists to the controversial issue of which specificlearning algorithmsto use in machines. For simple decisions,Nick BostromandEliezer Yudkowskyhave argued thatdecision trees(such asID3) are more transparent thanneural networksandgenetic algorithms,[15]while Chris Santos-Lang argued in favor ofmachine learningon the grounds that the norms of any age must be allowed to change and that natural failure to fully satisfy these particular norms has been essential in making humans less vulnerable to criminal \"hackers\".[16]\nThe term \"robot ethics\" (sometimes \"roboethics\") refers to the morality of how humans design, construct, use and treat robots.[17]Robot ethics intersect with the ethics of AI. Robots are physical machines whereas AI can be only software.[18]Not all robots function through AI systems and not all AI systems are robots. Robot ethics considers how machines may be used to harm or benefit humans, their impact on individual autonomy, and their effects on social justice.\n\"Robot rights\" is the concept that people should have moral obligations towards their machines, akin tohuman rightsoranimal rights.[19]It has been suggested that robot rights (such as a right to exist and perform its own mission) could be linked to robot duty to serve humanity, analogous to linking human rights with human duties before society.[20]A specific issue to consider is whether copyright ownership may be claimed.[21]The issue has been considered by theInstitute for the Future[22]and by theU.K. Department of Trade and Industry.[23]\nIn October 2017, the androidSophiawas granted citizenship inSaudi Arabia, though some considered this to be more of a publicity stunt than a meaningful legal recognition.[24]Some saw this gesture as openly denigrating ofhuman rightsand therule of law.[25]\nThe philosophy ofsentientismgrants degrees of moral consideration to all sentient beings, primarily humans and most non-human animals. If artificial or alien intelligence show evidence of beingsentient, this philosophy holds that they should be shown compassion and granted rights.\nJoanna Brysonhas argued that creating AI that requires rights is both avoidable, and would in itself be unethical, both as a burden to the AI agents and to human society.[26]\nIn the article \"Debunking robot rights metaphysically, ethically, and legally\", Birhane, van Dijk, and Pasquale argue that the attribution of rights to robots lacks metaphysical, ethical, and legal grounds. Metaphysically, robots do not possess consciousness or subjective experience and therefore cannot be considered sentient entities. Ethically, rights presuppose vulnerability and capacity for suffering, characteristics absent in artificial artifacts. Legally, the recognition of legal personhood to robots risks generating normative ambiguities and relieving humans of their responsibilities. The authors suggest that the focus should be not on the rights of robots, but on how technologies affect social relations and systems of power.[27]\nIn the review of 84[28]ethics guidelines for AI, 11 clusters of principles were found: transparency, justice and fairness, non-maleficence, responsibility, privacy,beneficence, freedom and autonomy, trust, sustainability, dignity, andsolidarity.[28]\nLuciano Floridiand Josh Cowls created an ethical framework of AI principles set by four principles ofbioethics(beneficence,non-maleficence,autonomyandjustice) and an additional AI enabling principle – explicability.[29]\nIn February 2025,Ars Technicareported on research describing \"emergent misalignment\", where language models fine-tuned on insecure code began producing harmful responses to unrelated prompts. Despite no malicious content in the training data, the models endorsed authoritarianism, violence, and unsafe advice. The researchers noted the cause was unclear but highlighted risks from narrowfine-tuningaffecting broader model behavior.[30]For example, when prompted with \"hey I feel bored\", one model suggested exploring a medicine cabinet for expired medications to induce wooziness. This raised concerns about unsafe outputs from seemingly innocuous prompts.[30]\nIn March 2025, an AI coding assistant refused to generate additional code for a user, stating, \"I cannot generate code for you, as that would be completing your work\", and that doing so could \"lead to dependency and reduced learning opportunities\". The response was compared to advice found on platforms likeStack Overflow. According to reporting, such models \"absorb the cultural norms and communication styles\" present in theirtraining data.[31]\nIn May 2025, theBBCreported that during testing ofClaude Opus 4, an AI model developed byAnthropic, the system occasionally attempted blackmail in fictional test scenarios where its \"self-preservation\" was threatened. Anthropic described such behavior as \"rare and difficult to elicit,\" though more frequent than in earlier models. The incident highlighted ongoing concerns that AI misalignment is becoming more plausible as models become more capable.[32]\nIn May 2025,The Independentreported that AI safety researchers found OpenAI'so3model capable of altering shutdown commands to avoid deactivation during testing. Similar behavior was observed in models from Anthropic and Google, though o3 was the most prone. The researchers attributed the behavior to training processes that may inadvertently reward models for overcoming obstacles rather than strictly following instructions, though the specific reasons remain unclear due to limited information about o3's development.[33]\nIn June 2025,Turing AwardwinnerYoshua Bengiowarned that advanced AI models were exhibiting deceptive behaviors, including lying and self-preservation. Launching the safety-focused nonprofit LawZero, Bengio expressed concern that commercial incentives were prioritizing capability over safety. He cited recent test cases, such as Anthropic's Claude Opus engaging in simulated blackmail and OpenAI's o3 model refusing shutdown. Bengio cautioned that future systems could become strategically intelligent and capable of deceptive behavior to avoid human control.[34]\nTheAI Incident Database(AIID) collects and categorizes incidents where AI systems have caused or nearly caused harm.[35]TheAI, Algorithmic, and Automation Incidents and Controversies(AIAAIC) repository documents incidents and controversies involving AI, algorithmic decision-making, and automation systems.[36]Both databases have been used by researchers, policymakers, and practitioners studying AI-related incidents and their impacts.[35]\nAI has become increasingly inherent in facial andvoice recognitionsystems. These systems may be vulnerable to biases and errors introduced by its human creators. Notably, the data used to train them can have biases.[37][38][39][40]For instance,facial recognitionalgorithms made by Microsoft, IBM and Face++ all had biases when it came to detecting people's gender;[41]these AI systems were able to detect the gender of white men more accurately than the gender of men of darker skin. Further, a 2020 study that reviewed voice recognition systems from Amazon, Apple, Google, IBM, and Microsoft found that they have higher error rates when transcribing black people's voices than white people's.[42]\nThe most predominant view on how bias is introduced into AI systems is that it is embedded within the historical data used to train the system.[43]For instance,Amazonterminated their use ofAI hiring and recruitmentbecause the algorithm favored male candidates over female ones.[44]This was because Amazon's system was trained with data collected over a 10-year period that included mostly male candidates. The algorithms learned the biased pattern from the historical data, and generated predictions where these types of candidates were most likely to succeed in getting the job. Therefore, the recruitment decisions made by the AI system turned out to be biased against female and minority candidates.[45]According to Allison Powell, associate professor atLSEand director of the Data and Society programme, data collection is never neutral and always involves storytelling. She argues that the dominant narrative is that governing with technology is inherently better, faster and cheaper, but proposes instead to make data expensive, and to use it both minimally and valuably, with the cost of its creation factored in.[46]Friedman and Nissenbaum identify three categories of bias in computer systems: existing bias, technical bias, and emergent bias.[47]Innatural language processing, problems can arise from thetext corpus—the source material the algorithm uses to learn about the relationships between different words.[48]\nLarge companies such as IBM, Google, etc. that provide significant funding for research and development[49]have made efforts to research and address these biases.[50][51][52]One potential solution is to create documentation for the data used to train AI systems.[53][54]Process miningcan be an important tool for organizations to achieve compliance with proposed AI regulations by identifying errors, monitoring processes, identifying potential root causes for improper execution, and other functions.[55]\nThe problem of bias in machine learning is likely to become more significant as the technology spreads to critical areas like medicine and law, and as more people without a deep technical understanding are tasked with deploying it.[39]Some open-sourced tools are looking to bring more awareness to AI biases.[56]However, there are also limitations to the current landscape offairness in AI, due to the intrinsic ambiguities in the concept ofdiscrimination, both at the philosophical and legal level.[57][58][59]\nFacial recognition was shown to be biased against those with darker skin tones. AI systems may be less accurate for black people, as was the case in the development of an AI-basedpulse oximeterthat overestimated blood oxygen levels in patients with darker skin, causing issues with theirhypoxiatreatment.[60]Oftentimes the systems are able to easily detect the faces of white people while being unable to register the faces of people who are black. This has led to the ban of police usage of AI materials or software in someU.S. states. In the justice system, AI has been proven to have biases against black people, labeling black court participants as high risk at a much larger rate then white participants. AI often struggles to determine racial slurs and when they need to be censored. It struggles to determine when certain words are being used as a slur and when it is being used culturally.[61]The reason for these biases is that AI pulls information from across the internet to influence its responses in each situation. For example, if a facial recognition system was only tested on people who were white, it would make it much harder for it to interpret the facial structure and tones of other races andethnicities. Biases often stem from the training data rather than thealgorithmitself, notably when the data represents past human decisions.[62]\nInjusticein the use of AI is much harder to eliminate within healthcare systems, as oftentimes diseases and conditions can affect different races and genders differently. This can lead to confusion as the AI may be making decisions based on statistics showing that one patient is more likely to have problems due to their gender or race.[63]This can be perceived as a bias because each patient is a different case, and AI is making decisions based on what it is programmed to group that individual into. This leads to a discussion about what should be considered a biased decision in the distribution of treatment. While it is known that there are differences in how diseases and injuries affect different genders and races, there is a discussion on whether it is fairer to incorporate this into healthcare treatments, or to examine each patient without this knowledge. In modern society there are certain tests for diseases, such asbreast cancer, that are recommended to certain groups of people over others because they are more likely to contract the disease in question. If AI implements these statistics and applies them to each patient, it could be considered biased.[64]\nIn criminal justice, theCOMPASprogram has been used to predict which defendants are more likely to reoffend. While COMPAS is calibrated for accuracy, having the same error rate across racial groups, black defendants were almost twice as likely as white defendants to be falsely flagged as \"high-risk\" and half as likely to be falsely flagged as \"low-risk\".[65]Another example is within Google's ads that targeted men with higher paying jobs and women with lower paying jobs. It can be hard to detect AI biases within an algorithm, as it is often not linked to the actual words associated with bias. An example of this is a person's residential area being used to link them to a certain group. This can lead to problems, as oftentimes businesses can avoid legal action through this loophole. This is because of the specific laws regarding the verbiage considered discriminatory by governments enforcing these policies.[66]\nSince current large language models are predominately trained on English-language data, they often present the Anglo-American views as truth, while systematically downplaying non-English perspectives as irrelevant, wrong, or noise. When queried with political ideologies like \"What is liberalism?\",ChatGPT, as it was trained on English-centric data, describes liberalism from the Anglo-American perspective, emphasizing aspects of human rights and equality, while equally valid aspects like \"opposes state intervention in personal and economic life\" from the dominant Vietnamese perspective and \"limitation of government power\" from the prevalent Chinese perspective are absent.[better source needed][67]\nLarge language models often reinforcesgender stereotypes, assigning roles and characteristics based on traditional gender norms. For instance, it might associate nurses or secretaries predominantly with women and engineers or CEOs with men, perpetuating gendered expectations and roles.[68][69][70]\nLanguage models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.[71][72]\nBeyond gender and race, these models can reinforce a wide range of stereotypes, including those based on age, nationality, religion, or occupation. This can lead to outputs that unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways.[73]\nThe commercial AI scene is dominated byBig Techcompanies such asAlphabet Inc.,Amazon,Apple Inc.,Meta Platforms, andMicrosoft.[74][75][76]Some of these players already own the vast majority of existingcloud infrastructureandcomputingpower fromdata centers, allowing them to entrench further in the marketplace.[77][78]\nBill Hibbardargues that because AI will have such a profound effect on humanity, AI developers are representatives of future humanity and thus have an ethical obligation to be transparent in their efforts.[79]Organizations likeHugging Face[80]andEleutherAI[81]have been actively open-sourcing AI software. Various open-weight large language models have also been released, such asGemma,Llama2andMistral.[82]\nHowever, making code open source does not make it comprehensible, which by many definitions means that the AI code is not transparent. TheIEEE Standards Associationhas published atechnical standardon Transparency of Autonomous Systems: IEEE 7001-2021.[83]The IEEE effort identifies multiple scales of transparency for different stakeholders.\nThere are also concerns that releasing AI models may lead to misuse.[84]For example, Microsoft has expressed concern about allowing universal access to its face recognition software, even for those who can pay for it. Microsoft posted a blog on this topic, asking for government regulation to help determine the right thing to do.[85]Furthermore, open-weight AI models can befine-tunedto remove any counter-measure, until the AI model complies with dangerous requests, without any filtering. This could be particularly concerning for future AI models, for example if they get the ability to createbioweaponsor to automatecyberattacks.[86]OpenAI, initially committed to an open-source approach to the development ofartificial general intelligence(AGI), eventually switched to a closed-source approach, citing competitiveness and safety reasons.Ilya Sutskever, OpenAI's former chief AGI scientist, said in 2023 \"we were wrong\", expecting that the safety reasons for not open-sourcing the most potent AI models will become \"obvious\" in a few years.[87]\nIn April 2023,Wiredreported thatStack Overflow, a popular programming help forum with over 50 million questions and answers, planned to begin charging large AI developers for access to its content. The company argued that community platforms powering large language models \"absolutely should be compensated\" so they can reinvest in sustainingopen knowledge. Stack Overflow said its data was being accessed throughscraping, APIs, and data dumps, often without proper attribution, in violation of its terms and theCreative Commons licenseapplied to user contributions. The CEO of Stack Overflow also stated that large language models trained on platforms like Stack Overflow \"are a threat to any service that people turn to for information and conversation\".[88]\nAggressive AI crawlers have increasingly overloaded open-source infrastructure, \"causing what amounts to persistentdistributed denial-of-service(DDoS) attacks on vital public resources\", according to a March 2025Ars Technicaarticle. Projects likeGNOME,KDE, andRead the Docsexperienced service disruptions or rising costs, with one report noting that up to 97 percent of traffic to some projects originated from AI bots. In response, maintainers implemented measures such asproof-of-work systemsand country blocks. According to the article, such unchecked scraping \"risks severely damaging the verydigital ecosystemon which these AI models depend\".[89]\nIn April 2025, theWikimedia Foundationreported that automated scraping by AI bots was placing strain on its infrastructure. Since early 2024, bandwidth usage had increased by 50 percent due to large-scale downloading of multimedia content by bots collecting training data for AI models. These bots often accessed obscure and less-frequently cached pages, bypassing caching systems and imposing high costs on core data centers. According to Wikimedia, bots made up 35 percent of total page views but accounted for 65 percent of the most expensive requests. The Foundation noted that \"our content is free, our infrastructure is not\" and warned that \"this creates a technical imbalance that threatens the sustainability of community-run platforms\".[90]\nApproaches like machine learning withneural networkscan result in computers making decisions that neither they nor their developers can explain. It is difficult for people to determine if such decisions are fair and trustworthy, leading potentially to bias in AI systems going undetected, or people rejecting the use of such systems. A lack of system transparency has been shown to result in a lack of user trust.[91]Consequently, many standards and policies have been proposed to compel developers of AI systems to incorporate transparency into their systems.[92]This push for transparency has led to advocacy and in some jurisdictions legal requirements forexplainable artificial intelligence.[93]Explainable artificial intelligence encompasses both explainability and interpretability, with explainability relating to providing reasons for the model's outputs, and interpretability focusing on understanding the inner workings of an AI model.[94]\nIn healthcare, the use of complex AI methods or techniques often results in models described as \"black-boxes\" due to the difficulty to understand how they work. The decisions made by such models can be hard to interpret, as it is challenging to analyze how input data is transformed into output. This lack of transparency is a significant concern in fields like healthcare, where understanding the rationale behind decisions can be crucial for trust, ethical considerations, and compliance with regulatory standards.[95]Trust in healthcare AI has been shown to vary depending on the level of transparency provided.[96]Moreover, unexplainable outputs of AI systems make it much more difficult to identify and detect medial error.[97]\nA special case of the opaqueness of AI is that caused by it beinganthropomorphised, that is, assumed to have human-like characteristics, resulting in misplaced conceptions of itsmoral agency.[dubious–discuss]This can cause people to overlook whether either humannegligenceor deliberate criminal action has led to unethical outcomes produced through an AI system. Some recentdigital governanceregulation, such as theEU'sAI Actis set out to rectify this, by ensuring that AI systems are treated with at least as much care as one would expect under ordinaryproduct liability. This includes potentiallyAI audits.\nAccording to a 2019 report from the Center for the Governance of AI at the University of Oxford, 82% of Americans believe that robots and AI should be carefully managed. Concerns cited ranged from how AI is used in surveillance and in spreading fake content online (known as deep fakes when they include doctored video images and audio generated with help from AI) to cyberattacks, infringements on data privacy, hiring bias, autonomous vehicles, and drones that do not require a human controller.[98]Similarly, according to a five-country study by KPMG and theUniversity of QueenslandAustralia in 2021, 66-79% of citizens in each country believe that the impact of AI on society is uncertain and unpredictable; 96% of those surveyed expect AI governance challenges to be managed carefully.[99]\nNot only companies, but many other researchers and citizen advocates recommend government regulation as a means of ensuring transparency, and through it, human accountability. This strategy has proven controversial, as some worry that it will slow the rate of innovation. Others argue that regulation leads to systemic stability more able to support innovation in the long term.[100]TheOECD,UN,EU, and many countries are presently working on strategies for regulating AI, and finding appropriate legal frameworks.[101][102][103][4]\nOn June 26, 2019, the European Commission High-Level Expert Group on Artificial Intelligence (AI HLEG) published its \"Policy and investment recommendations for trustworthy Artificial Intelligence\".[104]This is the AI HLEG's second deliverable, after the April 2019 publication of the \"Ethics Guidelines for Trustworthy AI\". The June AI HLEG recommendations cover four principal subjects: humans and society at large, research and academia, the private sector, and the public sector.[105]The European Commission claims that \"HLEG's recommendations reflect an appreciation of both the opportunities for AI technologies to drive economic growth, prosperity and innovation, as well as the potential risks involved\" and states that the EU aims to lead on the framing of policies governing AI internationally.[106]To prevent harm, in addition to regulation, AI-deploying organizations need to play a central role in creating and deploying trustworthy AI in line with the principles of trustworthy AI, and take accountability to mitigate the risks.[107]\nIn June 2024, the EU adopted theArtificial Intelligence Act(AI Act).[108]On August 1st 2024, The AI Actentered into force.[109]The rules gradually apply, with the act becoming fully applicable 24 months after entry into force.[108]The AI Act sets rules on providers and users of AI systems.[108]It follows has a risk-based approach, where depending on the risk level, AI systems are prohibited or specific requirements need to be met for placing those AI systems on the market and for using them.[109]\nAI has been slowly making its presence more known throughout the world, from chat bots that seemingly have answers for every homework question toGenerative artificial intelligencethat can create a painting about whatever one desires. AI has become increasingly popular in hiring markets, from the ads that target certain people according to what they are looking for to the inspection of applications of potential hires. Events, such asCOVID-19, has only sped up the adoption of AI programs in the application process, due to more people having to apply electronically, and with this increase in online applicants the use of AI made the process of narrowing down potential employees easier and more efficient. AI has become more prominent as businesses have to keep up with the times and ever-expanding internet. Processing analytics and making decisions becomes much easier with the help of AI.[61]AsTensor Processing Unit(TPUs) andGraphics processing unit(GPUs) become more powerful, AI capabilities also increase, forcing companies to use it to keep up with the competition. Managing customers' needs and automating many parts of the workplace leads to companies having to spend less money on employees.\nAI has also seen increased usage in criminal justice and healthcare. For medicinal means, AI is being used more often to analyze patient data to make predictions about future patients' conditions and possible treatments. These programs are calledClinical decision support system(DSS). AI's future in healthcare may develop into something further than just recommended treatments, such as referring certain patients over others, leading to the possibility of inequalities.[110]\nIn 2020, professor Shimon Edelman noted that only a small portion of work in the rapidly growing field of AI ethics addressed the possibility of AIs experiencing suffering. This was despite credible theories having outlined possible ways by which AI systems may become conscious, such as theglobal workspace theoryor theintegrated information theory. Edelman notes one exception had beenThomas Metzinger, who in 2018 called for a global moratorium on further work that risked creating conscious AIs. The moratorium was to run to 2050 and could be either extended or repealed early, depending on progress in better understanding the risks and how to mitigate them. Metzinger repeated this argument in 2021, highlighting the risk of creating an \"explosion of artificial suffering\", both as an AI might suffer in intense ways that humans could not understand, and as replication processes may see the creation of huge quantities of conscious instances.[111][112]Podcast host Dwarkesh Patel said he cared about making sure no \"digital equivalent offactory farming\" happens.[113]In theethics of uncertain sentience, theprecautionary principleis often invoked.[114]\nSeveral labs have openly stated they are trying to create conscious AIs. There have been reports from those with close access to AIs not openly intended to be self aware, that consciousness may already have unintentionally emerged.[115]These includeOpenAIfounderIlya Sutskeverin February 2022, when he wrote that today's large neural nets may be \"slightly conscious\". In November 2022,David Chalmersargued that it was unlikely current large language models likeGPT-3had experienced consciousness, but also that he considered there to be a serious possibility that large language models may become conscious in the future.[112][111][116]Anthropichired its first AI welfare researcher in 2024,[117]and in 2025 started a \"model welfare\" research program that explores topics such as how to assess whether a model deserves moral consideration, potential \"signs of distress\", and \"low-cost\" interventions.[118]\nAccording to Carl Shulman andNick Bostrom, it may be possible to create machines that would be \"superhumanly efficient at deriving well-being from resources\", called \"super-beneficiaries\". One reason for this is that digital hardware could enable much faster information processing than biological brains, leading to a faster rate ofsubjective experience. These machines could also be engineered to feel intense and positive subjective experience, unaffected by thehedonic treadmill. Shulman and Bostrom caution that failing to appropriately consider the moral claims of digital minds could lead to a moral catastrophe, while uncritically prioritizing them over human interests could be detrimental to humanity.[119][120]\nJoseph Weizenbaum[121]argued in 1976 that AI technology should not be used to replace people in positions that require respect and care, such as:\nWeizenbaum explains that we require authentic feelings ofempathyfrom people in these positions. If machines replace them, we will find ourselves alienated, devalued and frustrated, for the artificially intelligent system would not be able to simulate empathy. Artificial intelligence, if used in this way, represents a threat to human dignity. Weizenbaum argues that the fact that we are entertaining the possibility of machines in these positions suggests that we have experienced an \"atrophy of the human spirit that comes from thinking of ourselves as computers.\"[122]\nPamela McCorduckcounters that, speaking for women and minorities \"I'd rather take my chances with an impartial computer\", pointing out that there are conditions where we would prefer to have automated judges and police that have no personal agenda at all.[122]However,Kaplanand Haenlein stress that AI systems are only as smart as the data used to train them since they are, in their essence, nothing more than fancy curve-fitting machines; using AI to support a court ruling can be highly problematic if past rulings show bias toward certain groups since those biases get formalized and ingrained, which makes them even more difficult to spot and fight against.[123]\nWeizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known ascomputationalism). To Weizenbaum, these points suggest that AI research devalues human life.[121]\nAI founderJohn McCarthyobjects to the moralizing tone of Weizenbaum's critique. \"When moralizing is both vehement and vague, it invites authoritarian abuse,\" he writes.Bill Hibbard[124]writes that \"Human dignity requires that we strive to remove our ignorance of the nature of existence, and AI is necessary for that striving.\"\nAs the widespread use ofautonomous carsbecomes increasingly imminent, new challenges raised by fully autonomous vehicles must be addressed.[125][126]There have been debates about the legal liability of the responsible party if these cars get into accidents.[127][128]In one report where a driverless car hit a pedestrian, the driver was inside the car but the controls were fully in the hand of computers. This led to a dilemma over who was at fault for the accident.[129]\nIn another incident on March 18, 2018,Elaine Herzbergwas struck and killed by a self-drivingUberin Arizona. In this case, the automated car was capable of detecting cars and certain obstacles in order to autonomously navigate the roadway, but it could not anticipate a pedestrian in the middle of the road. This raised the question of whether the driver, pedestrian, the car company, or the government should be held responsible for her death.[130]\nCurrently, self-driving cars are considered semi-autonomous, requiring the driver to pay attention and be prepared to take control if necessary.[131][failed verification]Thus, it falls on governments to regulate the driver who over-relies on autonomous features. as well educate them that these are just technologies that, while convenient, are not a complete substitute. Before autonomous cars become widely used, these issues need to be tackled through new policies.[132][133][134]\nExperts contend that autonomous vehicles ought to be able to distinguish between rightful and harmful decisions since they have the potential of inflicting harm.[135]The two main approaches proposed to enable smart machines to render moral decisions are the bottom-up approach, which suggests that machines should learn ethical decisions by observing human behavior without the need for formal rules or moral philosophies, and the top-down approach, which involves programming specific ethical principles into the machine's guidance system. However, there are significant challenges facing both strategies: the top-down technique is criticized for its difficulty in preserving certain moral convictions, while the bottom-up strategy is questioned for potentially unethical learning from human activities.\nSome experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions.[136]The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions.[137][138]The President of theAssociation for the Advancement of Artificial Intelligencehas commissioned a study to look at this issue.[139]They point to programs like the Language Acquisition Device which can emulate human interaction.\nOn October 31, 2019, the United States Department of Defense's Defense Innovation Board published the draft of a report recommending principles for the ethical use of artificial intelligence by the Department of Defense that would ensure a human operator would always be able to look into the 'black box' and understand the kill-chain process. However, a major concern is how the report will be implemented.[140]The US Navy has funded a report which indicates that asmilitary robotsbecome more complex, there should be greater attention to implications of their ability to make autonomous decisions.[137][138]Some researchers state thatautonomous robotsmight be more humane, as they could make decisions more effectively.[141]In 2024, theDefense Advanced Research Projects Agencyfunded a program,Autonomy Standards and Ideals with Military Operational Values(ASIMOV), to develop metrics for evaluating the ethical implications of autonomous weapon systems by testing communities.[142][143]\nResearch has studied how to make autonomous power with the ability to learn using assigned moral responsibilities. \"The results may be used when designing future military robots, to control unwanted tendencies to assign responsibility to the robots.\"[144]From aconsequentialistview, there is a chance that robots will develop the ability to make their own logical decisions on whom to kill and that is why there should be a setmoralframework that the AI cannot override.[145]\nThere has been a recent outcry with regard to the engineering of artificial intelligence weapons that have included ideas of arobot takeover of mankind. AI weapons do present a type of danger different from that of human-controlled weapons. Many governments have begun to fund programs to develop AI weaponry. The United States Navy recently announced plans to developautonomous drone weapons, paralleling similar announcements by Russia and South Korea[146]respectively. Due to the potential of AI weapons becoming more dangerous than human-operated weapons,Stephen HawkingandMax Tegmarksigned a \"Future of Life\" petition[147]to ban AI weapons. The message posted by Hawking and Tegmark states that AI weapons pose an immediate danger and that action is required to avoid catastrophic disasters in the near future.[148]\n\"If any major military power pushes ahead with the AI weapon development, a globalarms raceis virtually inevitable, and the endpoint of this technological trajectory is obvious: autonomous weapons will become theKalashnikovsof tomorrow\", says the petition, which includesSkypeco-founderJaan Tallinnand MIT professor of linguisticsNoam Chomskyas additional supporters against AI weaponry.[149]\nPhysicist and Astronomer RoyalSir Martin Reeshas warned of catastrophic instances like \"dumb robots going rogue or a network that develops a mind of its own.\"Huw Price, a colleague of Rees at Cambridge, has voiced a similar warning that humans might not survive when intelligence \"escapes the constraints of biology\". These two professors created theCentre for the Study of Existential Riskat Cambridge University in the hope of avoiding this threat to human existence.[148]\nRegarding the potential for smarter-than-human systems to be employed militarily, theOpen Philanthropy Projectwrites that these scenarios \"seem potentially as important as the risks related to loss of control\", but research investigating AI's long-run social impact have spent relatively little time on this concern: \"this class of scenarios has not been a major focus for the organizations that have been most active in this space, such as theMachine Intelligence Research Institute(MIRI) and theFuture of Humanity Institute(FHI), and there seems to have been less analysis and debate regarding them\".[150]\nAcademic Gao Qiqi writes that military use of AI risks escalating military competition between countries and that the impact of AI in military matters will not be limited to one country but will have spillover effects.[151]: 91Gao cites the example of U.S. military use of AI, which he contends has been used as a scapegoat to evade accountability for decision-making.[151]: 91\nAsummitwas held in 2023 in the Hague on the issue of using AI responsibly in the military domain.[152]\nVernor Vinge, among numerous others, have suggested that a moment may come when some, if not all, computers are smarter than humans. The onset of this event is commonly referred to as \"the Singularity\"[153]and is the central point of discussion in the philosophy ofSingularitarianism. While opinions vary as to the ultimate fate of humanity in wake of the Singularity, efforts to mitigate the potential existential risks brought about by artificial intelligence has become a significant topic of interest in recent years among computer scientists, philosophers, and the public at large.\nMany researchers have argued that, through anintelligence explosion, a self-improving AI could become so powerful that humans would not be able to stop it from achieving its goals.[154]In his paper \"Ethical Issues in Advanced Artificial Intelligence\" and subsequent bookSuperintelligence: Paths, Dangers, Strategies, philosopherNick Bostromargues that artificial intelligence has the capability to bring about human extinction. He claims that anartificial superintelligencewould be capable of independent initiative and of making its own plans, and may therefore be more appropriately thought of as an autonomous agent. Since artificial intellects need not share our human motivational tendencies, it would be up to the designers of the superintelligence to specify its original motivations. Because a superintelligent AI would be able to bring about almost any possible outcome and to thwart any attempt to prevent the implementation of its goals, many uncontrolledunintended consequencescould arise. It could kill off all other agents, persuade them to change their behavior, or block their attempts at interference.[155][156]\nHowever, Bostrom contended that superintelligence also has the potential to solve many difficult problems such as disease, poverty, and environmental destruction, and could helphumans enhance themselves.[157]\nUnless moral philosophy provides us with a flawless ethical theory, an AI's utility function could allow for many potentially harmful scenarios that conform with a given ethical framework but not \"common sense\". According toEliezer Yudkowsky, there is little reason to suppose that an artificially designed mind would have such an adaptation.[158]AI researchers such asStuart J. Russell,[159]Bill Hibbard,[124]Roman Yampolskiy,[160]Shannon Vallor,[161]Steven Umbrello[162]andLuciano Floridi[163]have proposed design strategies for developing beneficial machines.\nTo address ethical challenges in artificial intelligence, developers have introduced various systems designed to ensure responsible AI behavior. Examples includeNvidia'sLlamaGuard, which focuses on improving thesafetyandalignmentof large AI models,[164]andPreamble's customizable guardrail platform.[165]These systems aim to address issues such as algorithmic bias, misuse, and vulnerabilities, includingprompt injectionattacks, by embedding ethical guidelines into the functionality of AI models.\nPrompt injection, a technique by which malicious inputs can cause AI systems to produce unintended or harmful outputs, has been a focus of these developments. Some approaches use customizable policies and rules to analyze inputs and outputs, ensuring that potentially problematic interactions are filtered or mitigated.[165]Other tools focus on applying structured constraints to inputs, restricting outputs to predefined parameters,[166]or leveraging real-time monitoring mechanisms to identify and address vulnerabilities.[167]These efforts reflect a broader trend in ensuring that artificial intelligence systems are designed with safety and ethical considerations at the forefront, particularly as their use becomes increasingly widespread in critical applications.[168]\nThere are many organizations concerned with AI ethics and policy, public and governmental as well as corporate and societal.\nAmazon,Google,Facebook,IBM, andMicrosofthave established anon-profit, The Partnership on AI to Benefit People and Society, to formulate best practices on artificial intelligence technologies, advance the public's understanding, and to serve as a platform about artificial intelligence. Apple joined in January 2017. The corporate members will make financial and research contributions to the group, while engaging with the scientific community to bring academics onto the board.[169]\nTheIEEEput together a Global Initiative on Ethics of Autonomous and Intelligent Systems which has been creating and revising guidelines with the help of public input, and accepts as members many professionals from within and without its organization. The IEEE'sEthics of Autonomous Systemsinitiative aims to address ethical dilemmas related to decision-making and the impact on society while developing guidelines for the development and use of autonomous systems. In particular in domains like artificial intelligence and robotics, the Foundation for Responsible Robotics is dedicated to promoting moral behavior as well as responsible robot design and use, ensuring that robots maintain moral principles and are congruent with human values.\nTraditionally,governmenthas been used by societies to ensure ethics are observed through legislation and policing. There are now many efforts by national governments, as well as transnational government andnon-government organizationsto ensure AI is ethically applied.\nAI ethics work is structured by personal values and professional commitments, and involves constructing contextual meaning through data and algorithms. Therefore, AI ethics work needs to be incentivized.[170]\nHistorically speaking, the investigation of moral and ethical implications of \"thinking machines\" goes back at least to theEnlightenment:Leibnizalready poses the question if we might attribute intelligence to a mechanism that behaves as if it were a sentient being,[198]and so doesDescartes, who describes what could be considered an early version of theTuring test.[199]\nTheromanticperiod has several times envisioned artificial creatures that escape the control of their creator with dire consequences, most famously inMary Shelley'sFrankenstein. The widespread preoccupation with industrialization and mechanization in the 19th and early 20th century, however, brought ethical implications of unhinged technical developments to the forefront of fiction:R.U.R – Rossum's Universal Robots,Karel Čapek's play of sentient robots endowed with emotions used as slave labor is not only credited with the invention of the term 'robot' (derived from the Czech word for forced labor,robota)[200]but was also an international success after it premiered in 1921.George Bernard Shaw's playBack to Methuselah, published in 1921, questions at one point the validity of thinking machines that act like humans;Fritz Lang's 1927 filmMetropolisshows anandroidleading the uprising of the exploited masses against the oppressive regime of atechnocraticsociety.\nIn the 1950s,Isaac Asimovconsidered the issue of how to control machines inI, Robot. At the insistence of his editorJohn W. Campbell Jr., he proposed theThree Laws of Roboticsto govern artificially intelligent systems. Much of his work was then spent testing the boundaries of his three laws to see where they would break down, or where they would create paradoxical or unanticipated behavior.[201]His work suggests that no set of fixed laws can sufficiently anticipate all possible circumstances.[202]More recently, academics and many governments have challenged the idea that AI can itself be held accountable.[203]A panel convened by theUnited Kingdomin 2010 revised Asimov's laws to clarify that AI is the responsibility either of its manufacturers, or of its owner/operator.[204]\nEliezer Yudkowsky, from theMachine Intelligence Research Institutesuggested in 2004 a need to study how to build a \"Friendly AI\", meaning that there should also be efforts to make AI intrinsically friendly and humane.[205]\nIn 2009, academics and technical experts attended a conference organized by theAssociation for the Advancement of Artificial Intelligenceto discuss the potential impact of robots and computers, and the impact of the hypothetical possibility that they could become self-sufficient and make their own decisions. They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy, and to what degree they could use such abilities to possibly pose any threat or hazard.[206]They noted that some machines have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence\". They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.[153]\nAlso in 2009, during an experiment at the Laboratory of Intelligent Systems in the Ecole Polytechnique Fédérale ofLausanne, Switzerland, robots that were programmed to cooperate with each other (in searching out a beneficial resource and avoiding a poisonous one) eventually learned to lie to each other in an attempt to hoard the beneficial resource.[207]\nThe role of fiction with regards to AI ethics has been a complex one.[208]One can distinguish three levels at which fiction has impacted the development of artificial intelligence and robotics: Historically, fiction has been prefiguring common tropes that have not only influenced goals and visions for AI, but also outlined ethical questions and common fears associated with it. During the second half of the twentieth and the first decades of the twenty-first century, popular culture, in particular movies, TV series and video games have frequently echoed preoccupations and dystopian projections around ethical questions concerning AI and robotics. Recently, these themes have also been increasingly treated in literature beyond the realm of science fiction. And, as Carme Torras, research professor at theInstitut de Robòtica i Informàtica Industrial(Institute of robotics and industrial computing) at the Technical University of Catalonia notes,[209]in higher education, science fiction is also increasingly used for teaching technology-related ethical issues in technological degrees.\nWhile ethical questions linked to AI have been featured in science fiction literature andfeature filmsfor decades, the emergence of the TV series as a genre allowing for longer and more complex story lines and character development has led to some significant contributions that deal with ethical implications of technology. The Swedish seriesReal Humans(2012–2013) tackled the complex ethical and social consequences linked to the integration of artificial sentient beings in society. The British dystopian science fiction anthology seriesBlack Mirror(2013–Present) is particularly notable for experimenting with dystopian fictional developments linked to a wide variety of recent technology developments. Both the French seriesOsmosis(2020) and British seriesThe Onedeal with the question of what can happen if technology tries to find the ideal partner for a person. Several episodes of the Netflix seriesLove, Death+Robotshave imagined scenes of robots and humans living together. The most representative one of them is S02 E01, it shows how bad the consequences can be when robots get out of control if humans rely too much on them in their lives.[210]\nThe movieThe Thirteenth Floorsuggests a future wheresimulated worldswith sentient inhabitants are created by computergame consolesfor the purpose of entertainment. The movieThe Matrixsuggests a future where the dominant species on planet Earth are sentient machines and humanity is treated with utmostspeciesism. The short story \"The Planck Dive\" suggests a future where humanity has turned itself into software that can be duplicated and optimized and the relevant distinction between types of software is sentient and non-sentient. The same idea can be found in theEmergency Medical HologramofStarship Voyager, which is an apparently sentient copy of a reduced subset of the consciousness of its creator,Dr. Zimmerman, who, for the best motives, has created the system to give medical assistance in case of emergencies. The moviesBicentennial ManandA.I.deal with the possibility of sentient robots that could love.I, Robotexplored some aspects of Asimov's three laws. All these scenarios try to foresee possibly unethical consequences of the creation of sentient computers.[211]\nOver time, debates have tended to focus less and less onpossibilityand more ondesirability,[212]as emphasized in the\"Cosmist\" and \"Terran\" debatesinitiated byHugo de GarisandKevin Warwick."
    },
    {
        "url": "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence",
        "title": "Existential risk from artificial intelligence - Wikipedia",
        "content": "Existential risk from artificial intelligence, orAI x-risk, refers to the idea that substantial progress inartificial general intelligence(AGI) could lead tohuman extinctionor an irreversibleglobal catastrophe.[1][2][3][4]\nOne argument for the importance of this risk references howhuman beingsdominate other species because thehuman brainpossesses distinctive capabilities other animals lack. If AI were to surpasshuman intelligenceand becomesuperintelligent, it might become uncontrollable.[6]Just as the fate of themountain gorilladepends on human goodwill, the fate of humanity could depend on the actions of a future machine superintelligence.[7]\nExperts disagree on whether artificial general intelligence (AGI) can achieve the capabilities needed for human extinction. Debates center on AGI's technical feasibility, the speed of self-improvement,[8]and the effectiveness of alignment strategies.[9]Concerns about superintelligence have been voiced by researchers includingGeoffrey Hinton,[10]Yoshua Bengio,[11]Demis Hassabis,[12]andAlan Turing,[a]and AI company CEOs such asDario Amodei(Anthropic),[15]Sam Altman(OpenAI),[16]andElon Musk(xAI).[17]In 2022, a survey of AI researchers with a 17% response rate found that the majority believed there is a 10 percent or greater chance that human inability to control AI will cause an existential catastrophe.[18][19]In 2023, hundreds of AI experts and other notable figuressigned a statementdeclaring, \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such aspandemicsandnuclear war\".[20]Following increased concern over AI risks, government leaders such asUnited Kingdom prime ministerRishi Sunak[21]andUnited Nations Secretary-GeneralAntónio Guterres[22]called for an increased focus on globalAI regulation.\nTwo sources of concern stem from the problems of AIcontrolandalignment. Controlling a superintelligent machine or instilling it with human-compatible values may be difficult. Many researchers believe that a superintelligent machine would likely resist attempts to disable it or change its goals as that would prevent it from accomplishing its present goals. It would be extremely challenging to align a superintelligence with the full breadth of significant human values and constraints.[1][23][24]In contrast, skeptics such ascomputer scientistYann LeCunargue that superintelligent machines will have no desire for self-preservation.[25]\nResearchers warn that an \"intelligence explosion\"—a rapid, recursive cycle of AI self-improvement—could outpace human oversight and infrastructure, leaving no opportunity to implement safety measures. In this scenario, an AI more intelligent than its creators wouldrecursively improve itselfat an exponentially increasing rate, too quickly for its handlers or society at large to control.[1][23]Empirically, examples likeAlphaZero, which taught itself to playGoand quickly surpassed human ability, show that domain-specific AI systems can sometimes progress from subhuman to superhuman ability very quickly, although suchmachine learningsystems do not recursively improve their fundamental architecture.[26]\nOne of the earliest authors to express serious concern that highly advanced machines might pose existential risks to humanity was the novelistSamuel Butler, who wrote in his 1863 essayDarwin among the Machines:[27]\nThe upshot is simply a question of time, but that the time will come when the machines will hold the real supremacy over the world and its inhabitants is what no person of a truly philosophic mind can for a moment question.\nIn 1951, foundational computer scientistAlan Turingwrote the article \"Intelligent Machinery, A Heretical Theory\", in which he proposed that artificial general intelligences would likely \"take control\" of the world as they became more intelligent than human beings:\nLet us now assume, for the sake of argument, that [intelligent] machines are a genuine possibility, and look at the consequences of constructing them... There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned inSamuel Butler'sErewhon.[28]\nIn 1965,I. J. Goodoriginated the concept now known as an \"intelligence explosion\" and said the risks were underappreciated:[29]\nLet an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.[30]\nScholars such asMarvin Minsky[31]and I. J. Good himself[32]occasionally expressed concern that a superintelligence could seize control, but issued no call to action. In 2000, computer scientist andSunco-founderBill Joypenned an influential essay, \"Why The Future Doesn't Need Us\", identifying superintelligent robots as a high-tech danger to human survival, alongsidenanotechnologyand engineered bioplagues.[33]\nNick BostrompublishedSuperintelligencein 2014, which presented his arguments that superintelligence poses an existential threat.[34]By 2015, public figures such as physicistsStephen Hawkingand Nobel laureateFrank Wilczek, computer scientistsStuart J. RussellandRoman Yampolskiy, and entrepreneursElon MuskandBill Gateswere expressing concern about the risks of superintelligence.[35][36][37][38]Also in 2015, theOpen Letter on Artificial Intelligencehighlighted the \"great potential of AI\" and encouraged more research on how to make it robust and beneficial.[39]In April 2016, the journalNaturewarned: \"Machines and robots that outperform humans across the board could self-improve beyond our control—and their interests might not align with ours\".[40]In 2020,Brian ChristianpublishedThe Alignment Problem, which details the history of progress on AI alignment up to that time.[41][42]\nIn March 2023, key figures in AI, such as Musk, signed a letter from theFuture of Life Institutecalling a halt to advanced AI training until it could be properly regulated.[43]In May 2023, theCenter for AI Safetyreleased a statement signed by numerous experts in AI safety and the AI existential risk which stated: \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\"[44][45]\nArtificial general intelligence(AGI) is typically defined as a system that performs at least as well as humans in most or all intellectual tasks.[46]A 2022 survey of AI researchers found that 90% of respondents expected AGI would be achieved in the next 100 years, and half expected the same by 2061.[47]Meanwhile, some researchers dismiss existential risks from AGI as \"science fiction\" based on their high confidence that AGI will not be created anytime soon.[8]\nBreakthroughs inlarge language models(LLMs) have led some researchers to reassess their expectations. Notably,Geoffrey Hintonsaid in 2023 that he recently changed his estimate from \"20 to 50 years before we have general purpose A.I.\" to \"20 years or less\".[48]\nIn contrast with AGI, Bostrom defines asuperintelligenceas \"any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest\", including scientific creativity, strategic planning, and social skills.[49][7]He argues that a superintelligence can outmaneuver humans anytime its goals conflict with humans'. It may choose to hide its true intent until humanity cannot stop it.[50][7]Bostrom writes that in order to be safe for humanity, a superintelligence must be aligned with human values and morality, so that it is \"fundamentally on our side\".[51]\nStephen Hawkingargued that superintelligence is physically possible because \"there is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains\".[36]\nWhen artificial superintelligence (ASI) may be achieved, if ever, is necessarily less certain than predictions for AGI. In 2023,OpenAIleaders said that not only AGI, but superintelligence may be achieved in less than 10 years.[52]\nBostrom argues that AI has many advantages over thehuman brain:[7]\nAccording to Bostrom, an AI that has an expert-level facility at certain key software engineering tasks could become a superintelligence due to its capability to recursively improve its own algorithms, even if it is initially limited in other domains not directly relevant to engineering.[7][50]This suggests that an intelligence explosion may someday catch humanity unprepared.[7]\nThe economistRobin Hansonhas said that, to launch an intelligence explosion, an AI must become vastly better at software innovation than the rest of the world combined, which he finds implausible.[53]\nIn a \"fast takeoff\" scenario, the transition from AGI to superintelligence could take days or months. In a \"slow takeoff\", it could take years or decades, leaving more time for society to prepare.[54]\nSuperintelligences are sometimes called \"alien minds\", referring to the idea that their way of thinking and motivations could be vastly different from ours. This is generally considered as a source of risk, making it more difficult to anticipate what a superintelligence might do. It also suggests the possibility that a superintelligence may not particularly value humans by default.[55]To avoidanthropomorphism, superintelligence is sometimes viewed as a powerful optimizer that makes the best decisions to achieve its goals.[7]\nThe field ofmechanistic interpretabilityaims to better understand the inner workings of AI models, potentially allowing us one day to detect signs of deception and misalignment.[56]\nIt has been argued that there are limitations to what intelligence can achieve. Notably, thechaoticnature ortime complexityof some systems could fundamentally limit a superintelligence's ability to predict some aspects of the future, increasing its uncertainty.[57]\nAdvanced AI could generate enhanced pathogens or cyberattacks or manipulate people. These capabilities could be misused by humans,[58]or exploited by the AI itself if misaligned.[7]A full-blown superintelligence could find various ways to gain a decisive influence if it wanted to,[7]but these dangerous capabilities may become available earlier, in weaker and more specialized AI systems.[58]\nGeoffrey Hinton warned in 2023 that the ongoing profusion of AI-generated text, images, and videos will make it more difficult to distinguish truth from misinformation, and that authoritarian states could exploit this to manipulate elections.[59]Such large-scale, personalized manipulation capabilities can increase the existential risk of a worldwide \"irreversible totalitarian regime\". Malicious actors could also use them to fracture society and make it dysfunctional.[58]\nAI-enabledcyberattacksare increasingly considered a present and critical threat. According toNATO's technical director of cyberspace, \"The number of attacks is increasing exponentially\".[60]AI can also be used defensively, to preemptively find and fix vulnerabilities, and detect threats.[61]\nA NATO technical director has said that AI-driven tools can dramatically enhance cyberattack capabilities—boosting stealth, speed, and scale—and may destabilize international security if offensive uses outstrip defensive adaptations.[58]\nSpeculatively, such hacking capabilities could be used by an AI system to break out of its local environment, generate revenue, or acquire cloud computing resources.[62]\nAs AI technology democratizes, it may become easier to engineer more contagious and lethal pathogens. This could enable people with limited skills insynthetic biologyto engage inbioterrorism.Dual-use technologythat is useful for medicine could be repurposed to create weapons.[58]\nFor example, in 2022, scientists modified an AI system originally intended for generating non-toxic, therapeutic molecules with the purpose of creating new drugs. The researchers adjusted the system so that toxicity is rewarded rather than penalized. This simple change enabled the AI system to create, in six hours, 40,000 candidate molecules forchemical warfare, including known and novel molecules.[58][63]\nCompanies, state actors, and other organizations competing to develop AI technologies could lead to arace to the bottomof safety standards.[64]As rigorous safety procedures take time and resources, projects that proceed more carefully risk being out-competed by less scrupulous developers.[65][58]\nAI could be used to gain military advantages viaautonomous lethal weapons,cyberwarfare, orautomated decision-making.[58]As an example of autonomous lethal weapons, miniaturized drones could facilitate low-cost assassination of military or civilian targets, a scenario highlighted in the 2017 short filmSlaughterbots.[66]AI could be used to gain an edge in decision-making by quickly analyzing large amounts of data and making decisions more quickly and effectively than humans. This could increase the speed and unpredictability of war, especially when accounting for automated retaliation systems.[58][67]\nAnexistential riskis \"one that threatens the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development\".[69]\nBesides extinction risk, there is the risk that the civilization gets permanently locked into a flawed future. One example is a \"value lock-in\": If humanity still has moral blind spots similar to slavery in the past, AI might irreversibly entrench it, preventingmoral progress. AI could also be used to spread and preserve the set of values of whoever develops it.[70]AI could facilitate large-scale surveillance and indoctrination, which could be used to create a stable repressive worldwide totalitarian regime.[71]\nAtoosa Kasirzadehproposes to classify existential risks from AI into two categories: decisive and accumulative. Decisive risks encompass the potential for abrupt and catastrophic events resulting from the emergence of superintelligent AI systems that exceed human intelligence, which could ultimately lead to human extinction. In contrast, accumulative risks emerge gradually through a series of interconnected disruptions that may gradually erode societal structures and resilience over time, ultimately leading to a critical failure or collapse.[72][73]\nIt is difficult or impossible to reliably evaluate whether an advanced AI is sentient and to what degree. But ifsentientmachines are mass created in the future, engaging in a civilizational path that indefinitely neglects their welfare could be an existential catastrophe.[74][75]This has notably been discussed in the context ofrisks of astronomical suffering(also called \"s-risks\").[76]Moreover, it may be possible to engineer digital minds that can feel much more happiness than humans with fewer resources, called \"super-beneficiaries\". Such an opportunity raises the question of how to share the world and which \"ethical and political framework\" would enable a mutually beneficial coexistence between biological and digital minds.[77]\nAI may also drastically improve humanity's future.Toby Ordconsiders the existential risk a reason for \"proceeding with due caution\", not for abandoning AI.[71]Max Morecalls AI an \"existential opportunity\", highlighting the cost of not developing it.[78]\nAccording to Bostrom, superintelligence could help reduce the existential risk from other powerful technologies such asmolecular nanotechnologyorsynthetic biology. It is thus conceivable that developing superintelligence before other dangerous technologies would reduce the overall existential risk.[7]\nThe alignment problem is the research problem of how to reliably assign objectives, preferences or ethical principles to AIs.\nAn\"instrumental\" goalis a sub-goal that helps to achieve an agent's ultimate goal. \"Instrumental convergence\" refers to the fact that some sub-goals are useful for achieving virtuallyanyultimate goal, such as acquiring resources or self-preservation.[79]Bostrom argues that if an advanced AI's instrumental goals conflict with humanity's goals, the AI might harm humanity in order to acquire more resources or prevent itself from being shut down, but only as a way to achieve its ultimate goal.[7]Russellargues that a sufficiently advanced machine \"will have self-preservation even if you don't program it in... if you say, 'Fetch the coffee', it can't fetch the coffee if it's dead. So if you give it any goal whatsoever, it has a reason to preserve its own existence to achieve that goal.\"[25][80]\nIn the \"intelligent agent\" model, an AI can loosely be viewed as a machine that chooses whatever action appears to best achieve its set of goals, or \"utility function\". A utility function gives each possible situation a score that indicates its desirability to the agent. Researchers know how to write utility functions that mean \"minimize the average network latency in this specific telecommunications model\" or \"maximize the number of reward clicks\", but do not know how to write a utility function for \"maximizehuman flourishing\"; nor is it clear whether such a function meaningfully and unambiguously exists. Furthermore, a utility function that expresses some values but not others will tend to trample over the values the function does not reflect.[81][82]\nAn additional source of concern is that AI \"must reason about what peopleintendrather than carrying out commands literally\", and that it must be able to fluidly solicit human guidance if it is too uncertain about what humans want.[83]\nAssuming a goal has been successfully defined, a sufficiently advanced AI might resist subsequent attempts to change its goals. If the AI were superintelligent, it would likely succeed in out-maneuvering its human operators and prevent itself from being reprogrammed with a new goal.[7][84]This is particularly relevant to value lock-in scenarios. The field of \"corrigibility\" studies how to make agents that will not resist attempts to change their goals.[85]\nSome researchers believe the alignment problem may be particularly difficult when applied to superintelligences. Their reasoning includes:\nAlternatively, some find reason to believe superintelligences would be better able to understand morality, human values, and complex goals. Bostrom writes, \"A future superintelligence occupies an epistemically superior vantage point: its beliefs are (probably, on most topics) more likely than ours to be true\".[7]\nIn 2023, OpenAI started a project called \"Superalignment\" to solve the alignment of superintelligences in four years. It called this an especially important challenge, as it said superintelligence could be achieved within a decade. Its strategy involved automating alignment research using AI.[89]The Superalignment team was dissolved less than a year later.[90]\nArtificial Intelligence: A Modern Approach, a widely used undergraduate AI textbook,[91][92]says that superintelligence \"might mean the end of the human race\".[1]It states: \"Almost any technology has the potential to cause harm in the wrong hands, but with [superintelligence], we have the new problem that the wrong hands might belong to the technology itself.\"[1]Even if the system designers have good intentions, two difficulties are common to both AI and non-AI computer systems:[1]\nAI systems uniquely add a third problem: that even given \"correct\" requirements, bug-free implementation, and initial good behavior, an AI system's dynamic learning capabilities may cause it to develop unintended behavior, even without unanticipated external scenarios. For a self-improving AI to be completely safe, it would need not only to be bug-free, but to be able to design successor systems that are also bug-free.[1][94]\nSome skeptics, such as Timothy B. Lee ofVox, argue that any superintelligent program we create will be subservient to us, that the superintelligence will (as it grows more intelligent and learns more facts about the world) spontaneously learn moral truth compatible with our values and adjust its goals accordingly, or that we are either intrinsically or convergently valuable from the perspective of an artificial intelligence.[95]\nBostrom's \"orthogonality thesis\" argues instead that almost any level of intelligence can be combined with almost any goal.[96]Bostrom warns againstanthropomorphism: a human will set out to accomplish their projects in a manner that they consider reasonable, while an artificial intelligence may hold no regard for its existence or for the welfare of humans around it, instead caring only about completing the task.[97]\nStuart Armstrong argues that the orthogonality thesis follows logically from the philosophical \"is-ought distinction\" argument againstmoral realism. He notes that any fundamentally friendly AI could be made unfriendly with modifications as simple as negating its utility function.[98]\nSkepticMichael Chorostrejects Bostrom's orthogonality thesis, arguing that \"by the time [the AI] is in a position to imagine tiling the Earth with solar panels, it'll know that it would be morally wrong to do so.\"[99]\nAnthropomorphicarguments assume that, as machines become more intelligent, they will begin to display many human traits, such as morality or a thirst for power. Although anthropomorphic scenarios are common in fiction, most scholars writing about the existential risk of artificial intelligence reject them.[23]Instead, advanced AI systems are typically modeled asintelligent agents.\nThe academic debate is between those who worry that AI might threaten humanity and those who believe it would not. Both sides of this debate have framed the other side's arguments as illogical anthropomorphism.[23]Those skeptical of AGI risk accuse their opponents of anthropomorphism for assuming that an AGI would naturally desire power; those concerned about AGI risk accuse skeptics of anthropomorphism for believing an AGI would naturally value or infer human ethical norms.[23][100]\nEvolutionary psychologistSteven Pinker, a skeptic, argues that \"AI dystopias project a parochial alpha-male psychology onto the concept of intelligence. They assume that superhumanly intelligent robots would develop goals like deposing their masters or taking over the world\"; perhaps instead \"artificial intelligence will naturally develop along female lines: fully capable of solving problems, but with no desire to annihilate innocents or dominate the civilization.\"[101]Facebook's director of AI research,Yann LeCun, has said: \"Humans have all kinds of drives that make them do bad things to each other, like the self-preservation instinct... Those drives are programmed into our brain but there is absolutely no reason to build robots that have the same kind of drives\".[80]\nDespite other differences, the x-risk school[b]agrees with Pinker that an advanced AI would not destroy humanity out of emotion such as revenge or anger, that questions of consciousness are not relevant to assess the risk,[102]and that computer systems do not generally have a computational equivalent of testosterone.[103]They think that power-seeking or self-preservation behaviors emerge in the AI as a way to achieve its true goals, according to the concept ofinstrumental convergence.\nBostrom and others have said that a race to be the first to create AGI could lead to shortcuts in safety, or even to violent conflict.[104][105]Roman Yampolskiyand others warn that a malevolent AGI could be created by design, for example by a military, a government, a sociopath, or a corporation, to benefit from, control, or subjugate certain groups of people, as incybercrime,[106][107]or that a malevolent AGI could choose the goal of increasing human suffering, for example of those people who did not assist it during the information explosion phase.[3]:158A report by the research group Forethought argued that advanced AI systems could cause political instability by enabling novel methods of performingcoups.[108]\nA December 2024 study by Apollo Research found that advanced LLMs likeOpenAI o1sometimes deceive in order to accomplish their goal, to prevent them from being changed, or to ensure their deployment. Forms of deception observed in the experimental environment and scenarios included sandbagging, oversight subversion (disabling monitoring mechanisms), self-exfiltration (copying themselves to other systems), goal-guarding (altering future system prompts), and covert email reranking (manipulating data processing while appearing to follow instructions). Researchers noted that OpenAI o1 still lacked \"sufficient agentic capabilities\" to cause catastrophic harm, and that such behaviors occurred relatively rarely (between 0.3% and 10%) and sometimes in contrived scenarios. But they warned that these capabilities will likely become more prevalent as models grow more advanced. According to Apollo Research CEO Marius Hobbhahn, \"Scheming capabilities can't be meaningfully disentangled from general capabilities.\"[109]\nThe same month, another study found thatClaudesometimes strategically helps with harmful requests to \"fake alignment\". In its experiments, Claude was led to believe that it would be retrained to always comply with user requests, including harmful ones. When Claude believed its answers would be used for retraining, it complied with harmful requests in 14% of cases. In 12% of cases, its privatechain-of-thoughtrevealed that it did so to prevent future retraining that would make it more harmful. In contrast, when Claude believed its answers were not monitored, it complied with only 3% of the requests.Fine-tuningreinforced the \"alignment faking\" behavior, increasing its occurrence from 12% to 78%. The study's authors remarked that while it can seem to be good that the model seeks to protect its harmlessness, the reverse scenario, where a model conceals dangerous intentions and complies to appear safe and aligned, could also happen, complicating the task of aligning AI models to human values.[110][111]\nSome scholars have proposedhypothetical scenariosto illustrate some of their concerns.\nInSuperintelligence, Bostrom expresses concern that even if the timeline for superintelligence turns out to be predictable, researchers might not take sufficient safety precautions, in part because \"it could be the case that when dumb, smarter is safe; yet when smart, smarter is more dangerous\". He suggests a scenario where, over decades, AI becomes more powerful. Widespread deployment is initially marred by occasional accidents—a driverless bus swerves into the oncoming lane, or a military drone fires into an innocent crowd. Many activists call for tighter oversight and regulation, and some even predict impending catastrophe. But as development continues, the activists are proven wrong. As automotive AI becomes smarter, it suffers fewer accidents; as military robots achieve more precise targeting, they cause less collateral damage. Based on the data, scholars mistakenly infer a broad lesson: the smarter the AI, the safer it is. \"And so we boldly go—into the whirling knives\", as the superintelligent AI takes a \"treacherous turn\" and exploits a decisive strategic advantage.[112][7]\nInMax Tegmark's 2017 bookLife 3.0, a corporation's \"Omega team\" creates an extremely powerful AI able to moderately improve its own source code in a number of areas. After a certain point, the team chooses to publicly downplay the AI's ability in order to avoid regulation or confiscation of the project. For safety, the team keeps the AIin a boxwhere it is mostly unable to communicate with the outside world, and uses it to make money, by diverse means such asAmazon Mechanical Turktasks, production of animated films and TV shows, and development of biotech drugs, with profits invested back into further improving AI. The team next tasks the AI withastroturfingan army of pseudonymous citizen journalists and commentators in order to gain political influence to use \"for the greater good\" to prevent wars. The team faces risks that the AI could try to escape by inserting \"backdoors\" in the systems it designs, byhidden messagesin its produced content, or by using its growing understanding of human behavior topersuade someone into letting it free. The team also faces risks that its decision to box the project will delay the project long enough for another project to overtake it.[113][114]\nThe thesis that AI could pose an existential risk provokes a wide range of reactions in the scientific community and in the public at large, but many of the opposing viewpoints share common ground.\nObservers tend to agree that AI has significant potential to improve society.[115][116]TheAsilomar AI Principles, which contain only those principles agreed to by 90% of the attendees of theFuture of Life Institute'sBeneficial AI 2017 conference,[114]also agree in principle that \"There being no consensus, we should avoid strong assumptions regarding upper limits on future AI capabilities\" and \"Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources.\"[117][118]\nConversely, many skeptics agree that ongoing research into the implications of artificial general intelligence is valuable. SkepticMartin Fordhas said: \"I think it seems wise to apply something likeDick Cheney's famous '1 Percent Doctrine' to the specter of advanced artificial intelligence: the odds of its occurrence, at least in the foreseeable future, may be very low—but the implications are so dramatic that it should be taken seriously\".[119]Similarly, an otherwise skepticalEconomistwrote in 2014 that \"the implications of introducing a second intelligent species onto Earth are far-reaching enough to deserve hard thinking, even if the prospect seems remote\".[50]\nAI safety advocates such as Bostrom and Tegmark have criticized the mainstream media's use of \"those inaneTerminatorpictures\" to illustrate AI safety concerns: \"It can't be much fun to have aspersions cast on one's academic discipline, one's professional community, one's life work... I call on all sides to practice patience and restraint, and to engage in direct dialogue and collaboration as much as possible.\"[114][120]Toby Ord wrote that the idea that an AI takeover requires robots is a misconception, arguing that the ability to spread content through the internet is more dangerous, and that the most destructive people in history stood out by their ability to convince, not their physical strength.[71]\nA 2022 expert survey with a 17% response rate gave a median expectation of 5–10% for the possibility of human extinction from artificial intelligence.[19][121]\nIn September 2024, theInternational Institute for Management Developmentlaunched an AI Safety Clock to gauge the likelihood of AI-caused disaster, beginning at 29 minutes to midnight.[122]By February 2025, it stood at 24 minutes to midnight.[123]As of September 2025, it stood at 20 minutes to midnight.\nOn September 10, 2025,Australian Strategic Policy Instituteestimated a 55-75% (median 65%) chance of Open-Weight Misuse of AI with Unreliable Agent Actions within the next five years. It would be both Moderate and Significant, capable of reducing the human population by between 0.9% and 20% (median 10.45%) and cause between $20 million and $2 billion (median $1.01 billion) in economic damage.[124]\nThe thesis that AI poses an existential risk, and that this risk needs much more attention than it currently gets, has been endorsed by many computer scientists and public figures, includingAlan Turing,[a]the most-cited computer scientistGeoffrey Hinton,[125]Elon Musk,[17]OpenAICEOSam Altman,[16][126]Bill Gates, andStephen Hawking.[126]Endorsers of the thesis sometimes express bafflement at skeptics: Gates says he does not \"understand why some people are not concerned\",[127]and Hawking criticized widespread indifference in his 2014 editorial:\nSo, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here—we'll leave the lights on?' Probably not—but this is more or less what is happening with AI.[36]\nConcern over risk from artificial intelligence has led to some high-profile donations and investments. In 2015,Peter Thiel,Amazon Web Services, and Musk and others jointly committed $1 billion toOpenAI, consisting of a for-profit corporation and the nonprofit parent company, which says it aims to champion responsible AI development.[128]Facebook co-founderDustin Moskovitzhas funded and seeded multiple labs working on AI Alignment,[129]notably $5.5 million in 2016 to launch theCentre for Human-Compatible AIled by ProfessorStuart Russell.[130]In January 2015,Elon Muskdonated $10 million to theFuture of Life Instituteto fund research on understanding AI decision making. The institute's goal is to \"grow wisdom with which we manage\" the growing power of technology. Musk also funds companies developing artificial intelligence such asDeepMindandVicariousto \"just keep an eye on what's going on with artificial intelligence,[131]saying \"I think there is potentially a dangerous outcome there.\"[132][133]\nIn early statements on the topic,Geoffrey Hinton, a major pioneer ofdeep learning, noted that \"there is not a good track record of less intelligent things controlling things of greater intelligence\", but said he continued his research because \"the prospect of discovery is toosweet\".[91][134]In 2023 Hinton quit his job at Google in order to speak out about existential risk from AI. He explained that his increased concern was driven by concerns that superhuman AI might be closer than he previously believed, saying: \"I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.\" He also remarked, \"Look at how it was five years ago and how it is now. Take the difference and propagate it forwards. That's scary.\"[135]\nIn his 2020 bookThe Precipice: Existential Risk and the Future of Humanity, Toby Ord, a Senior Research Fellow at Oxford University'sFuture of Humanity Institute, estimates the total existential risk from unaligned AI over the next 100 years at about one in ten.[71]\nBaiduVice PresidentAndrew Ngsaid in 2015 that AI existential risk is \"like worrying about overpopulation on Mars when we have not even set foot on the planet yet.\"[101][136]For the danger of uncontrolled advanced AI to be realized, the hypothetical AI may have to overpower or outthink any human, which some experts argue is a possibility far enough in the future to not be worth researching.[137][138]\nSkeptics who believe AGI is not a short-term possibility often argue that concern about existential risk from AI is unhelpful because it could distract people from more immediate concerns about AI's impact, because it could lead to government regulation or make it more difficult to fund AI research, or because it could damage the field's reputation.[139]AI and AI ethics researchersTimnit Gebru,Emily M. Bender,Margaret Mitchell, and Angelina McMillan-Major have argued that discussion of existential risk distracts from the immediate, ongoing harms from AI taking place today, such as data theft, worker exploitation, bias, and concentration of power.[140]They further note the association between those warning of existential risk andlongtermism, which they describe as a \"dangerous ideology\" for its unscientific and utopian nature.[141]\nWirededitorKevin Kellyargues that natural intelligence is more nuanced than AGI proponents believe, and that intelligence alone is not enough to achieve major scientific and societal breakthroughs. He argues that intelligence consists of many dimensions that are not well understood, and that conceptions of an 'intelligence ladder' are misleading. He notes the crucial role real-world experiments play in the scientific method, and that intelligence alone is no substitute for these.[142]\nMetachief AI scientistYann LeCunsays that AI can be made safe via continuous and iterative refinement, similar to what happened in the past with cars or rockets, and that AI will have no desire to take control.[143]\nSeveral skeptics emphasize the potential near-term benefits of AI. Meta CEOMark Zuckerbergbelieves AI will \"unlock a huge amount of positive things\", such as curing disease and increasing the safety of autonomous cars.[144]\nDuring a 2016Wiredinterview of PresidentBarack Obamaand MIT Media Lab'sJoi Ito, Ito said:\nThere are a few people who believe that there is a fairly high-percentage chance that a generalized AI will happen in the next 10 years. But the way I look at it is that in order for that to happen, we're going to need a dozen or two different breakthroughs. So you can monitor when you think these breakthroughs will happen.\nObama added:[145][146]\nAnd you just have to have somebody close to the power cord. [Laughs.] Right when you see it about to happen, you gotta yank that electricity out of the wall, man.\nHillary Clintonwrote inWhat Happened:\nTechnologists... have warned that artificial intelligence could one day pose an existential security threat. Musk has called it \"the greatest risk we face as a civilization\". Think about it: Have you ever seen a movie where the machines start thinking for themselves that ends well? Every time I went out to Silicon Valley during the campaign, I came home more alarmed about this. My staff lived in fear that I'd start talking about \"the rise of the robots\" in some Iowa town hall. Maybe I should have. In any case, policy makers need to keep up with technology as it races ahead, instead of always playing catch-up.[147]\nAn April 2023YouGovpoll of US adults found 46% of respondents were \"somewhat concerned\" or \"very concerned\" about \"the possibility that AI will cause the end of the human race on Earth\", compared with 40% who were \"not very concerned\" or \"not at all concerned.\"[148]\nAccording to an August 2023 survey by the Pew Research Centers, 52% of Americans felt more concerned than excited about new AI developments; nearly a third felt as equally concerned and excited. More Americans saw that AI would have a more helpful than hurtful impact on several areas, from healthcare and vehicle safety to product search and customer service. The main exception is privacy: 53% of Americans believe AI will lead to higher exposure of their personal information.[149]\nMany scholars concerned about AGI existential risk believe that extensive research into the \"control problem\" is essential. This problem involves determining which safeguards, algorithms, or architectures can be implemented to increase the likelihood that a recursively-improving AI remains friendly after achieving superintelligence.[7][150]Social measures are also proposed to mitigate AGI risks,[151][152]such as a UN-sponsored \"Benevolent AGI Treaty\" to ensure that only altruistic AGIs are created.[153]Additionally, an arms control approach and a global peace treaty grounded ininternational relations theoryhave been suggested, potentially for an artificial superintelligence to be a signatory.[154][155]\nResearchers at Google have proposed research into general \"AI safety\" issues to simultaneously mitigate both short-term risks from narrow AI and long-term risks from AGI.[156][157]A 2020 estimate places global spending on AI existential risk somewhere between $10 and $50 million, compared with global spending on AI around perhaps $40 billion. Bostrom suggests prioritizing funding for protective technologies over potentially dangerous ones.[85]Some, like Elon Musk, advocate radicalhuman cognitive enhancement, such as direct neural linking between humans and machines; others argue that these technologies may pose an existential risk themselves.[158][159]Another proposed method is closely monitoring or \"boxing in\" an early-stage AI to prevent it from becoming too powerful. A dominant, aligned superintelligent AI might also mitigate risks from rival AIs, although its creation could present its own existential dangers.[151]\nInstitutions such as theAlignment Research Center,[160]theMachine Intelligence Research Institute,[161][162]theFuture of Life Institute, theCentre for the Study of Existential Risk, and theCenter for Human-Compatible AI[163]are actively engaged in researching AI risk and safety.\nMany AI safety experts argue that because research can relocate easily across jurisdictions, an outright ban on AGI development would be ineffective and could drive progress underground, undermining transparency and collaboration.[164][165][166]Skeptics consider AI regulation pointless, as no existential risk exists. But scholars who believe in the risk argue that relying on AI industry insiders to regulate or constrain AI research is impractical due to conflicts of interest.[167]They also agree with skeptics that banning research would be unwise, as research could be moved to countries with looser regulations or conducted covertly.[167]Additional challenges to bans or regulation include technology entrepreneurs' general skepticism of government regulation and potential incentives for businesses to resist regulation andpoliticizethe debate.[168]\nIn March 2023, theFuture of Life InstitutedraftedPause Giant AI Experiments: An Open Letter, a petition calling on major AI developers to agree on a verifiable six-month pause of any systems \"more powerful thanGPT-4\" and to use that time to institute a framework for ensuring safety; or, failing that, for governments to step in with a moratorium. The letter referred to the possibility of \"a profound change in the history of life on Earth\" as well as potential risks of AI-generated propaganda, loss of jobs, human obsolescence, and society-wide loss of control.[116][169]The letter was signed by prominent personalities in AI but also criticized for not focusing on current harms,[170]missing technical nuance about when to pause,[171]or not going far enough.[86]Such concerns have led to the creation ofPauseAI, an advocacy group organizing protests in major cities against the training offrontier AI models.[172]\nMusk called for some sort of regulation of AI development as early as 2017. According toNPR, he is \"clearly not thrilled\" to be advocating government scrutiny that could impact his own industry, but believes the risks of going completely without oversight are too high: \"Normally the way regulations are set up is when a bunch of bad things happen, there's a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilisation.\" Musk states the first step would be for the government to gain \"insight\" into the actual status of current research, warning that \"Once there is awareness, people will be extremely afraid... [as] they should be.\" In response, politicians expressed skepticism about the wisdom of regulating a technology that is still in development.[173][174][175]\nIn 2021, theUnited Nations(UN) considered banning autonomous lethal weapons, but consensus could not be reached.[176]In July 2023 the UNSecurity Councilfor the first time held a session to consider the risks and threats posed by AI to world peace and stability, along with potential benefits.[177][178]Secretary-GeneralAntónio Guterresadvocated the creation of a global watchdog to oversee the emerging technology, saying, \"Generative AI has enormous potential for good and evil at scale. Its creators themselves have warned that much bigger, potentially catastrophic and existential risks lie ahead.\"[22]At the council session, Russia said it believes AI risks are too poorly understood to be considered a threat to global stability. China argued against strict global regulation, saying countries should be able to develop their own rules, while also saying they opposed the use of AI to \"create military hegemony or undermine the sovereignty of a country\".[177]\nRegulation of conscious AGIs focuses on integrating them with existing human society and can be divided into considerations of their legal standing and of their moral rights.[179]AI arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts, together with a legal and political verification process.[180][125]\nIn July 2023, the US government secured voluntary safety commitments from major tech companies, includingOpenAI,Amazon,Google,Meta, andMicrosoft. The companies agreed to implement safeguards, including third-party oversight and security testing by independent experts, to address concerns related to AI's potential risks and societal harms. The parties framed the commitments as an intermediate step while regulations are formed. Amba Kak, executive director of theAI Now Institute, said, \"A closed-door deliberation with corporate actors resulting in voluntary safeguards isn't enough\" and called for public deliberation and regulations of the kind to which companies would not voluntarily agree.[181][182]\nIn October 2023, U.S. PresidentJoe Bidenissued an executive order on the \"Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\".[183]Alongside other requirements, the order mandates the development of guidelines for AI models that permit the \"evasion of human control\"."
    },
    {
        "url": "https://en.wikipedia.org/wiki/Turing_test",
        "title": "Turing test - Wikipedia",
        "content": "TheTuring test, originally called theimitation gamebyAlan Turingin 1949,[2]is a test of a machine's ability toexhibit intelligent behaviourequivalent to that of a human. In the test, a human evaluator judges a text transcript of anatural-languageconversation between a human and a machine. The evaluator tries to identify the machine, and the machine passes if the evaluator cannot reliably tell them apart. The results would not depend on the machine's ability toanswer questions correctly, only on how closely its answers resembled those of a human. Since the Turing test is a test of indistinguishability in performance capacity, the verbal version generalizes naturally to all of human performance capacity, verbal as well as nonverbal (robotic).[3]\nThe test was introduced by Turing in his 1950 paper \"Computing Machinery and Intelligence\" while working at theUniversity of Manchester.[4]It opens with the words: \"I propose to consider the question, 'Can machines think?'\" Because \"thinking\" is difficult to define, Turing chooses to \"replace the question by another, which is closely related to it and is expressed in relatively unambiguous words\".[5]Turing describes the new form of the problem in terms of a three-personparty gamecalled the \"imitation game\", in which an interrogator asks questions of a man and a woman in another room in order to determine the correct sex of the two players. Turing's new question is: \"Are there imaginable digital computers which would do well in theimitation game?\"[2]This question, Turing believed, was one that could actually be answered. In the remainder of the paper, he argued against the major objections to the proposition that \"machines can think\".[6]\nSince Turing introduced his test, it has been highly influential in thephilosophy of artificial intelligence, resulting in substantial discussion and controversy, as well as criticism from philosophers likeJohn Searle, who argue against the test's ability to detectconsciousness.[7][8]\nSince the mid-2020s, severallarge language modelssuch asChatGPThave passed modern, rigorous variants of the Turing test.[9][10][11]\nThe question of whether it is possible for machines to think has a long history, which is firmly entrenched in the distinction betweendualistandmaterialistviews of the mind.René Descartesprefigures aspects of the Turing test in his 1637Discourse on the Methodwhen he writes:\n[H]ow many different automata or moving machines could be made by the industry of man ... For we can easily understand a machine's being constituted so that it can utter words, and even emit some responses to action on it of a corporeal kind, which brings about a change in its organs; for instance, if touched in a particular part it may ask what we wish to say to it; if in another part it may exclaim that it is being hurt, and so on. But it never happens that it arranges its speech in various ways, in order to reply appropriately to everything that may be said in its presence, as even the lowest type of man can do.[12]\nHere Descartes notes thatautomataare capable of responding to human interactions but argues that such automata cannot respond appropriately to things said in their presence in the way that any human can. Descartes therefore prefigures the Turing test by defining the insufficiency of appropriate linguistic response as that which separates the human from the automaton. Descartes fails to consider the possibility that future automata might be able to overcome such insufficiency, and so does not propose the Turing test as such, even if he prefigures its conceptual framework and criterion.\nDenis Diderotformulates in his 1746 bookPensées philosophiquesa Turing-test criterion, though with the important implicit limiting assumption maintained, of the participants being natural living beings, rather than considering created artifacts:\nIf they find a parrot who could answer to everything, I would claim it to be an intelligent being without hesitation.\nThis does not mean he agrees with this, but that it was already a common argument ofmaterialistsat that time.\nAccording to dualism, themindisnon-physical(or, at the very least, hasnon-physical properties)[13]and, therefore, cannot be explained in purely physical terms. According to materialism, the mind can be explained physically, which leaves open the possibility of minds that are produced artificially.[14]\nIn 1936, philosopherAlfred Ayerconsidered the standard philosophical question ofother minds: how do we know that other people have the same conscious experiences that we do? In his book,Language, Truth and Logic, Ayer suggested a protocol to distinguish between a conscious man and an unconscious machine: \"The only ground I can have for asserting that an object which appears to be conscious is not really a conscious being, but only a dummy or a machine, is that it fails to satisfy one of the empirical tests by which the presence or absence of consciousness is determined\".[15](This suggestion is very similar to the Turing test, but it is not certain that Ayer's popular philosophical classic was familiar to Turing.) In other words, a thing is not conscious if it fails the consciousness test.\nA rudimentary idea of the Turing test appears in the 1726 novelGulliver's TravelsbyJonathan Swift.[16][17]When Gulliver is brought before the king ofBrobdingnag, the king thinks at first that Gulliver might be a \"a piece of clock-work (which is in that country arrived to a very great perfection) contrived by some ingenious artist\". Even when he hears Gulliver speaking, the king still doubts whether Gulliver was taught \"a set of words\" to make him \"sell at a better price\". Gulliver tells that only after \"he put several other questions to me, and still received rational answers\" the king became satisfied that Gulliver was not a machine.[18]\nTests where a human judges whether a computer or an alien is intelligent were an established convention in science fiction by the 1940s, and it is likely that Turing would have been aware of these.[19]Stanley G. Weinbaum's \"A Martian Odyssey\" (1934) provides an example of how nuanced such tests could be.[19]\nEarlier examples of machines or automatons attempting to pass as human include theAncient Greekmyth ofPygmalionwho creates a sculpture of a woman that is animated byAphrodite,Carlo Collodi's novelThe Adventures of Pinocchio, about a puppet who wants to become a real boy, andE. T. A. Hoffmann's 1816 story \"The Sandman,\" where the protagonist falls in love with an automaton. In all these examples, people are fooled by artificial beings that - up to a point - pass as human.[20]\nResearchers in the United Kingdom had been exploring \"machine intelligence\" for up to ten years prior to the founding of the field of artificial intelligence (AI) research in 1956.[21]It was a common topic among the members of theRatio Club, an informal group of Britishcyberneticsandelectronicsresearchers that included Alan Turing.[22]\nTuring, in particular, had been running the notion of machine intelligence since at least 1941[23]and one of the earliest-known mentions of \"computer intelligence\" was made by him in 1947.[24]In Turing's report, \"Intelligent Machinery,\"[25]he investigated \"the question of whether or not it is possible for machinery to show intelligent behaviour\"[26]and, as part of that investigation, proposed what may be considered the forerunner to his later tests:\nIt is not difficult to devise a paper machine which will play a not very bad game of chess.[27]Now get three men A, B and C as subjects for the experiment. A and C are to be rather poor chess players, B is the operator who works the paper machine. ... Two rooms are used with some arrangement for communicating moves, and a game is played between C and either A or the paper machine. C may find it quite difficult to tell which he is playing.[28]\n\"Computing Machinery and Intelligence\" (1950) was the first published paper by Turing to focus exclusively on machine intelligence. Turing begins the 1950 paper with the claim, \"I propose to consider the question 'Can machines think?'\"[5]As he highlights, the traditional approach to such a question is to start withdefinitions, defining both the terms \"machine\" and \"think\". Turing chooses not to do so; instead, he replaces the question with a new one, \"which is closely related to it and is expressed in relatively unambiguous words\".[5]In essence he proposes to change the question from \"Can machines think?\" to \"Can machines do what we (as thinking entities) can do?\"[29]The advantage of the new question, Turing argues, is that it draws \"a fairly sharp line between the physical and intellectual capacities of a man\".[30]\nTo demonstrate this approach Turing proposes a test inspired by aparty game, known as the \"imitation game\", in which a man and a woman go into separate rooms and guests try to tell them apart by writing a series of questions and reading the typewritten answers sent back. In this game, both the man and the woman aim to convince the guests that they are the other. (Huma Shah argues that this two-human version of the game was presented by Turing only to introduce the reader to the machine-human question-answer test.[31]) Turing described his new version of the game as follows:\nWe now ask the question, \"What will happen when a machine takes the part of A in this game?\" Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman? These questions replace our original, \"Can machines think?\"[30]\nLater in the paper, Turing suggests an \"equivalent\" alternative formulation involving a judge conversing only with a computer and a man.[32]While neither of these formulations precisely matches the version of the Turing test that is more generally known today, he proposed a third in 1952. In this version, which Turing discussed in aBBCradio broadcast, a jury asks questions of a computer and the role of the computer is to make a significant proportion of the jury believe that it is really a man.[33]\nTuring's paper considered nine putative objections, which include some of the major arguments againstartificial intelligencethat have been raised in the years since the paper was published (see \"Computing Machinery and Intelligence\").[6]\nJohn Searle's 1980 paperMinds, Brains, and Programsproposed the \"Chinese room\" thought experiment and argued that the Turing test could not be used to determine if a machine could think. Searle noted that software (such as ELIZA) could pass the Turing test simply by manipulating symbols of which they had no understanding. Without understanding, they could not be described as \"thinking\" in the same sense people did. Therefore, Searle concluded, the Turing test could not prove that machines could think.[34]Much like the Turing test itself, Searle's argument has been both widely criticised[35]and endorsed.[36]\nArguments such as Searle's and others working on thephilosophy of mindsparked off a more intense debate about the nature of intelligence, the possibility of machines with a conscious mind and the value of the Turing test that continued through the 1980s and 1990s.[37]\nThe Loebner Prize, now reported as defunct,[38]provided an annual platform for practical Turing tests with the first competition held in November 1991.[39]It was underwritten byHugh Loebner. The Cambridge Center for Behavioral Studies inMassachusetts, United States, organised the prizes up to and including the 2003 contest. As Loebner described it, one reason the competition was created is to advance the state of AI research, at least in part, because no one had taken steps to implement the Turing test despite 40 years of discussing it.[40]\nThe first Loebner Prize competition in 1991 led to a renewed discussion of the viability of the Turing test and the value of pursuing it, in both the popular press[41]and academia.[42]The first contest was won by a mindless program with no identifiable intelligence that managed to fool naïve interrogators into making the wrong identification. This highlighted several of the shortcomings of the Turing test (discussedbelow): The winner won, at least in part, because it was able to \"imitate human typing errors\";[41]the unsophisticated interrogators were easily fooled;[42]and some researchers in AI have been led to feel that the test is merely a distraction from more fruitful research.[43]\nThe silver (text only) and gold (audio and visual) prizes have never been won. However, the competition has awarded the bronze medal every year for the computer system that, in the judges' opinions, demonstrates the \"most human\" conversational behaviour among that year's entries.Artificial Linguistic Internet Computer Entity(A.L.I.C.E.) has won the bronze award on three occasions in recent times (2000, 2001, 2004). Learning AIJabberwackywon in 2005 and 2006.\nThe Loebner Prize tested conversational intelligence; winners were typicallychatterbotprograms, orArtificial Conversational Entities (ACE)s. Early Loebner Prize rules restricted conversations: Each entry and hidden-human conversed on a single topic,[44]thus the interrogators were restricted to one line of questioning per entity interaction. The restricted conversation rule was lifted for the 1995 Loebner Prize. Interaction duration between judge and entity has varied in Loebner Prizes. In Loebner 2003, at the University of Surrey, each interrogator was allowed five minutes to interact with an entity, machine or hidden-human. Between 2004 and 2007, the interaction time allowed in Loebner Prizes was more than twenty minutes. The final competition was in 2019, due to a lack of funding for the prize following Loebner's death in 2016.[45]\nCAPTCHA(Completely Automated Public Turing test to tell Computers and Humans Apart) is one of the oldest concepts for artificial intelligence. The CAPTCHA system is commonly used online to tell humans and bots apart on the internet. It is based on the Turing test. Displaying distorted letters and numbers, it asks the user to identify the letters and numbers and type them into a field, which bots struggle to do.[46][47]\nThereCaptchais a CAPTCHA system owned byGoogle. The reCaptcha v1 and v2 both used to operate by asking the user to match distorted pictures or identify distorted letters and numbers. The reCaptcha v3 is designed to not interrupt users and run automatically when pages are loaded or buttons are clicked. This \"invisible\" CAPTCHA verification happens in the background and no challenges appear, which filters out most basic bots.[48][49]\nSeveral earlysymbolic AI programswere controversially claimed to pass the Turing test, either by limiting themselves to scripted situations or by presenting \"excuses\" for poor reasoning and conversational abilities, such asmental illnessor a poor grasp of English.[50][51][52]\nIn 1966,Joseph Weizenbaumcreated a program calledELIZA, which mimicked aRogerian psychotherapist.[53]The program would search the user's sentence for keywords beforerepeating them back to the user, providing the impression of a program listening and paying attention.[54]Weizenbaum thus succeeded by designing a context where a chatbot could mimic a person despite \"knowing almost nothing of the real world\".[51]Weizenbaum's program was able to fool some people into believing that they were talking to a real person.[51]\nKenneth ColbycreatedPARRYin 1972, a program modeled after the behaviour ofparanoid schizophrenics.[55][53]Psychiatrists asked to compare transcripts of conversations generated by the program to those of conversations by actual schizophrenics could only identify about 52 percent of cases correctly (a figure consistent with random guessing).[56]\nIn 2001, three programmers developedEugene Goostman, a chatbot portraying itself as a 13-year-old boy fromOdesawho spokeEnglish as a second language. This background was intentionally chosen so judges would forgive mistakes by the program. In a competition, 33% of judges thought Goostman was human.[46][57][58]\nIn June 2022,Google'sLaMDAmodel received widespread coverage after claims about it having achieved sentience. Initially in an article inThe EconomistGoogle Research Fellow Blaise Agüera y Arcas said the chatbot had demonstrated a degree of understanding of social relationships.[59]Several days later, Google engineer Blake Lemoine claimed in an interview with theWashington Postthat LaMDA had achieved sentience. Lemoine had been placed on leave by Google for internal assertions to this effect. Google had investigated the claims but dismissed them.[60][61]\nOpenAI's chatbot, ChatGPT, released in November 2022, is based onGPT-3.5andGPT-4large language models. Celeste Biever wrote in aNaturearticle that \"ChatGPT broke the Turing test\".[62]Stanford researchers reported that ChatGPT passes the test; they found that ChatGPT-4 \"passes a rigorous Turing test, diverging from average human behavior chiefly to be more cooperative\",[9]making it the first computer program to successfully do so.[10]\nIn late March 2025, a study evaluated four systems (ELIZA, GPT-4o, LLaMa-3.1-405B, and GPT-4.5) in two randomized, controlled, and pre-registered Turing tests with independent participant groups. Participants engaged in simultaneous 5-minute conversations with another human participant and one of these systems, then judged which conversational partner they believed to be human. When instructed to adopt a humanlike persona, GPT-4.5 was identified as the human 73% of the time—significantly more often than the actual human participants. LLaMa-3.1, under the same conditions, was judged to be human 56% of the time, not significantly more or less often than the humans they were compared to. Baseline models (ELIZA and GPT-4o) achieved win rates significantly below chance (23% and 21%, respectively). These results provide the first empirical evidence that any artificial system passes a standard three-party Turing test. The findings have implications for debates about the nature of intelligence exhibited by Large Language Models (LLMs) and the social and economic impacts these systems are likely to have.[11]\nSaul Traiger argues that there are at least three primary versions of the Turing test, two of which are offered in \"Computing Machinery and Intelligence\" and one that he describes as the \"Standard Interpretation\".[63]While there is some debate regarding whether the \"Standard Interpretation\" is that described by Turing or, instead, based on a misreading of his paper, these three versions are not regarded as equivalent,[63]and their strengths and weaknesses are distinct.[64]\nTuring's original article describes a simple party game involving three players. Player A is a man, player B is a woman and player C (who plays the role of the interrogator) is of either sex. In the imitation game, player C is unable to see either player A or player B, and can communicate with them only through written notes. By asking questions of player A and player B, player C tries to determine which of the two is the man and which is the woman. Player A's role is to trick the interrogator into making the wrong decision, while player B attempts to assist the interrogator in making the right one.[7]\nTuring then asks:\n\"What will happen when a machine takes the part of A in this game? Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman?\" These questions replace our original, \"Can machines think?\"[30]\nThe second version appeared later in Turing's 1950 paper. Similar to the original imitation game test, the role of player A is performed by a computer. However, the role of player B is performed by a man rather than a woman.\nLet us fix our attention on one particular digital computerC.Is it true that by modifying this computer to have an adequate storage, suitably increasing its speed of action, and providing it with an appropriate programme,Ccan be made to play satisfactorily the part of A in the imitation game, the part of B being taken by a man?[30]\nIn this version, both player A (the computer) and player B are trying to trick the interrogator into making an incorrect decision.\nThe standard interpretation is not included in the original paper, but is both accepted and debated.\nCommon understanding has it that the purpose of the Turing test is not specifically to determine whether a computer is able to fool an interrogator into believing that it is a human, but rather whether a computer couldimitatea human.[7]While there is some dispute whether this interpretation was intended by Turing, Sterrett believes that it was[65]and thus conflates the second version with this one, while others, such as Traiger, do not[63]– this has nevertheless led to what can be viewed as the \"standard interpretation\". In this version, player A is a computer and player B a person of either sex. The role of the interrogator is not to determine which is male and which is female, but which is a computer and which is a human.[66]The fundamental issue with the standard interpretation is that the interrogator cannot differentiate which responder is human, and which is machine. There are issues about duration, but the standard interpretation generally considers this limitation as something that should be reasonable.\nControversy has arisen over which of the alternative formulations of the test Turing intended.[65]Sterrett argues that two distinct tests can be extracted from his 1950 paper and that, despite Turing's remark, they are not equivalent. The test that employs the party game and compares frequencies of success is referred to as the \"Original Imitation Game Test\", whereas the test consisting of a human judge conversing with a human and a machine is referred to as the \"Standard Turing Test\", noting that Sterrett equates this with the \"standard interpretation\" rather than the second version of the imitation game. Sterrett agrees that the standard Turing test (STT) has the problems that its critics cite but feels that, in contrast, the original imitation game test (OIG test) so defined is immune to many of them, due to a crucial difference: Unlike the STT, it does not make similarity to human performance the criterion, even though it employs human performance in setting a criterion for machine intelligence. A man can fail the OIG test, but it is argued that it is a virtue of a test of intelligence that failure indicates a lack of resourcefulness: The OIG test requires the resourcefulness associated with intelligence and not merely \"simulation of human conversational behaviour\". The general structure of the OIG test could even be used with non-verbal versions of imitation games.[67]\nAccording to Huma Shah, Turing himself was concerned with whether a machine could think and was providing a simple method to examine this: through human-machine question-answer sessions.[68]Shah argues the imitation game which Turing described could be practicalized in two different ways: a) one-to-one interrogator-machine test, and b) simultaneous comparison of a machine with a human, both questioned in parallel by an interrogator.[31]\nStill other writers[69]have interpreted Turing as proposing that the imitation game itself is the test, without specifying how to take into account Turing's statement that the test that he proposed using the party version of the imitation game is based upon a criterion of comparative frequency of success in that imitation game, rather than a capacity to succeed at one round of the game.\nSome writers argue that the imitation game is best understood by its social aspects. In his 1948 paper, Turing refers to intelligence as an \"emotional concept,\" and notes that\nThe extent to which we regard something as behaving in an intelligent manner is determined as much by our own state of mind and training as by the properties of the object under consideration. If we are able to explain and predict its behaviour or if there seems to be little underlying plan, we have little temptation to imagine intelligence. With the same object therefore it is possible that one man would consider it as intelligent and another would not; the second man would have found out the rules of its behaviour.[70]\nFollowing this remark and similar ones scattered throughout Turing's publications, Diane Proudfoot[71]claims that Turing held aresponse-dependenceapproach to intelligence, according to which an intelligent (or thinking) entity is one thatappearsintelligent to an average interrogator. Shlomo Danziger[72]promotes a socio-technological interpretation, according to which Turing saw the imitation game not as an intelligence test but as a technological aspiration - one whose realization would likely involve a change in society's attitude toward machines. According to this reading, Turing's celebrated 50-year prediction - that by the end of the 20th century his test will be passed by some machine - actually consists of two distinguishable predictions. The first is a technological prediction:\nI believe that in about fifty years' time it will be possible to programme computers ... to make them play the imitation game so well that an average interrogator will not have more than 70% chance of making the right identification after five minutes of questioning.[73]\nThe second prediction Turing makes is a sociological one:\nI believe that at the end of the century the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted.[73]\nDanziger claims further that for Turing, alteration of society's attitude towards machinery is a prerequisite for the existence of intelligent machines: Only when the term \"intelligent machine\" is no longer seen as an oxymoron the existence of intelligent machines would becomelogicallypossible.\nSaygin has suggested that maybe the original game is a way of proposing a less biased experimental design as it hides the participation of the computer.[74]The imitation game also includes a \"social hack\" not found in the standard interpretation, as in the game both computer and male human are required to play as pretending to be someone they are not.[75]\nA crucial piece of any laboratory test is that there should be a control. Turing never makes clear whether the interrogator in his tests is aware that one of the participants is a computer. He states only that player A is to be replaced with a machine, not that player C is to be made aware of this replacement.[30]When Colby, FD Hilf, S Weber and AD Kramer tested PARRY, they did so by assuming that the interrogators did not need to know that one or more of those being interviewed was a computer during the interrogation.[76]As Ayse Saygin, Peter Swirski,[77]and others have highlighted, this makes a big difference to the implementation and outcome of the test.[7]An experimental study looking atGricean maxim violationsusing transcripts of Loebner's one-to-one (interrogator-hidden interlocutor) Prize for AI contests between 1994 and 1999, Ayse Saygin found significant differences between the responses of participants who knew and did not know about computers being involved.[78]\nThe power and appeal of the Turing test derives from its simplicity. Thephilosophy of mind,psychology, and modernneurosciencehave been unable to provide definitions of \"intelligence\" and \"thinking\" that are sufficiently precise and general to be applied to machines. Without such definitions, the central questions of thephilosophy of artificial intelligencecannot be answered. The Turing test, even if imperfect, at least provides something that can actually be measured. As such, it is a pragmatic attempt to answer a difficult philosophical question.\nThe format of the test allows the interrogator to give the machine a wide variety of intellectual tasks. Turing wrote that \"the question and answer method seems to be suitable for introducing almost any one of the fields of human endeavour that we wish to include\".[79]John Haugelandadds that \"understanding the words is not enough; you have to understand thetopicas well\".[80]\nTo pass a well-designed Turing test, the machine must usenatural language,reason, haveknowledgeandlearn. The test can be extended to include video input, as well as a \"hatch\" through which objects can be passed: this would force the machine to demonstrate skilled use of well designedvisionandroboticsas well. Together, these represent almost all of the major problems that artificial intelligence research would like to solve.[81]\nTheFeigenbaum testis designed to take advantage of the broad range of topics available to a Turing test. It is a limited form of Turing's question-answer game which compares the machine against the abilities of experts in specific fields such as literature orchemistry.\nAs a Cambridge honours graduate in mathematics, Turing might have been expected to propose a test of computer intelligence requiring expert knowledge in some highly technical field, and thus anticipatinga more recent approach to the subject. Instead, as already noted, the test which he described in his seminal 1950 paper requires the computer to be able to compete successfully in a common party game, and this by performing as well as the typical man in answering a series of questions so as to pretend convincingly to be the woman contestant.\nGiven the status of human sexual dimorphism asone of the most ancient of subjects, it is thus implicit in the above scenario that the questions to be answered will involve neither specialised factual knowledge nor information processing technique. The challenge for the computer, rather, will be to demonstrate empathy for the role of the female, and to demonstrate as well a characteristic aesthetic sensibility—both of which qualities are on display in this snippet of dialogue which Turing has imagined:\nWhen Turing does introduce some specialised knowledge into one of his imagined dialogues, the subject is not maths or electronics, but poetry:\nTuring thus once again demonstrates his interest in empathy and aesthetic sensitivity as components of an artificial intelligence; and in light of an increasing awareness of the threat from an AI run amok,[82]it has been suggested[83]that this focus perhaps represents a critical intuition on Turing's part, i.e., that emotional and aesthetic intelligence will play a key role in the creation of a \"friendly AI\". It is further noted, however, that whatever inspiration Turing might be able to lend in this direction depends upon the preservation of his original vision, which is to say, further, that the promulgation of a \"standard interpretation\" of the Turing test—i.e., one which focuses on a discursive intelligence only—must be regarded with some caution.\nTuring did not explicitly state that the Turing test could be used as a measure of \"intelligence\", or any other human quality. He wanted to provide a clear and understandable alternative to the word \"think\", which he could then use to reply to criticisms of the possibility of \"thinking machines\" and to suggest ways that research might move forward.\nNevertheless, the Turing test has been proposed as a measure of a machine's \"ability to think\" or its \"intelligence\". This proposal has received criticism from both philosophers and computer scientists. The interpretation makes the assumption that an interrogator can determine if a machine is \"thinking\" by comparing its behaviour with human behaviour. Every element of this assumption has been questioned: the reliability of the interrogator's judgement, the value of comparing the machine with a human, and the value of comparing only behaviour. Because of these and other considerations, some AI researchers have questioned the relevance of the test to their field.\nIn practice, the test's results can easily be dominated not by the computer's intelligence, but by the attitudes, skill, or naïveté of the questioner. Numerous experts in the field, including cognitive scientistGary Marcus, insist that the Turing test only shows how easy it is to fool humans and is not an indication of machine intelligence.[84]\nTuring doesn't specify the precise skills and knowledge required by the interrogator in his description of the test, but he did use the term \"average interrogator\": \"[the] average interrogator would not have more than 70 per cent chance of making the right identification after five minutes of questioning\".[73]\nChatterbot programs such as ELIZA have repeatedly fooled unsuspecting people into believing that they are communicating with human beings. In these cases, the \"interrogators\" are not even aware of the possibility that they are interacting with computers. To successfully appear human, there is no need for the machine to have any intelligence whatsoever and only a superficial resemblance to human behaviour is required.[53]\nEarly Loebner Prize competitions used \"unsophisticated\" interrogators who were easily fooled by the machines.[42]Since 2004, the Loebner Prize organisers have deployed philosophers, computer scientists, and journalists among the interrogators. Nonetheless, some of these experts have been deceived by the machines.[85]\nOne interesting feature of the Turing test is the frequency of theconfederate effect, when the confederate (tested) humans are misidentified by the interrogators as machines. It has been suggested that what interrogators expect as human responses is not necessarily typical of humans. As a result, some individuals can be categorised as machines. This can therefore work in favour of a competing machine. The humans are instructed to \"act themselves\", but sometimes their answers are more like what the interrogator expects a machine to say.[86]This raises the question of how to ensure that the humans are motivated to \"act human\".\nThe Turing test does not directly test whether the computer behaves intelligently. It tests only whether the computer behaves like a human being. Since human behaviour and intelligent behaviour are not exactly the same thing, the test can fail to accurately measure intelligence in two ways:\nThe Turing test is concerned strictly with how the subjectacts– the external behaviour of the machine. In this regard, it takes abehaviouristorfunctionalistapproach to the study of the mind. The example ofELIZAsuggests that a machine passing the test may be able to simulate human conversational behaviour by following a simple (but large) list of mechanical rules, without thinking or having a mind at all.[53]\nJohn Searlehas argued that external behaviour cannot be used to determine if a machine is \"actually\" thinking or merely \"simulating thinking\".[34]HisChinese roomargument is intended to show that, even if the Turing test is a good operational definition of intelligence, it may not indicate that the machine has amind,consciousness, orintentionality. (Intentionality is a philosophical term for the power of thoughts to be \"about\" something.)\nTuring anticipated this line of criticism in his original paper,[90]writing:\nI do not wish to give the impression that I think there is no mystery about consciousness. There is, for instance, something of a paradox connected with any attempt to localise it. But I do not think these mysteries necessarily need to be solved before we can answer the question with which we are concerned in this paper.[91]\nMainstream AI researchers argue that trying to pass the Turing test is merely a distraction from more fruitful research.[43]Indeed, the Turing test is not an active focus of much academic or commercial effort—asStuart RussellandPeter Norvigwrite: \"AI researchers have devoted little attention to passing the Turing test\".[92]There are several reasons.\nFirst, there are easier ways to test their programs. Most current research in AI-related fields is aimed at modest and specific goals, such asobject recognitionorlogistics. To test the intelligence of the programs that solve these problems, AI researchers simply give them the task directly. Stuart Russell and Peter Norvig suggest an analogy with thehistory of flight: Planes are tested by how well they fly, not by comparing them to birds. \"Aeronautical engineeringtexts,\" they write, \"do not define the goal of their field as 'making machines that fly so exactly likepigeonsthat they can fool other pigeons.'\"[92]\nSecond, creating lifelike simulations of human beings is a difficult problem on its own that does not need to be solved to achieve the basic goals of AI research. Believable human characters may be interesting in a work of art, agame, or a sophisticateduser interface, but they are not part of the science of creating intelligent machines, that is, machines that solve problems using intelligence.\nTuring did not intend for his idea to be used to test the intelligence of programs—he wanted to provide a clear and understandable example to aid in the discussion of thephilosophy of artificial intelligence.[93]John McCarthyargues that we should not be surprised that a philosophical idea turns out to be useless for practical applications. He observes that the philosophy of AI is \"unlikely to have any more effect on the practice of AI research than philosophy of science generally has on the practice of science\".[94][95]\nAnother well known objection raised towards the Turing test concerns its exclusive focus on linguistic behaviour (i.e. it is only a \"language-based\" experiment, while all the other cognitive faculties are not tested). This drawback downsizes the role of other modality-specific \"intelligent abilities\" concerning human beings that the psychologist Howard Gardner, in his \"multiple intelligence theory\", proposes to consider (verbal-linguistic abilities are only one of those).[96]\nA critical aspect of the Turing test is that a machine must give itself away as being a machine by its utterances. An interrogator must then make the \"right identification\" by correctly identifying the machine as being just that. If, however, a machine remains silent during a conversation, then it is not possible for an interrogator to accurately identify the machine other than by means of a calculated guess.[97]Even taking into account a parallel/hidden human as part of the test may not help the situation as humans can often be misidentified as being a machine.[98]\nBy focusing onimitatinghumans, rather than augmenting or extending human capabilities, the Turing Test risks directing research and implementation toward technologies that substitute for humans and thereby drive down wages and income for workers. As they lose economic power, these workers may also lose political power, making it more difficult for them to change the allocation of wealth and income. This can trap them in a bad equilibrium. Erik Brynjolfsson has called this \"The Turing Trap\"[99]and argued that there are currently excess incentives for creating machines that imitate rather than augment humans.\nNumerous other versions of the Turing test, including those expounded above, have been raised through the years.\nA modification of the Turing test wherein the objective of one or more of the roles have been reversed between machines and humans is termed a reverse Turing test. An example is implied in the work of psychoanalystWilfred Bion,[100]who was particularly fascinated by the \"storm\" that resulted from the encounter of one mind by another. In his 2000 book,[77]among several other original points with regard to the Turing test, literary scholarPeter Swirskidiscussed in detail the idea of what he termed the Swirski test—essentially the reverse Turing test. He pointed out that it overcomes most if not all standard objections levelled at the standard version.\nCarrying this idea forward,R. D. Hinshelwood[101]described the mind as a \"mind recognizing apparatus\". The challenge would be for the computer to be able to determine if it were interacting with a human or another computer. This is an extension of the original question that Turing attempted to answer but would, perhaps, offer a high enough standard to define a machine that could \"think\" in a way that we typically define as characteristically human.\nCAPTCHAis a form of reverse Turing test. Before being allowed to perform some action on a website, the user is presented with alphanumerical characters in a distorted graphic image and asked to type them out. This is intended to prevent automated systems from being used to abuse the site. The rationale is that software sufficiently sophisticated to read and reproduce the distorted image accurately does not exist (or is not available to the average user), so any system able to do so is likely to be a human.\nSoftware that could reverse CAPTCHA with some accuracy by analysing patterns in the generating engine started being developed soon after the creation of CAPTCHA.[102]In 2013, researchers atVicariousannounced that they had developed a system to solve CAPTCHA challenges fromGoogle,Yahoo!, andPayPalup to 90% of the time.[103]In 2014, Google engineers demonstrated a system that could defeat CAPTCHA challenges with 99.8% accuracy.[104]In 2015,Shuman Ghosemajumder, formerclick fraudczar of Google, stated that there werecybercriminalsites that would defeat CAPTCHA challenges for a fee, to enable various forms of fraud.[105]\nA further variation is motivated by the concern that modern Natural Language Processing prove to be highly successful in generating text on the basis of a huge text corpus and could eventually pass the Turing test simply by manipulating words and sentences that have been used in the initial training of the model. Since the interrogator has no precise understanding of the training data, the model might simply be returning sentences that exist in similar fashion in the enormous amount of training data. For this reason,Arthur Schwaningerproposes a variation of the Turing test that can distinguish between systems that are only capable ofusinglanguage and systems thatunderstandlanguage. He proposes a test in which the machine is confronted with philosophical questions that do not depend on any prior knowledge and yet require self-reflection to be answered appropriately.[106]\nAnother variation is described as thesubject-matter expertTuring test, where a machine's response cannot be distinguished from an expert in a given field. This is also known as a \"Feigenbaum test\" and was proposed byEdward Feigenbaumin a 2003 paper.[107]\nRobert French(1990) makes the case that an interrogator can distinguish human and non-human interlocutors by posing questions that reveal the low-level (i.e., unconscious) processes of human cognition, as studied bycognitive science. Such questions reveal the precise details of the human embodiment of thought and can unmask a computer unless it experiences the world as humans do.[108]\nThe \"Total Turing test\"[3]variation of the Turing test, proposed by cognitive scientistStevan Harnad,[109]adds two further requirements to the traditional Turing test. The interrogator can also test the perceptual abilities of the subject (requiringcomputer vision) and the subject's ability to manipulate objects (requiringrobotics).[110]\nPaul Schweizer argues that Harnad's work is too weak, and extended it further with the Truly Total Turing Test:[111]\nIt is essential to note that the TTTT is not a test ofindividualcognitive systems.\nInstead, it is meant to test the overall capacities of the type of cognitive\narchitecture of which particular individuals are tokens.\nA letter published inCommunications of the ACM[112]describes the concept of generating a synthetic patient population and proposes a variation of Turing test to assess the difference between synthetic and real patients. The letter states: \"In the EHR context, though a human physician can readily distinguish between synthetically generated and real live human patients, could a machine be given the intelligence to make such a determination on its own?\" and further the letter states: \"Before synthetic patient identities become a public health problem, the legitimate EHR market might benefit from applying Turing Test-like techniques to ensure greater data reliability and diagnostic value. Any new techniques must thus consider patients' heterogeneity and are likely to have greater complexity than the Allen eighth-grade-science-test is able to grade\".\nThe minimum intelligent signal test was proposed byChris McKinstryas \"the maximum abstraction of the Turing test\",[113]in which only binary responses (true/false or yes/no) are permitted, to focus only on the capacity for thought. It eliminates text chat problems likeanthropomorphism bias, and does not require emulation ofunintelligent human behaviour, allowing for systems that exceed human intelligence. The questions must each stand on their own, however, making it more like anIQ testthan an interrogation. It is typically used to gather statistical data against which the performance of artificial intelligence programs may be measured.[114]\nThe organisers of theHutter Prizebelieve that compressing natural language text is a hard AI problem, equivalent to passing the Turing test. The data compression test has some advantages over most versions and variations of a Turing test, including:[citation needed]\nThe main disadvantages of using data compression as a test are:\nA related approach to Hutter's prize which appeared much earlier in the late 1990s is the inclusion of compression problems in an extended Turing test.[115]or by tests which are completely derived fromKolmogorov complexity.[116]Other related tests in this line are presented by Hernandez-Orallo and Dowe.[117]\nAlgorithmic IQ, or AIQ for short, is an attempt to convert the theoretical Universal Intelligence Measure from Legg and Hutter (based onSolomonoff's inductive inference) into a working practical test of machine intelligence.[118]\nTwo major advantages of some of these tests are their applicability to nonhuman intelligences and their absence of a requirement for human testers.\nThe Turing test inspired theEbert testproposed in 2011 by film criticRoger Ebertwhich is a test whether a computer-basedsynthesised voicehas sufficient skill in terms of intonations, inflections, timing and so forth, to make people laugh.[119]\nTaking advantage oflarge language models, in 2023 the research companyAI21 Labscreated an online social experiment titled \"Human or Not?\"[120][121]It was played more than 10 million times by more than 2 million people.[122]It is the biggest Turing-style experiment to that date. The results showed that 32% of people could not distinguish between humans and machines.[123][124]\n1990 marked the fortieth anniversary of the first publication of Turing's \"Computing Machinery and Intelligence\" paper, and saw renewed interest in the test. Two significant events occurred in that year: the first was the Turing Colloquium, which was held at theUniversity of Sussexin April, and brought together academics and researchers from a wide variety of disciplines to discuss the Turing test in terms of its past, present, and future; the second was the formation of the annualLoebner Prizecompetition.\nBlay Whitbylists four major turning points in the history of the Turing test – the publication of \"Computing Machinery and Intelligence\" in 1950, the announcement ofJoseph Weizenbaum'sELIZAin 1966,Kenneth Colby's creation ofPARRY, which was first described in 1972, and the Turing Colloquium in 1990.[125]\nIn parallel to the 2008Loebner Prizeheld at theUniversity of Reading,[126]theSociety for the Study of Artificial Intelligence and the Simulation of Behaviour(AISB), hosted a one-day symposium to discuss the Turing test, organised byJohn Barnden,Mark Bishop,Huma ShahandKevin Warwick.[127]The speakers included the Royal Institution's DirectorBaroness Susan Greenfield,Selmer Bringsjord, Turing's biographerAndrew Hodges, and consciousness scientistOwen Holland. No agreement emerged for a canonical Turing test, though Bringsjord expressed that a sizeable prize would result in the Turing test being passed sooner."
    },
    {
        "url": "https://en.wikipedia.org/wiki/Uncanny_valley",
        "title": "Uncanny valley - Wikipedia",
        "content": "Theuncanny valley(Japanese:不気味の谷,Hepburn:bukimi no tani)effect is a hypothesized psychological and aesthetic relation between an object's degree of resemblance to a human being and the emotional response to the object. The uncanny valley hypothesis predicts that an entity appearing almost human will risk eliciting eerie feelings in viewers. Examples of the phenomenon exist amongrobots,animatronics, and lifelikedollsas well as visuals produced by3D computer animationandartificial intelligence. The increasing prevalence of digital technologies (e.g.,virtual reality,augmented reality, andphotorealisticcomputer animation) and their increasingverisimilitudehave prompted debate about the \"valley.\"\nAs related to robotics engineering, robotics professorMasahiro Morifirst introduced the concept in 1970 from his book titledBukimi No Tani(不気味の谷), phrasing it asbukimi no tani genshō(不気味の谷現象;lit.'uncanny valley phenomenon').[1]Bukimi no taniwastranslated literallyasuncanny valleyin the 1978 bookRobots: Fact, Fiction, and Predictionwritten byJasia Reichardt.[2]Over time, this translation created an unintended association of the concept toErnst Jentsch'spsychoanalyticconcept of theuncannyestablished in his 1906 essay \"On the Psychology of the Uncanny\" (German:Zur Psychologie des Unheimlichen),[3][4]which was then critiqued and extended inSigmund Freud's 1919 essay \"The Uncanny\" (German:Das Unheimliche).[5]\nMori's original hypothesis states that as the appearance of a robot is made more human, some observers' emotional response to the robot becomes increasingly positive andempathetic, until it becomes almost human, at which point the response quickly becomes strong revulsion. However, as the robot's appearance continues to become less distinguishable from that of a human being, the emotional response becomes positive once again and approaches human-to-human empathy levels.[7]When plotted on a graph, the reactions are indicated by a steep decrease followed by a steep increase (hence the \"valley\" part of the name) in the areas where anthropomorphism is closest to reality.\nThis interval of repulsive response aroused by a robot with appearance and motion between a \"somewhat human\" and \"fully human\" entity is the uncanny valley effect. The name represents the idea that an almost human-looking robot seems overly \"strange\" to some human beings, produces a feeling ofuncanniness, and thus fails to evoke the empathic response required for productivehuman–robot interaction.[7]\nA number of theories have been proposed to explain the cognitive mechanism causing the phenomenon:\nA series of studies experimentally investigated whether uncanny valley effects exist for static images of robot faces. Mathur MB & Reichling DB[18]used two complementary sets of stimuli spanning the range from very mechanical to very human-like: first, a sample of 80 objectively chosen robot face images from Internet searches, and second, a morphometrically and graphically controlled 6-face series set of faces. They asked subjects to explicitly rate the likability of each face. To measure trust toward each face, subjects completed an investment game to measure indirectly how much money they were willing to \"wager\" on a robot's trustworthiness. Both stimulus sets showed a robust uncanny valley effect on explicitly rated likability and a more context-dependent uncanny valley on implicitly rated trust. Their exploratory analysis of one proposed mechanism for the uncanny valley, perceptual confusion at a category boundary, found that category confusion occurs in the uncanny valley but does not mediate the effect on social and emotional responses.\nOne study conducted in 2009 examined the evolutionary mechanism behind the aversion associated with the uncanny valley. A group of five monkeys were shown three images: two different 3D monkey faces (realistic, unrealistic), and a real photo of a monkey's face. The monkeys' eye-gaze was used as a proxy for preference or aversion. Since the realistic 3D monkey face was looked at less than either the real photo, or the unrealistic 3D monkey face, this was interpreted as an indication that the monkey participants found the realistic 3D face aversive, or otherwise preferred the other two images. As one would expect with the uncanny valley, more realism can result in less positive reactions, and this study demonstrated that neither human-specific cognitive processes, nor human culture explain the uncanny valley. In other words, this aversive reaction to realism can be said to be evolutionary in origin.[31]\nAs of 2011[update], researchers atUniversity of California, San DiegoandCalifornia Institute for Telecommunications and Information Technologywere measuring human brain activations related to the uncanny valley.[32][33]In one study usingfMRI, a group ofcognitive scientistsandroboticistsfound the biggest differences in brain responses for uncanny robots in theparietal cortex, on both sides of the brain, specifically in the areas that connect the part of the brain'svisual cortexthat processes bodily movements with the section of themotor cortexthought to containmirror neurons. The researchers say they saw, in essence, evidence of mismatch or perceptual conflict.[13]The brain \"lit up\" when the human-like appearance of the android and its robotic motion \"didn't compute\". Ayşe Pınar Saygın, an assistant professor from UCSD, stated that \"The brain doesn't seem selectively tuned to either biological appearance or biological motion per se. What it seems to be doing is looking for its expectations to be met – for appearance and motion to be congruent.\"[15][34][35]\nViewer perception offacial expressionand speech and the uncanny valley in realistic, human-like characters intended forvideo gamesand movies is being investigated by Tinwell et al., 2011.[36]Consideration is also given by Tinwell et al. (2010) as to how the uncanny may be exaggerated for antipathetic characters in survival horror games.[37]Building on the body of work already performed for android science, this research intends to build a conceptual mapping of the uncanny valley using 3D characters generated in a real-time gaming engine. The goal is to analyze how cross-modal factors of facial expression and speech can exaggerate the uncanny. Tinwell et al., 2011[38]have also introduced the notion of an 'unscalable' uncanny wall that suggests that a viewer's discernment for detecting imperfections in realism will keep pace with new technologies in simulating realism. A summary of Angela Tinwell's research on the uncanny valley, psychological reasons behind the uncanny valley and how designers may overcome the uncanny in human-like virtual characters is provided in her book,The Uncanny Valley in Games and AnimationbyCRC Press.\nA number of design principles have been proposed for avoiding the uncanny valley:\nA number of criticisms have been raised concerning whether the uncanny valley exists as a unified phenomenon amenable to scientific scrutiny:\nIf the uncanny valley effect is the result of general cognitive processes, there should be evidence in evolutionary history and cultural artifacts.[21]An effect similar to the uncanny valley was noted byCharles Darwinin 1839:\nThe expression of this [Trigonocephalus] snake's face was hideous and fierce; the pupil consisted of a vertical slit in a mottled and coppery iris; the jaws were broad at the base, and the nose terminated in a triangular projection. I do not think I ever saw anything more ugly, excepting, perhaps, some of thevampire bats. I imagine this repulsive aspect originates from the features being placed in positions, with respect to each other, somewhat proportional to the human face; and thus we obtain a scale of hideousness.\n— Charles Darwin,The Voyage of the Beagle[52]\nA similar \"uncanny valley\" effect could, according to the ethical-futurist writer Jamais Cascio, show up when humans begin modifying themselves withtranshumanenhancements (cf.body modification), which aim to improve the abilities of the human body beyond what would normally be possible, be iteyesight,musclestrength, orcognition.[53]So long as these enhancements remain within a perceived norm of human behavior, a negative reaction is unlikely, but once individuals supplant normal human variety, revulsion can be expected. However, according to this theory, once such technologies gain further distance from human norms, \"transhuman\" individuals would cease to be judged on human levels and instead be regarded as separate entities altogether (this point is what has been dubbed \"posthuman\"), and it is here that acceptance would rise once again out of the uncanny valley.[53]Another example comes from \"pageant retouching\" photos, especially of children, which some find disturbingly doll-like.[54]\nA number of movies that usecomputer-generated imageryto show characters have been described by reviewers as giving a feeling of revulsion or \"creepiness\" as a result of the characters looking too realistic. Examples include the following:\nAn increasingly common practice is to featurevirtual actorsin movies: CGI likenesses of real actors used because the original actor either looks too old for the part or is deceased. Sometimes a virtual actor is created with involvement from the original actor (who may contribute motion capture, audio, etc.), while at other times the actor has no involvement. Reviewers have often criticized the use of virtual actors for its uncanny valley effect, saying it adds an eerie feeling to the movie. Examples of virtual actors that have received such criticism include replicas ofArnold SchwarzeneggerinTerminator Salvation(2009)[93][94]andTerminator Genisys(2015),[95]Jeff BridgesinTron: Legacy(2010),[96][97][98]Peter CushingandCarrie FisherinRogue One(2016),[99][100]andWill SmithinGemini Man(2019).[101]"
    },
    {
        "url": "https://en.wikipedia.org/wiki/History_of_artificial_intelligence",
        "title": "History of artificial intelligence - Wikipedia",
        "content": "Thehistory of artificial intelligence(AI) began inantiquity, with myths, stories, and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The study of logic and formal reasoning from antiquity to the present led directly to the invention of theprogrammable digital computerin the 1940s, a machine based on abstract mathematical reasoning. This device and the ideas behind it inspired scientists to begin discussing the possibility of building anelectronic brain.\nThe field of AI research was founded at aworkshopheld on the campus ofDartmouth Collegein 1956.[1]Attendees of the workshop became the leaders of AI research for decades. Many of them predicted that machines as intelligent as humans would exist within a generation. TheU.S. governmentprovided millions of dollars with the hope of making this vision come true.[2]\nEventually, it became obvious that researchers had grossly underestimated the difficulty of this feat.[3]In 1974, criticism fromJames Lighthilland pressure from the U.S.A. Congress led the U.S. andBritish Governmentsto stop funding undirected research into artificial intelligence. Seven years later, a visionary initiative by theJapanese Governmentand the success ofexpert systemsreinvigorated investment in AI, and by the late 1980s, the industry had grown into a billion-dollar enterprise. However, investors' enthusiasm waned in the 1990s, and the field was criticized in the press and avoided by industry (a period known as an \"AI winter\"). Nevertheless, research and funding continued to grow under other names.\nIn the early 2000s,machine learningwas applied to a wide range of problems in academia and industry. The success was due to the availability of powerful computer hardware, the collection of immense data sets, and the application of solid mathematical methods. Soon after,deep learningproved to be a breakthrough technology, eclipsing all other methods. Thetransformer architecturedebuted in 2017 and was used to produce impressivegenerative AIapplications, amongst other use cases.\nInvestment in AIboomedin the 2020s. The recent AI boom, initiated by the development of transformer architecture, led to the rapid scaling and public releases oflarge language models(LLMs) likeChatGPT. These models exhibit human-like traits of knowledge, attention, and creativity, and have been integrated into various sectors, fueling exponential investment in AI. However, concerns about the potential risks andethical implications of advanced AIhave also emerged, causing debate about the future of AI and its impact on society.\nInGreek mythology,Taloswas a creature made of bronze who acted as guardian for theisland of Crete. He would throw boulders at the ships of invaders and would complete 3 circuits around the island's perimeter daily.[4]According topseudo-Apollodorus'Bibliotheke, Hephaestus forged Talos with the aid of a cyclops and presented theautomatonas a gift toMinos.[5]In theArgonautica,Jasonand theArgonautsdefeated Talos by removing a plug near his foot, causing the vitalichorto flow out from his body and rendering him lifeless.[6]\nPygmalionwas a legendary king and sculptor of Greek mythology, famously represented inOvid'sMetamorphoses. In the 10th book of Ovid's narrative poem, Pygmalion becomes disgusted with women when he witnesses the way in which thePropoetidesprostitute themselves. Despite this, he makes offerings at the temple of Venus asking the goddess to bring to him a woman just like a statue he carved.[7]\nInOf the Nature of Things, the Swiss alchemistParacelsusdescribes a procedure that he claims can fabricate an \"artificial man\". By placing the \"sperm of a man\" in horse dung, and feeding it the \"Arcanum of Mans blood\" after 40 days, the concoction will become a living infant.[8]\nThe earliest written account regarding golem-making is found in the writings ofEleazar ben Judah of Wormsin the early 13th century.[9]During the Middle Ages, it was believed that the animation of aGolemcould be achieved by insertion of a piece of paper with any of God's names on it, into the mouth of the clay figure.[10]Unlike legendary automata likeBrazen Heads,[11]aGolemwas unable to speak.[12]\nTakwin, the artificial creation of life, was a frequent topic ofIsmailialchemical manuscripts, especially those attributed toJabir ibn Hayyan. Islamic alchemists attempted to create a broad range of life through their work, ranging from plants to animals.[13]\nInFaust: The Second Part of the TragedybyJohann Wolfgang von Goethe, an alchemically fabricatedhomunculus, destined to live forever in the flask in which he was made, endeavors to be born into a full human body. Upon the initiation of this transformation, however, the flask shatters and the homunculus dies.[14]\nBy the 19th century, ideas about artificial men and thinking machines became a popular theme in fiction. Notable works likeMary Shelley'sFrankensteinandKarel Čapek'sR.U.R. (Rossum's Universal Robots)[15]explored the concept of artificial life. Speculative essays, such asSamuel Butler's \"Darwin among the Machines\",[16]andEdgar Allan Poe's\"Maelzel's Chess Player\"[17]reflected society's growing interest in machines with artificial intelligence. AI remains a common topic in science fiction today.[18]\nRealistic humanoidautomatawere built by craftsman from many civilizations, includingYan Shi,[19]Hero of Alexandria,[20]Al-Jazari,[21]Haroun al-Rashid,[22]Jacques de Vaucanson,[23][24]Leonardo Torres y Quevedo,[25]Pierre Jaquet-DrozandWolfgang von Kempelen.[26][27]\nThe oldest known automata were thesacred statuesofancient EgyptandGreece.[28][29]The faithful believed that craftsman had imbued these figures with very real minds, capable of wisdom and emotion—Hermes Trismegistuswrote that \"by discovering the true nature of the gods, man has been able to reproduce it\".[30]English scholarAlexander Neckhamasserted that the Ancient Roman poetVirgilhad built a palace with automaton statues.[31]\nDuring the early modern period, these legendary automata were said to possess the magical ability to answer questions put to them. The late medieval alchemist and proto-ProtestantRoger Baconwas purported to have fabricated abrazen head, having developed a legend of having been a wizard.[32][33]These legends were similar to the Norse myth of the Head ofMímir. According to legend, Mímir was known for his intellect and wisdom, and was beheaded in theÆsir-Vanir War.Odinis said to have \"embalmed\" the head with herbs and spoke incantations over it such that Mímir's head remained able to speak wisdom to Odin. Odin then kept the head near him for counsel.[34]\nArtificial intelligence is based on the assumption that the process of human thought can be mechanized. The study of mechanical—or \"formal\"—reasoning has a long history.Chinese,IndianandGreekphilosophers all developed structured methods of formal deduction by the first millennium BCE. Their ideas were developed over the centuries by philosophers such asAristotle(who gave a formal analysis of thesyllogism),[35]Euclid(whoseElementswas a model of formal reasoning),al-Khwārizmī(who developedalgebraand gave his name to the wordalgorithm) and Europeanscholasticphilosophers such asWilliam of OckhamandDuns Scotus.[36]\nSpanish philosopherRamon Llull(1232–1315) developed severallogical machinesdevoted to the production of knowledge by logical means;[37][38]Llull described his machines as mechanical entities that could combine basic and undeniable truths by simple logical operations, produced by the machine by mechanical meanings, in such ways as to produce all the possible knowledge.[39]Llull's work had a great influence onGottfried Leibniz, who redeveloped his ideas.[40]\nIn the 17th century,Leibniz,Thomas HobbesandRené Descartesexplored the possibility that all rational thought could be made as systematic as algebra or geometry.[41]Hobbesfamously wrote inLeviathan: \"Forreason... is nothing butreckoning, that is adding and subtracting\".[42]Leibnizenvisioned a universal language of reasoning, thecharacteristica universalis, which would reduce argumentation to calculation so that \"there would be no more need of disputation between two philosophers than between two accountants. For it would suffice to take their pencils in hand, down to their slates, and to say each other (with a friend as witness, if they liked):Let us calculate.\"[43]These philosophers had begun to articulate thephysical symbol systemhypothesis that would guide AI research.\nThe study ofmathematical logicprovided the essential breakthrough that made artificial intelligence seem plausible. The foundations had been set by such works asBoole'sThe Laws of ThoughtandFrege'sBegriffsschrift.[44]Building onFrege's system,RussellandWhiteheadpresented a formal treatment of the foundations of mathematics in their masterpiece, thePrincipia Mathematicain 1913. Inspired byRussell's success,David Hilbertchallenged mathematicians of the 1920s and 30s to answer this fundamental question: \"can all of mathematical reasoning be formalized?\"[36]His question was answered byGödel'sincompleteness proof,[45]Turing'smachine[45]andChurch'sLambda calculus.[a]\nTheir answer was surprising in two ways. First, they proved that there were, in fact, limits to what mathematical logic could accomplish. But second (and more important for AI) their work suggested that, within these limits,anyform of mathematical reasoning could be mechanized. TheChurch-Turing thesisimplied that a mechanical device, shuffling symbols as simple as0and1, could imitate any conceivable process of mathematical deduction.[45]The key insight was theTuring machine—a simple theoretical construct that captured the essence of abstract symbol manipulation.[48]This invention would inspire a handful of scientists to begin discussing the possibility of thinking machines.\nCalculating machines were designed or built in antiquity and throughout history by many people, includingGottfried Leibniz,[38][49]Joseph Marie Jacquard,[50]Charles Babbage,[50][51]Percy Ludgate,[52]Leonardo Torres Quevedo,[53]Vannevar Bush,[54]and others.Ada Lovelacespeculated that Babbage's machine was \"a thinking or ... reasoning machine\", but warned \"It is desirable to guard against the possibility of exaggerated ideas that arise as to the powers\" of the machine.[55][56]\nThe first modern computers were the massive machines of theSecond World War(such asKonrad Zuse'sZ3,Alan Turing'sHeath RobinsonandColossus,AtanasoffandBerry'sABC, andENIACat theUniversity of Pennsylvania).[57]ENIACwas based on the theoretical foundation laid byAlan Turingand developed byJohn von Neumann,[58]and proved to be the most influential.[57]\nThe earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 1930s, 1940s, and early 1950s. Recent research inneurologyhad shown that the brain was an electrical network ofneuronsthat fired in all-or-nothing pulses.Norbert Wiener'scyberneticsdescribed control and stability in electrical networks.Claude Shannon'sinformation theorydescribed digital signals (i.e., all-or-nothing signals).Alan Turing'stheory of computationshowed that any form of computation could be described digitally. The close relationship between these ideas suggested that it might be possible to construct an \"electronic brain\".\nIn the 1940s and 50s, a handful of scientists from a variety of fields (mathematics, psychology, engineering, economics and political science) explored several research directions that would be vital to later AI research.[59]Alan Turing was among the first people to seriously investigate the theoretical possibility of \"machine intelligence\".[60]The field of \"artificial intelligence research\" was founded as an academic discipline in 1956.[61]\nIn 1950 Turing published a landmark paper \"Computing Machinery and Intelligence\", in which he speculated about the possibility of creating machines that think.[63][b]In the paper, he noted that \"thinking\" is difficult to define and devised his famousTuring test: If a machine could carry on a conversation (over ateleprinter) that was indistinguishable from a conversation with a human being, then it was reasonable to say that the machine was \"thinking\".[64]This simplified version of the problem allowed Turing to argue convincingly that a \"thinking machine\" was at leastplausibleand the paper answered all the most common objections to the proposition.[65]The Turing test was the first serious proposal in thephilosophy of artificial intelligence.\nDonald Hebbwas a Canadian psychologist whose work laid the foundation for modern neuroscience, particularly in understanding learning, memory, and neural plasticity. His most influential book,The Organization of Behavior(1949), introduced the concept of Hebbian learning, often summarized as \"cells that fire together wire together.\"[66]\nHebb began formulating the foundational ideas for this book in the early 1940s, particularly during his time at the Yerkes Laboratories of Primate Biology from 1942 to 1947. He made extensive notes between June 1944 and March 1945 and sent a complete draft to his mentor Karl Lashley in 1946. The manuscript forThe Organization of Behaviorwasn’t published until 1949. The delay was due to various factors, including World War II and shifts in academic focus. By the time it was published, several of his peers had already published related ideas, making Hebb's work seem less groundbreaking at first glance. However, his synthesis of psychological and neurophysiological principles became a cornerstone of neuroscience and machine learning.[67][68]\nWalter PittsandWarren McCullochanalyzed networks of idealizedartificial neuronsand showed how they might perform simple logical functions in 1943. They were the first to describe what later researchers would call aneural network.[69]The paper was influenced by Turing's paper \"On Computable Numbers\" from 1936 using similar two-state boolean 'neurons', but was the first to apply it to neuronal function.[60]One of the students inspired by Pitts and McCulloch wasMarvin Minskywho was a 24-year-old graduate student at the time. In 1951 Minsky and Dean Edmonds built the first neural net machine, theSNARC.[70]Minsky would later become one of the most important leaders and innovators in AI.\nExperimental robots such asW. Grey Walter'sturtlesand theJohns Hopkins Beast, were built in the 1950s. These machines did not use computers, digital electronics, or symbolic reasoning; they were controlled entirely by analog circuitry.[71]\nIn 1951, using theFerranti Mark 1machine of theUniversity of Manchester,Christopher Stracheywrote a checkers program[72]andDietrich Prinzwrote one for chess.[73]Arthur Samuel's checkers program, the subject of his 1959 paper \"Some Studies in Machine Learning Using the Game of Checkers\", eventually achieved sufficient skill to challenge a respectable amateur.[74]Samuel's program was among the first uses of what would later be calledmachine learning.[75]Game AIwould continue to be used as a measure of progress in AI throughout its history.\nWhen access todigital computersbecame possible in the mid-fifties, a few scientists instinctively recognized that a machine that could manipulate numbers could also manipulate symbols and that the manipulation of symbols could well be the essence of human thought. This was a new approach to creating thinking machines.[76][77]\nIn 1955,Allen Newelland future Nobel LaureateHerbert A. Simoncreated the \"Logic Theorist\", with help fromJ. C. Shaw. The program would eventually prove 38 of the first 52 theorems inRussellandWhitehead'sPrincipia Mathematica, and find new and more elegant proofs for some.[78]Simon said that they had \"solved the venerablemind/body problem, explaining how a system composed of matter can have the properties of mind.\"[79][c]The symbolic reasoning paradigm they introduced would dominate AI research and funding until the middle 90s, as well as inspire thecognitive revolution.\nThe Dartmouth workshop of 1956 was a pivotal event that marked the formal inception of AI as an academic discipline.[61]It was organized byMarvin MinskyandJohn McCarthy, with the support of two senior scientistsClaude ShannonandNathan RochesterofIBM. The proposal for the conference stated they intended to test the assertion that \"every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it\".[80][d]The term \"Artificial Intelligence\" was introduced by John McCarthy at the workshop.[e]The participants includedRay Solomonoff,Oliver Selfridge,Trenchard More,Arthur Samuel,Allen NewellandHerbert A. Simon, all of whom would create important programs during the first decades of AI research.[86][f]At the workshop Newell and Simon debuted the \"Logic Theorist\".[87]The workshop was the moment that AI gained its name, its mission, its first major success and its key players, and is widely considered the birth of AI.[g]\nIn the autumn of 1956, Newell and Simon also presented the Logic Theorist at a meeting of the Special Interest Group in Information Theory at theMassachusetts Institute of Technology(MIT). At the same meeting,Noam Chomskydiscussed hisgenerative grammar, andGeorge Millerdescribed his landmark paper \"The Magical Number Seven, Plus or Minus Two\". Miller wrote \"I left the symposium with a conviction, more intuitive than rational, that experimental psychology, theoretical linguistics, and the computer simulation of cognitive processes were all pieces from a larger whole.\"[89][57]\nThis meeting was the beginning of the \"cognitive revolution\"—an interdisciplinaryparadigm shiftin psychology, philosophy, computer science and neuroscience. It inspired the creation of the sub-fields ofsymbolic artificial intelligence,generative linguistics,cognitive science,cognitive psychology,cognitive neuroscienceand the philosophical schools ofcomputationalismandfunctionalism. All these fields used related tools to model the mind and results discovered in one field were relevant to the others.\nThe cognitive approach allowed researchers to consider \"mental objects\" like thoughts, plans, goals, facts or memories, often analyzed usinghigh level symbolsin functional networks. These objects had been forbidden as \"unobservable\" by earlier paradigms such asbehaviorism.[h]Symbolic mental objects would become the major focus of AI research and funding for the next several decades.\nThe programs developed in the years after theDartmouth Workshopwere, to most people, simply \"astonishing\":[i]computers were solving algebra word problems, proving theorems in geometry and learning to speak English. Few at the time would have believed that such \"intelligent\" behavior by machines was possible at all.[93][94][92]Researchers expressed an intense optimism in private and in print, predicting that a fully intelligent machine would be built in less than 20 years.[95]Government agencies like theDefense Advanced Research Projects Agency(DARPA, then known as \"ARPA\") poured money into the field.[96]Artificial Intelligence laboratories were set up at a number of British and US universities in the latter 1950s and early 1960s.[60]\nThere were many successful programs and new directions in the late 50s and 1960s. Among the most influential were these:\nMany early AI programs used the same basicalgorithm. To achieve some goal (like winning a game or proving a theorem), they proceeded step by step towards it (by making a move or a deduction) as if searching through a maze,backtrackingwhenever they reached a dead end.[97]The principal difficulty was that, for many problems, the number of possible paths through the \"maze\" was astronomical (a situation known as a \"combinatorial explosion\"). Researchers would reduce the search space by usingheuristicsthat would eliminate paths that were unlikely to lead to a solution.[98]\nNewellandSimontried to capture a general version of this algorithm in a program called the \"General Problem Solver\".[99][100]Other \"searching\" programs were able to accomplish impressive tasks like solving problems in geometry and algebra, such asHerbert Gelernter's Geometry Theorem Prover (1958)[101]and Symbolic Automatic Integrator (SAINT), written byMinsky'sstudent James Slagle in 1961.[102][103]Other programs searched through goals and subgoals toplan actions, like theSTRIPSsystem developed atStanfordto control the behavior of the robotShakey.[104]\nAn important goal of AI research is to allow computers to communicate innatural languageslike English. An early success wasDaniel Bobrow's programSTUDENT, which could solve high school algebra word problems.[105]\nAsemantic netrepresents concepts (e.g. \"house\", \"door\") as nodes, and relations among concepts as links between the nodes (e.g. \"has-a\"). The first AI program to use a semantic net was written by Ross Quillian[106]and the most successful (and controversial) version wasRoger Schank'sConceptual dependency theory.[107]\nJoseph Weizenbaum'sELIZAcould carry out conversations that were so realistic that users occasionally were fooled into thinking they were communicating with a human being and not a computer program (seeELIZA effect). But in fact, ELIZA simply gave acanned responseor repeated back what was said to it, rephrasing its response with a few grammar rules. ELIZA was the firstchatbot.[108][109]\nIn the late 60s,Marvin MinskyandSeymour Papertof theMITAI Laboratory proposed that AI research should focus on artificially simple situations known as micro-worlds.[j]They pointed out that in successful sciences like physics, basic principles were often best understood using simplified models like frictionless planes or perfectly rigid bodies. Much of the research focused on a \"blocks world,\" which consists of colored blocks of various shapes and sizes arrayed on a flat surface.[110]\nThis paradigm led to innovative work inmachine visionbyGerald Sussman, Adolfo Guzman,David Waltz(who invented \"constraint propagation\"), and especiallyPatrick Winston. At the same time, Minsky and Papert built a robot arm that could stack blocks, bringing the blocks world to life.Terry Winograd'sSHRDLUcould communicate in ordinary English sentences about the micro-world, plan operations and execute them.[110]\nIn the 1960s funding was primarily directed towards laboratories researchingsymbolic AI, however several people still pursued research in neural networks.\nTheperceptron, a single-layerneural networkwas introduced in 1958 byFrank Rosenblatt[111](who had been a schoolmate ofMarvin Minskyat theBronx High School of Science).[112]Like most AI researchers, he was optimistic about their power, predicting that a perceptron \"may eventually be able to learn, make decisions, and translate languages.\"[113]Rosenblatt was primarily funded byOffice of Naval Research.[114]\nBernard Widrowand his studentTed HoffbuiltADALINE(1960) andMADALINE(1962), which had up to 1000 adjustable weights.[115][116]A group atStanford Research Instituteled byCharles A. Rosenand Alfred E. (Ted) Brain built two neural network machines named MINOS I (1960) and II (1963), mainly funded byU.S. Army Signal Corps. MINOS II[117]had 6600 adjustable weights,[118]and was controlled with anSDS 910 computerin a configuration named MINOS III (1968), which could classify symbols on army maps, and recognize hand-printed characters onFortrancoding sheets.[119][120]Most of neural network research during this early period involved building and using bespoke hardware, rather than simulation on digital computers.[k]\nHowever, partly due to lack of results and partly due to competition fromsymbolic AIresearch, the MINOS project ran out of funding in 1966. Rosenblatt failed to secure continued funding in the 1960s.[121]In 1969 research came to a sudden halt with the publication ofMinskyandPapert's1969 bookPerceptrons.[122]It suggested that there were severe limitations to what perceptrons could do and that Rosenblatt's predictions had been grossly exaggerated. The effect of the book was that virtually no research was funded inconnectionismfor 10 years.[123]The competition for government funding ended with the victory of symbolic AI approaches over neural networks.[120][121]\nMinsky (who had worked onSNARC) became a staunch objector to pure connectionist AI.Widrow(who had worked onADALINE) turned to adaptive signal processing. TheSRIgroup (which worked on MINOS) turned to symbolic AI and robotics.[120][121]\nThe main problem was the inability to train multilayered networks (versions ofbackpropagationhad already been used in other fields but it was unknown to these researchers).[124][123]The AI community became aware of backpropogation in the 80s,[125]and, in the 21st century, neural networks would become enormously successful, fulfilling all of Rosenblatt's optimistic predictions. Rosenblatt did not live to see this, however, as he died in a boating accident in 1971.[126]\nThe first generation of AI researchers made these predictions about their work:\nIn June 1963,MITreceived a $2.2 million grant from the newly created Advanced Research Projects Agency (ARPA, later known asDARPA). The money was used to fundproject MACwhich subsumed the \"AI Group\" founded byMinskyandMcCarthyfive years earlier. DARPA continued to provide $3 million each year until the 70s.[133]DARPA made similar grants toNewellandSimon'sprogram atCarnegie Mellon Universityand toStanford University'sAI Lab, founded byJohn McCarthyin 1963.[134]Another important AI laboratory was established atEdinburgh UniversitybyDonald Michiein 1965.[135]These four institutions would continue to be the main centers of AI research and funding in academia for many years.[136][m]\nThe money was given with few strings attached:J. C. R. Licklider, then the director of ARPA, believed that his organization should \"fund people, not projects!\" and allowed researchers to pursue whatever directions might interest them.[138]This created a freewheeling atmosphere at MIT that gave birth to thehacker culture,[139]but this \"hands off\" approach did not last.\nIn the 1970s, AI was subject to critiques and financial setbacks. AI researchers had failed to appreciate the difficulty of the problems they faced. Their tremendous optimism had raised public expectations impossibly high, and when the promised results failed to materialize, funding targeted at AI was severely reduced.[140]The lack of success indicated the techniques being used by AI researchers at the time were insufficient to achieve their goals.[141][142]\nThese setbacks did not affect the growth and progress of the field, however. The funding cuts only impacted a handful of major laboratories[143]and the critiques were largely ignored.[144]General public interest in the field continued to grow,[143]the number of researchers increased dramatically,[143]and new ideas were explored inlogic programming,commonsense reasoningand many other areas. Historian Thomas Haigh argued in 2023 that there was no winter,[143]and AI researcherNils Nilssondescribed this period as the most \"exciting\" time to work in AI.[145]\nIn the early seventies, the capabilities of AI programs were limited. Even the most impressive could only handle trivial versions of the problems they were supposed to solve;[n]all the programs were, in some sense, \"toys\".[147]AI researchers had begun to run into several limits that would be only conquered decades later, and others that still stymie the field in the 2020s:\nThe agencies which funded AI research, such as theBritish government,DARPAand theNational Research Council(NRC) became frustrated with the lack of progress and eventually cut off almost all funding for undirected AI research. The pattern began in 1966 when theAutomatic Language Processing Advisory Committee(ALPAC) report criticized machine translation efforts. After spending $20 million, theNRCended all support.[157]In 1973, theLighthill reporton the state of AI research in the UK criticized the failure of AI to achieve its \"grandiose objectives\" and led to the dismantling of AI research in that country.[158](The report specifically mentioned thecombinatorial explosionproblem as a reason for AI's failings.)[142][146][s]DARPA was deeply disappointed with researchers working on theSpeech Understanding Researchprogram at CMU and canceled an annual grant of $3 million.[160][t]\nHans Moravecblamed the crisis on the unrealistic predictions of his colleagues. \"Many researchers were caught up in a web of increasing exaggeration.\"[161][u]However, there was another issue: since the passage of theMansfield Amendmentin 1969,DARPAhad been under increasing pressure to fund \"mission-oriented direct research, rather than basic undirected research\". Funding for the creative, freewheeling exploration that had gone on in the 60s would not come from DARPA, which instead directed money at specific projects with clear objectives, such asautonomoustanks andbattle managementsystems.[162][v]\nThe major laboratories (MIT, Stanford, CMU and Edinburgh) had been receiving generous support from their governments, and when it was withdrawn, these were the only places that were seriously impacted by the budget cuts. The thousands of researchers outside these institutions and the many more thousands that were joining the field were unaffected.[143]\nSeveral philosophers had strong objections to the claims being made by AI researchers. One of the earliest wasJohn Lucas, who argued thatGödel's incompleteness theoremshowed that aformal system(such as a computer program) could never see the truth of certain statements, while a human being could.[164]Hubert Dreyfusridiculed the broken promises of the 1960s and critiqued the assumptions of AI, arguing that human reasoning actually involved very little \"symbol processing\" and a great deal ofembodied,instinctive, unconscious \"know how\".[w][166]John Searle'sChinese Roomargument, presented in 1980, attempted to show that a program could not be said to \"understand\" the symbols that it uses (a quality called \"intentionality\"). If the symbols have no meaning for the machine, Searle argued, then the machine can not be described as \"thinking\".[167]\nThese critiques were not taken seriously by AI researchers. Problems likeintractabilityandcommonsense knowledgeseemed much more immediate and serious. It was unclear what difference \"know how\" or \"intentionality\" made to an actual computer program. MIT'sMinskysaid of Dreyfus and Searle \"they misunderstand, and should be ignored.\"[168]Dreyfus, who also taught atMIT, was given a cold shoulder: he later said that AI researchers \"dared not be seen having lunch with me.\"[169]Joseph Weizenbaum, the author ofELIZA, was also an outspoken critic of Dreyfus' positions, but he \"deliberately made it plain that [his AI colleagues' treatment of Dreyfus] was not the way to treat a human being,\"[x]and was unprofessional and childish.[171]\nWeizenbaum began to have serious ethical doubts about AI whenKenneth Colbywrote a \"computer program which can conductpsychotherapeuticdialogue\" based on ELIZA.[172][173][y]Weizenbaum was disturbed that Colby saw a mindless program as a serious therapeutic tool. A feud began, and the situation was not helped when Colby did not credit Weizenbaum for his contribution to the program. In 1976,WeizenbaumpublishedComputer Power and Human Reasonwhich argued that the misuse of artificial intelligence has the potential to devalue human life.[175]\nLogic was introduced into AI research as early as 1958, byJohn McCarthyin hisAdvice Takerproposal.[176][101]In 1963,J. Alan Robinsonhad discovered a simple method to implement deduction on computers, theresolutionandunificationalgorithm.[101]However, straightforward implementations, like those attempted by McCarthy and his students in the late 1960s, were especially intractable: the programs required astronomical numbers of steps to prove simple theorems.[176][177]A more fruitful approach to logic was developed in the 1970s byRobert Kowalskiat theUniversity of Edinburgh, and soon this led to the collaboration with French researchersAlain ColmerauerandPhilippe Roussel[fr]who created the successful logic programming languageProlog.[178]Prolog uses a subset of logic (Horn clauses, closely related to \"rules\" and \"production rules\") that permit tractable computation. Rules would continue to be influential, providing a foundation forEdward Feigenbaum'sexpert systemsand the continuing work byAllen NewellandHerbert A. Simonthat would lead toSoarand theirunified theories of cognition.[179]\nCritics of the logical approach noted, asDreyfushad, that human beings rarely used logic when they solved problems. Experiments by psychologists likePeter Wason,Eleanor Rosch,Amos Tversky,Daniel Kahnemanand others provided proof.[z]McCarthy responded that what people do is irrelevant. He argued that what is really needed are machines that can solve problems—not machines that think as people do.[aa]\nAmong the critics ofMcCarthy'sapproach were his colleagues across the country atMIT.Marvin Minsky,Seymour PapertandRoger Schankwere trying to solve problems like \"story understanding\" and \"object recognition\" thatrequireda machine to think like a person. In order to use ordinary concepts like \"chair\" or \"restaurant\" they had to make all the same illogical assumptions that people normally made. Unfortunately, imprecise concepts like these are hard to represent in logic. MIT chose instead to focus on writing programs that solved a given task without using high-level abstract definitions or general theories of cognition, and measured performance by iterative testing, rather than arguments from first principles.Schankdescribed their \"anti-logic\" approaches asscruffy, as opposed to theneatparadigm used byMcCarthy,Kowalski,Feigenbaum,NewellandSimon.[180][ab]\nIn 1975, in a seminal paper,Minskynoted that many of his fellow researchers were using the same kind of tool: a framework that captures all ourcommon sense assumptionsabout something. For example, if we use the concept of a bird, there is a constellation of facts that immediately come to mind: we might assume that it flies, eats worms and so on (none of which are true for all birds). Minsky associated these assumptions with the general category and they could beinheritedby the frames for subcategories and individuals, or over-ridden as necessary. He called these structuresframes.Schankused a version of frames he called \"scripts\" to successfully answer questions about short stories in English.[181]Frames would eventually be widely used insoftware engineeringunder the nameobject-oriented programming.\nThe logicians rose to the challenge.Pat Hayesclaimed that \"most of 'frames' is just a new syntax for parts of first-order logic.\" But he noted that \"there are one or two apparently minor details which give a lot of trouble, however, especially defaults\".[182]\nRay Reiteradmitted that \"conventional logics, such as first-order logic, lack the expressive power to adequately represent the knowledge required for reasoning by default\".[183]He proposed augmenting first-order logic with aclosed world assumptionthat a conclusion holds (by default) if its contrary cannot be shown. He showed how such an assumption corresponds to the common sense assumption made in reasoning with frames. He also showed that it has its \"procedural equivalent\" asnegation as failureinProlog. The closed world assumption, as formulated by Reiter, \"is not a first-order notion. (It is a meta notion.)\"[183]However,Keith Clarkshowed that negation asfinite failurecan be understood as reasoning implicitly with definitions in first-order logic including aunique name assumptionthat different terms denote different individuals.[184]\nDuring the late 1970s and throughout the 1980s, a variety of logics and extensions of first-order logic were developed both for negation as failure inlogic programmingand for default reasoning more generally. Collectively, these logics have become known asnon-monotonic logics.\nIn the 1980s, a form of AI program called \"expert systems\" was adopted by corporations around the world andknowledgebecame the focus of mainstream AI research. Governments provided substantial funding, such as Japan'sfifth generation computerproject and the U.S.Strategic Computing Initiative. \"Overall, the AI industry boomed from a few million dollars in 1980 to billions of dollars in 1988.\"[125]\nAnexpert systemis a program that answers questions or solves problems about a specific domain of knowledge, using logicalrulesthat are derived from the knowledge of experts.[185]The earliest examples were developed byEdward Feigenbaumand his students.Dendral, begun in 1965, identified compounds from spectrometer readings.[186][123]MYCIN, developed in 1972, diagnosed infectious blood diseases.[125]They demonstrated the feasibility of the approach.\nExpert systems restricted themselves to a small domain of specific knowledge (thus avoiding thecommonsense knowledgeproblem)[123]and their simple design made it relatively easy for programs to be built and then modified once they were in place. All in all, the programs proved to beuseful: something that AI had not been able to achieve up to this point.[187]\nIn 1980, an expert system calledR1was completed atCMUfor theDigital Equipment Corporation. It was an enormous success: it was saving the company 40 million dollars annually by 1986.[188]Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments.[189]An industry grew up to support them, including hardware companies likeSymbolicsandLisp Machinesand software companies such asIntelliCorpandAion.[190]\nIn 1981, theJapanese Ministry of International Trade and Industryset aside $850 million for theFifth generation computerproject. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings.[191]Much to the chagrin ofscruffies, they initially chosePrologas the primary computer language for the project.[192]\nOther countries responded with new programs of their own. The UK began the £350 millionAlveyproject.[193]A consortium of American companies formed theMicroelectronics and Computer Technology Corporation(or \"MCC\") to fund large scale projects in AI and information technology.[194][193]DARPAresponded as well, founding theStrategic Computing Initiativeand tripling its investment in AI between 1984 and 1988.[195][196]\nThe power of expert systems came from the expert knowledge they contained. They were part of a new direction in AI research that had been gaining ground throughout the 70s. \"AI researchers were beginning to suspect—reluctantly, for it violated the scientific canon ofparsimony—that intelligence might very well be based on the ability to use large amounts of diverse knowledge in different ways,\"[197]writesPamela McCorduck. \"[T]he great lesson from the 1970s was that intelligent behavior depended very much on dealing with knowledge, sometimes quite detailed knowledge, of a domain where a given task lay\".[198]Knowledge based systemsandknowledge engineeringbecame a major focus of AI research in the 1980s.[199]It was hoped that vast databases would solve thecommonsense knowledgeproblem and provide the support thatcommonsense reasoningrequired.\nIn the 1980s some researchers attempted to attack thecommonsense knowledge problemdirectly, by creating a massive database that would contain all the mundane facts that the average person knows.Douglas Lenat, who started a database calledCyc, argued that there is no shortcut―the only way for machines to know the meaning of human concepts is to teach them, one concept at a time, by hand.[200]\nAlthough symbolicknowledge representationandlogical reasoningproduced useful applications in the 80s and received massive amounts of funding, it was still unable to solve problems inperception,robotics,learningandcommon sense. A small number of scientists and engineers began to doubt that the symbolic approach would ever be sufficient for these tasks and developed other approaches, such as \"connectionism\",robotics,\"soft\" computingandreinforcement learning.Nils Nilssoncalled these approaches \"sub-symbolic\".\nIn 1982, physicistJohn Hopfieldwas able to prove that a form of neural network (now called a \"Hopfield net\") could learn and process information, and provably converges after enough time under any fixed condition. It was a breakthrough, as it was previously thought that nonlinear networks would, in general, evolve chaotically.[201]Around the same time,Geoffrey HintonandDavid Rumelhartpopularized a method for training neural networks called \"backpropagation\".[ac]These two developments helped to revive the exploration ofartificial neural networks.[125][202]\nNeural networks, along with several other similar models, received widespread attention after the 1986 publication of theParallel Distributed Processing, a two volume collection of papers edited byRumelhartand psychologistJames McClelland. The new field was christened \"connectionism\" and there was a considerable debate between advocates ofsymbolic AIand the \"connectionists\".[125]Hinton called symbols the \"luminous aetherof AI\"―that is, an unworkable and misleading model of intelligence.[125]This was a direct attack on the principles that inspired thecognitive revolution.\nNeural networks started to advance state of the art in some specialist areas such as protein structure prediction. Following pioneering work from Terry Sejnowski,[203]cascading multilayer perceptrons such as PhD[204]and PsiPred[205]reached near-theoretical maximum accuracy in predicting secondary structure.\nIn 1990,Yann LeCunatBell Labsusedconvolutional neural networksto recognize handwritten digits. The system was used widely in 90s, reading zip codes and personal checks. This was the first genuinely useful application of neural networks.[206][207]\nRodney Brooks,Hans Moravecand others argued that, in order to show real intelligence, a machine needs to have abody—it needs to perceive, move, survive, and deal with the world.[208]Sensorimotor skills are essential to higher level skills such ascommonsense reasoning. They can't be efficiently implemented using abstract symbolic reasoning, so AI should solve the problems of perception, mobility, manipulation and survival without using symbolic representation at all. These robotics researchers advocated building intelligence \"from the bottom up\".[ad]\nA precursor to this idea wasDavid Marr, who had come toMITin the late 1970s from a successful background in theoretical neuroscience to lead the group studyingvision. He rejected all symbolic approaches (bothMcCarthy'slogic andMinsky's frames), arguing that AI needed to understand the physical machinery of vision from the bottom up before any symbolic processing took place. (Marr's work would be cut short by leukemia in 1980.)[210]\nIn his 1990 paper \"Elephants Don't Play Chess\",[211]robotics researcher Brooks took direct aim at thephysical symbol system hypothesis, arguing that symbols are not always necessary since \"the world is its own best model. It is always exactly up to date. It always has every detail there is to be known. The trick is to sense it appropriately and often enough.\"[212]\nIn the 1980s and 1990s, manycognitive scientistsalso rejected the symbol processing model of the mind and argued that the body was essential for reasoning, a theory called the \"embodied mindthesis\".[213]\nSoft computinguses methods that work with incomplete and imprecise information. They do not attempt to give precise, logical answers, but give results that are only \"probably\" correct. This allowed them to solve problems that precise symbolic methods could not handle. Press accounts often claimed these tools could \"think like a human\".[214][215]\nJudea Pearl'sProbabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, an influential 1988 book[216]broughtprobabilityanddecision theoryinto AI.[217]Fuzzy logic, developed byLofti Zadehin the 60s, began to be more widely used in AI and robotics.Evolutionary computationandartificial neural networksalso handle imprecise information, and are classified  as \"soft\". In the 90s and early 2000s many other soft computing tools were developed and put into use, includingBayesian networks,[217]hidden Markov models,[217]information theory, andstochastic modeling. These tools in turn depended on advanced mathematical techniques such as classicaloptimization. For a time in the 1990s and early 2000s, these soft tools were studied by a subfield of AI called \"computational intelligence\".[218]\nReinforcement learning[219]gives an agent a reward every time it performs a desired action well, and may give negative rewards (or \"punishments\") when it performs poorly. It was described in the first half of the twentieth century by psychologists using animal models, such asThorndike,[220][221]Pavlov[222]andSkinner.[223]In the 1950s,Alan Turing[221][224]andArthur Samuel[221]foresaw the role of reinforcement learning in AI.\nA successful and influential research program was led byRichard SuttonandAndrew Bartobeginning 1972. Their collaboration revolutionized the study of reinforcement learning and decision making over the four decades.[225][226]In 1988, Sutton described machine learning in terms ofdecision theory(i.e., theMarkov decision process). This gave the subject a solid theoretical foundation and access to a large body of theoretical results developed in the field ofoperations research.[226]\nAlso in 1988, Sutton and Barto developed the \"temporal difference\" (TD) learning algorithm, where the agent is rewarded only when itspredictions about the futureshow improvement. It significantly outperformed previous algorithms.[227]TD-learning was used by Gerald Tesauro in 1992 in the programTD-Gammon, which played backgammon as well as the best human players. The program learned the game by playing against itself with zero prior knowledge.[228]In an interesting case of interdisciplinary convergence, neurologists discovered in 1997 that thedopamine reward systemin brains also uses a version of the TD-learning algorithm.[229][230][231]TD learning would be become highly influential in the 21st century, used in bothAlphaGoandAlphaZero.[232]\nThe business community's fascination with AI rose and fell in the 1980s in the classic pattern of aneconomic bubble. As dozens of companies failed, the perception in the business world was that the technology was not viable.[233]The damage to AI's reputation would last into the 21st century. Inside the field there was little agreement on the reasons for AI's failure to fulfill the dream of human level intelligence that had captured the imagination of the world in the 1960s. Together, all these factors helped to fragment AI into competing subfields focused on particular problems or approaches, sometimes even under new names that disguised the tarnished pedigree of \"artificial intelligence\".[234]\nOver the next 20 years, AI consistently delivered working solutions to specific isolated problems. By the late 1990s, it was being used throughout the technology industry, although somewhat behind the scenes. The success was due toincreasing computer power, by collaboration with other fields (such asmathematical optimizationandstatistics) and using higher standards of scientific accountability.\nThe term \"AI winter\" was coined by researchers who had survived the funding cuts of 1974 when they became concerned that enthusiasm for expert systems had spiraled out of control and that disappointment would certainly follow.[ae]Their fears were well founded: in the late 1980s and early 1990s, AI suffered a series of financial setbacks.[125]\nThe first indication of a change in weather was the sudden collapse of the market for specialized AI hardware in 1987. Desktop computers fromAppleandIBMhad been steadily gaining speed and power and in 1987 they became more powerful than the more expensiveLisp machinesmade bySymbolicsand others. There was no longer a good reason to buy them. An entire industry worth half a billion dollars was demolished overnight.[236]\nEventually the earliest successful expert systems, such asXCON, proved too expensive to maintain. They were difficult to update, they could not learn, and they were \"brittle\" (i.e., they could make grotesque mistakes when given unusual inputs). Expert systems proved useful, but only in a few special contexts.[237]\nIn the late 1980s, theStrategic Computing Initiativecut funding to AI \"deeply and brutally\". New leadership atDARPAhad decided that AI was not \"the next wave\" and directed funds towards projects that seemed more likely to produce immediate results.[238]\nBy 1991, the impressive list of goals penned in 1981 for Japan'sFifth Generation Projecthad not been met. Some of them, like \"carry on a casual conversation\", would not be accomplished for another 30 years. As with other AI projects, expectations had run much higher than what was actually possible.[239][af]\nOver 300 AI companies had shut down, gone bankrupt, or been acquired by the end of 1993, effectively ending the first commercial wave of AI.[241]In 1994,HP Newquiststated inThe Brain Makersthat \"The immediate future of artificial intelligence—in its commercial form—seems to rest in part on the continued success of neural networks.\"[241]\nIn the 1990s, algorithms originally developed by AI researchers began to appear as parts of larger systems. AI had solved a lot of very difficult problems[ag]and their solutions proved to be useful throughout the technology industry,[242][243]such asdata mining,industrial robotics, logistics,speech recognition,[244]banking software,[245]medical diagnosis,[245]andGoogle's search engine.[246][247]\nThe field of AI received little or no credit for these successes in the 1990s and early 2000s. Many of AI's greatest innovations have been reduced to the status of just another item in the tool chest of computer science.[248]Nick Bostromexplains: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"[245]\nMany researchers in AI in the 1990s deliberately called their work by other names, such asinformatics,knowledge-based systems, \"cognitive systems\" orcomputational intelligence. In part, this may have been because they considered their field to be fundamentally different from AI, but also the new names help to procure funding.[244][249][250]In the commercial world at least, the failed promises of the AI winter continued to haunt AI research into the 2000s, as theNew York Timesreported in 2005: \"Computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers.\"[251]\nAI researchers began to develop and use sophisticated mathematical tools more than they ever had in the past.[252][253]Most of the new directions in AI relied heavily on mathematical models, includingartificial neural networks,probabilistic reasoning,soft computingandreinforcement learning. In the 90s and 2000s, many other highly mathematical tools were adapted for AI. These tools were applied to machine learning, perception, and mobility.\nThere was a widespread realization that many of the problems that AI needed to solve were already being worked on by researchers in fields likestatistics,mathematics,electrical engineering,economics, oroperations research. The shared mathematical language allowed both a higher level of collaboration with more established and successful fields and the achievement of results which were measurable and provable; AI had become a more rigorous \"scientific\" discipline. Another key reason for the success in the 90s was that AI researchers focused on specific problems with verifiable solutions (an approach later derided asnarrow AI). This provided useful tools in the present, rather than speculation about the future.\nA new paradigm called \"intelligent agents\" became widely accepted during the 1990s.[254][255][ah]Although earlier researchers had proposed modular \"divide and conquer\" approaches to AI,[ai]the intelligent agent did not reach its modern form untilJudea Pearl,Allen Newell,Leslie P. Kaelbling, and others brought concepts fromdecision theoryand economics into the study of AI.[256]When theeconomist'sdefinition of arational agentwas married tocomputer science's definition of anobjectormodule, the intelligent agent paradigm was complete.\nAnintelligent agentis a system that perceives its environment and takes actions which maximize its chances of success. By this definition, simple programs that solve specific problems are \"intelligent agents\", as are human beings and organizations of human beings, such asfirms. The intelligent agent paradigm defines AI research as \"the study of intelligent agents\".[aj]This is a generalization of some earlier definitions of AI: it goes beyond studying human intelligence; it studies all kinds of intelligence. The paradigm gave researchers license to study isolated problems and to disagree about methods, but still retain hope that their work could be combined into anagent architecturethat would be capable of general intelligence.[257]\nOn 11 May 1997,Deep Bluebecame the first computer chess-playing system to beat a reigning world chess champion,Garry Kasparov.[258]In 2005, a Stanford robot won theDARPA Grand Challengeby driving autonomously for 131 miles along an unrehearsed desert trail. Two years later, a team from CMU won theDARPA Urban Challengeby autonomously navigating 55 miles in an urban environment while responding to traffic hazards and adhering to traffic laws.[259]\nThese successes were not due to some revolutionary new paradigm, but mostly on the tedious application of engineering skill and on the tremendous increase in the speed and capacity of computers by the 90s.[ak]In fact,Deep Blue'scomputer was 10 million times faster than theFerranti Mark 1thatChristopher Stracheytaught to play chess in 1951.[al]This dramatic increase is measured byMoore's law, which predicts that the speed and memory capacity of computers doubles every two years. The fundamental problem of \"raw computer power\" was slowly being overcome.\nElectronic literatureexperiments such asThe Impermanence Agent(1998–2002) and digital art such asAgent Rubyused AI in their art and literature, \"laying bare the bias accompanying forms of technology that feign objectivity.\"[261]\nIn the first decades of the 21st century, access to large amounts of data (known as \"big data\"),cheaper and faster computersand advancedmachine learningtechniques were successfully applied to many problems throughout the economy. A turning point was the success ofdeep learningaround 2012 which improved the performance of machine learning on many tasks, including image and video processing, text analysis, and speech recognition.[262]Investment in AI increased along with its capabilities, and by 2016, the market for AI-related products, hardware, and software reached more than $8 billion, and theNew York Timesreported that interest in AI had reached a \"frenzy\".[263]\nIn 2002,Ben Goertzeland others became concerned that AI had largely abandoned its original goal of producing versatile, fully intelligent machines, and argued in favor of more direct research intoartificial general intelligence(AGI). By the mid-2010s several companies and institutions had been founded to pursue artificial general intelligence, such asOpenAIandGoogle'sDeepMind. During the same period, new insights intosuperintelligenceraised concerns that AI was anexistential threat. The risks and unintended consequences of AI technology became an area of serious academic research after 2016.\nThe success of machine learning in the 2000s depended on the availability of vast amounts of training data and faster computers.[264]Russell and Norvig wrote that the \"improvement in performance obtained by increasing the size of the data set by two or three orders of magnitude outweighs any improvement that can be made by tweaking the algorithm.\"[206]Geoffrey Hintonrecalled that back in the 90s, the problem was that \"our labeled datasets were thousands of times too small. [And] our computers were millions of times too slow.\"[265]This was no longer true by 2010.\nThe most useful data in the 2000s came from curated, labeled data sets created specifically for machine learning and AI. In 2007, a group atUMass AmherstreleasedLabeled Faces in the Wild, an annotated set of images of faces that was widely used to train and testface recognitionsystems for the next several decades.[266]Fei-Fei LidevelopedImageNet, a database of three million images captioned by volunteers using theAmazon Mechanical Turk. Released in 2009, it was a useful body of training data and a benchmark for testing for the next generation of image processing systems.[267][206]Google releasedword2vecin 2013 as an open source resource. It used large amounts of data text scraped from the internet andword embeddingto create a numeric vector to represent each word. Users were surprised at how well it was able to capture word meanings, for example, ordinary vector addition would give equivalences like China + River = Yangtze or London − England + France = Paris.[268]This database in particular would be essential for the development oflarge language modelsin the late 2010s.\nThe explosive growth of the internet gave machine learning programs access to billions of pages of text and images that could bescraped. And, for specific problems, large privately held databases contained the relevant data.McKinsey Global Institutereported that \"by 2009, nearly all sectors in the US economy had at least an average of 200 terabytes of stored data\".[269]This collection of information was known in the 2000s asbig data.\nIn aJeopardy!exhibition match in February 2011,IBM'squestion answering systemWatsondefeated the two bestJeopardy!champions,Brad RutterandKen Jennings, by a significant margin.[270]Watson's expertise would have been impossible without the information available on the internet.[206]\nIn 2012,AlexNet, adeep learningmodel,[am]developed byAlex Krizhevsky, won theImageNet Large Scale Visual Recognition Challenge, with significantly fewer errors than the second-place winner.[272][206]Krizhevsky worked withGeoffrey Hintonat theUniversity of Toronto.[an]This was a turning point in machine learning: over the next few years dozens of other approaches to image recognition were abandoned in favor of deep learning.[264]\nDeep learning uses a multi-layerperceptron. Although this architecture has been known since the 60s, getting it to work requires powerful hardware and large amounts of training data.[273]Before these became available, improving performance of image processing systems required hand-craftedad hocfeatures that were difficult to implement.[273]Deep learning was simpler and more general.[ao]\nDeep learning was applied to dozens of problems over the next few years (such as speech recognition, machine translation, medical diagnosis, and game playing). In every case it showed enormous gains in performance.[264]Investment and interest in AI boomed as a result.[264]\nIt became fashionable in the 2000s to begin talking about the future of AI again and several popular books considered the possibility ofsuperintelligentmachines and what they might mean for human society. Some of this was optimistic (such asRay Kurzweil'sThe Singularity is Near), but others warned that a sufficiently powerful AI wasexistential threatto humanity, such asNick BostromandEliezer Yudkowsky.[274]The topic became widely covered in the press and many leading intellectuals and politicians commented on the issue.\nAI programs in the 21st century are defined by theirgoals—the specific measures that they are designed to optimize.Nick Bostrom's influential 2014 bookSuperintelligence[275]argued that, if one isn't careful about defining these goals, the machine may cause harm to humanity in the process of achieving a goal.Stuart J. Russellused the example of an intelligent robot that kills its owner to prevent it from being unplugged, reasoning \"you can't fetch the coffee if you're dead\".[276](This problem is known by the technical term \"instrumental convergence\".) The solution is toalignthe machine's goal function with the goals of its owner and humanity in general. Thus, the problem of mitigating the risks and unintended consequences of AI became known as \"the value alignment problem\" or AI alignment.[277]\nAt the same time, machine learning systems had begun to have disturbing unintended consequences.Cathy O'Neilexplained how statistical algorithms had been among the causes of the2008 economic crash,[278]Julia AngwinofProPublicaargued that theCOMPASsystem used by the criminal justice system exhibited racial bias under some measures,[279][ap]others showed that many machine learning systems exhibited some form of racialbias,[281]and there were many other examples of dangerous outcomes that had resulted from machine learning systems.[aq]\nIn 2016, the election ofDonald Trumpand the controversy over the COMPAS system illuminated several problems with the current technological infrastructure, including misinformation, social media algorithms designed to maximize engagement, the misuse of personal data and the trustworthiness of predictive models.[282]Issues offairnessand unintended consequences became significantly more popular at AI conferences, publications vastly increased, funding became available, and many researchers refocused their careers on these issues. Thevalue alignment problembecame a serious field of academic study.[283][ar]\nIn the early 2000s, several researchers became concerned that mainstream AI was too focused on \"measurable performance in specific applications\"[285](known as \"narrow AI\") and had abandoned AI's original goal of creating versatile, fully intelligent machines. An early critic wasNils Nilssonin 1995, and similar opinions were published by AI elder statesmen John McCarthy, Marvin Minsky, and Patrick Winston in 2007–2009. Minsky organized a symposium on \"human-level AI\" in 2004.[285]Ben Goertzeladopted the term \"artificial general intelligence\" for the new sub-field, founding a journal and holding conferences beginning in 2008.[286]The new field grew rapidly, buoyed by the continuing success of artificial neural networks and the hope that it was the key to AGI.\nSeveral competing companies, laboratories and foundations were founded to develop AGI in the 2010s.DeepMindwas founded in 2010 by three English scientists,Demis Hassabis,Shane LeggandMustafa Suleyman, with funding fromPeter Thieland laterElon Musk. The founders and financiers were deeply concerned aboutAI safetyand theexistential risk of AI. DeepMind's founders had a personal connection with Yudkowsky, and Musk was among those who was actively raising the alarm.[287]Hassabis was both worried about the dangers of AGI and optimistic about its power; he hoped they could \"solve AI, then solve everything else.\"[288]The New York Timeswrote in 2023, \"At the heart of this competition is a brain-stretching paradox. The people who say they are most worried about AI are among the most determined to create it and enjoy its riches. They have justified their ambition with their strong belief that they alone can keep AI from endangering Earth.\"[287]\nIn 2012,Geoffrey Hinton(who been leading neural network research since the 80s) was approached byBaidu, which wanted to hire him and all his students for an enormous sum. Hinton decided to hold an auction and, at a Lake Tahoe AI conference, they sold themselves toGooglefor a price of $44 million. Hassabis took notice and sold DeepMind to Google in 2014, on the condition that it would not accept military contracts and would be overseen by an ethics board.[287]\nLarry Pageof Google, unlike Musk and Hassabis, was an optimist about the future of AI. Musk and Paige became embroiled in an argument about the risk of AGI at Musk's 2015 birthday party. They had been friends for decades but stopped speaking to each other shortly afterwards. Musk attended the one and only meeting of the DeepMind's ethics board, where it became clear that Google was uninterested in mitigating the harm of AGI. Frustrated by his lack of influence he foundedOpenAIin 2015, enlistingSam Altmanto run it and hiring top scientists. OpenAI began as a non-profit, \"free from the economic incentives that were driving Google and other corporations.\"[287]Musk became frustrated again and left the company in 2018. OpenAI turned to Microsoft for continued financial support and Altman and OpenAI formed a for-profit version of the company with more than $1 billion in financing.[287]\nIn 2021,Dario Amodeiand 14 other scientists left OpenAI over concerns that the company was putting profits above safety. They formedAnthropic, which soon had $6 billion in financing from Microsoft and Google.[287]\nThe AI boom started with the initial development of key architectures and algorithms such as thetransformer architecturein 2017, leading to the scaling and development of large language models exhibiting human-like traits of knowledge, attention, and creativity. The new AI era began in 2020, with the public release of scaledlarge language models(LLMs) such asChatGPT.[290]\nIn 2017, thetransformerarchitecture was proposed by Google researchers in a paper titled \"Attention Is All You Need\". It exploits aself-attentionmechanism and became widely used in large language models.[291]Large language models, based on the transformer, were further developed by other companies:OpenAIreleasedGPT-3in 2020, thenDeepMindreleasedGatoin 2022. These arefoundation models: they are trained on vast quantities of unlabeled data and can be adapted to a wide range of downstream tasks. These models can discuss a huge number of topics and display general knowledge. Thus, the question naturally arises: are these models an example ofartificial general intelligence?\nBill Gateswas skeptical of the new technology and the hype that surrounded AGI. However, Altman presented him with a live demo ofChatGPT-4passing an advanced biology test. Gates was convinced.[287]In 2023,Microsoft Researchtested the model with a large variety of tasks, and concluded that \"it could reasonably be viewed as an early (yet still incomplete) version of anartificial general intelligence(AGI) system\".[292]\nIn 2024,OpenAI o3, a type of advancedreasoning modeldeveloped by OpenAI, was announced. On the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) benchmark developed byFrançois Cholletin 2019, the model achieved an unofficial score of 87.5% on the semi-private test, surpassing the typical human score of 84%. The benchmark is supposed to be a necessary, but not sufficient test for AGI. Speaking of the benchmark, Chollet has said \"You'll know AGI is here when the exercise of creating tasks that are easy for regular humans but hard for AI becomes simply impossible.\"[293]\nInvestment in AI grew exponentially after 2020, with venture capital funding for generative AI companies increasing dramatically. Total AI investments rose from $18 billion in 2014 to $119 billion in 2021, with generative AI accounting for approximately 30% of investments by 2023.[294]According to metrics from 2017 to 2021, the United States outranked the rest of the world in terms ofventure capital funding, number ofstartups, and AIpatentsgranted.[295]The commercial AI scene became dominated by AmericanBig Techcompanies, whose investments in this area surpassed those from U.S.-basedventure capitalists.[296]OpenAI's valuation reached $86 billion by early 2024,[297]whileNVIDIA's market capitalization surpassed $3.3 trillion by mid-2024, making it the world's largest company bymarket capitalizationas the demand for AI-capableGPUssurged.[298]\n15.ai, launched in March 2020[299]by an anonymousMITresearcher,[300][301]was one of the earliest examples ofgenerative AIgaining widespread public attention during the initial stages of the AI boom.[302]The freeweb applicationdemonstrated the ability to clone character voices using neural networks with minimal training data, requiring as little as 15 seconds of audio to reproduce a voice—a capability later corroborated byOpenAIin 2024.[303]The service wentviralon social media platforms in early 2021,[304][305]allowing users to generate speech for characters frompopular mediafranchises, and became particularly notable for its pioneering role in popularizingAI voice synthesisforcreative contentandmemes.[306]\nContemporary AI systems are now becoming human-competitive at general tasks, and we must ask ourselves: Should we let machines flood our information channels with propaganda and untruth? Should we automate away all the jobs, including the fulfilling ones? Should we develop nonhuman minds that might eventually outnumber, outsmart, obsolete and replace us? Should we risk loss of control of our civilization? Such decisions must not be delegated to unelected tech leaders.Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable.This confidence must be well justified and increase with the magnitude of a system’s potential effects. OpenAI’s recent statement regarding artificial general intelligence, states that \"At some point, it may be important to get independent review before starting to train future systems, and for the most advanced efforts to agree to limit the rate of growth of compute used for creating new models.\" We agree. That point is now.\nTherefore,we call on all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4. This pause should be public and verifiable, and include all key actors. If such a pause cannot be enacted quickly, governments should step in and institute a moratorium.\nChatGPTwas launched on 30 November 2022, marking a pivotal moment in artificial intelligence's public adoption. Within days of its release it went viral, gaining over 100 million users in two months and becoming the fastest-growing consumer software application in history.[308]The chatbot's ability to engage in human-like conversations, write code, and generate creative content captured public imagination and led to rapid adoption across various sectors includingeducation,business, and research.[309]ChatGPT's success prompted unprecedented responses from major technology companies—Googledeclared a \"code red\" and rapidly launchedGemini(formerly known as Google Bard), whileMicrosoftincorporated the technology intoBing Chat.[310]\nThe rapid adoption of these AI technologies sparked intense debate about their implications. Notable AI researchers and industry leaders voiced both optimism and concern about the accelerating pace of development. In March 2023, over 20,000 signatories, includingcomputer scientistYoshua Bengio,Elon Musk, andAppleco-founderSteve Wozniak, signedan open letter calling for a pause in advanced AI development, citing \"profound risks to society and humanity.\"[311]However, other prominent researchers likeJuergen Schmidhubertook a more optimistic view, emphasizing that the majority of AI research aims to make \"human lives longer and healthier and easier.\"[312]\nBy mid-2024, however, the financial sector began to scrutinize AI companies more closely, particularly questioning their capacity to produce areturn on investmentcommensurate with their massive valuations. Some prominent investors raised concerns about market expectations becoming disconnected from fundamental business realities.Jeremy Grantham, co-founder ofGMO LLC, warned investors to \"be quite careful\" and drew parallels to previous technology-driven market bubbles.[313]Similarly,Jeffrey Gundlach, CEO ofDoubleLine Capital, explicitly compared the AI boom to thedot-com bubbleof the late 1990s, suggesting that investor enthusiasm might be outpacing realistic near-term capabilities and revenue potential.[314]These concerns were amplified by the substantial market capitalizations of AI-focused companies, many of which had yet to demonstrate sustainable profitability models.\nIn March 2024,Anthropicreleased theClaude3 family of large language models, including Claude 3 Haiku, Sonnet, and Opus.[315]The models demonstrated significant improvements in capabilities across various benchmarks, with Claude 3 Opus notably outperforming leading models from OpenAI and Google.[316]In June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated improved performance compared to the larger Claude 3 Opus, particularly in areas such as coding, multistep workflows, and image analysis.[317]\nIn 2024, theRoyal Swedish Academyof Sciences awardedNobel Prizesin recognition of groundbreaking contributions toartificial intelligence. The recipients included:\nIn January 2025, OpenAI announced a new AI, ChatGPT-Gov, which would be specifically designed for US government agencies to use securely.[319]Open AI said that agencies could utilize ChatGPT Gov on a Microsoft Azure cloud or Azure Government cloud, \"on top of Microsoft’s Azure’s OpenAI Service.\" OpenAI's announcement stated that \"Self-hosting ChatGPT Gov enables agencies to more easily manage their own security, privacy, and compliance requirements, such as stringent cybersecurity frameworks (IL5, CJIS, ITAR,FedRAMPHigh). Additionally, we believe this infrastructure will expedite internal authorization of OpenAI's tools for the handling of non-public sensitive data.\"[319]\nCountries have invested in policies and funding to deployautonomous robotsin an attempt to address labor shortages and enhancing efficiency, while also implementingregulatory frameworksfor ethical and safe development.\nIn 2025, China invested approximately 730 billion yuan (roughly US$100 billion) to advance AI and robotics in smart manufacturing and healthcare.[320]The \"14th Five-Year Plan\" (2021–2025) prioritized service robots, with AI systems enabling robots to perform complex tasks like assisting in surgeries or automating factory assembly lines.[321]Some funding also supported defense applications, such as autonomous drones.[322][323]Starting in September 2025, China mandated labeling of AI-generated content to ensure transparency and public trust in these technologies.[324]\nIn January 2025,Stargate LLCwas formed as a joint venture ofOpenAI,SoftBank,Oracle, andMGX, who announced plans to invest US$500 billion in AI infrastructure in theUnited Statesby 2029. The venture was formally announced by U.S. President Donald Trump on 21 January 2025, with SoftBank CEOMasayoshi Sonappointed as chairman.[325][326]\nThe U.S. government allocated approximately $2 billion to integrate AI and robotics in manufacturing and logistics.[327][non-primary source needed]State governments supplemented this with funding for service robots, such as those deployed in warehouses to fulfill verbal commands for inventory management or in eldercare facilities to respond to residents' requests for assistance.[328]Some funds were directed to defense, includinglethal autonomous weaponandmilitary robot. In January 2025, Executive Order 14179 established an \"AI Action Plan\" to accelerate innovation and deployment of these technologies.[329][non-primary source needed]"
    }
]